{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "256db42191074e088700bcd91ab6806e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9bff89d6196d4b2fa03b9b7a4bc1fe11",
              "IPY_MODEL_8662c747c5c245f0be04920157220207",
              "IPY_MODEL_c350e8fd93d94ec98176d3478b3ee466"
            ],
            "layout": "IPY_MODEL_cacf2894023c40d096973f4cacb3c362"
          }
        },
        "9bff89d6196d4b2fa03b9b7a4bc1fe11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34adce56001c41fabfb4d1716b429f06",
            "placeholder": "​",
            "style": "IPY_MODEL_89ff48a748ad4492b60ef261226cebed",
            "value": "model.safetensors: 100%"
          }
        },
        "8662c747c5c245f0be04920157220207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e1260c562574a878e5b9b12d015b017",
            "max": 352824413,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36e8e74970ba41efa4264e20577c4984",
            "value": 352824413
          }
        },
        "c350e8fd93d94ec98176d3478b3ee466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa2f0350c6444083bbdff7f54b6e3a92",
            "placeholder": "​",
            "style": "IPY_MODEL_da68a6387c9c4986b396fcc01cf42751",
            "value": " 353M/353M [00:01&lt;00:00, 310MB/s]"
          }
        },
        "cacf2894023c40d096973f4cacb3c362": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34adce56001c41fabfb4d1716b429f06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89ff48a748ad4492b60ef261226cebed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e1260c562574a878e5b9b12d015b017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36e8e74970ba41efa4264e20577c4984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa2f0350c6444083bbdff7f54b6e3a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da68a6387c9c4986b396fcc01cf42751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b174a8bbc0bb434b9afb38ae8f76722a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc5d72e2f6eb411fa9428b7b67387e80",
              "IPY_MODEL_2479fe19eada4482aacaf08cf9fae412",
              "IPY_MODEL_1cdc9cd7d23e41c0906381bae172aac8"
            ],
            "layout": "IPY_MODEL_2c00fc903a1441a482dcdaab3b2838d0"
          }
        },
        "fc5d72e2f6eb411fa9428b7b67387e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0c23d8532ce4e53a89e2283ad442d82",
            "placeholder": "​",
            "style": "IPY_MODEL_c5f838014c304a72a8fef05679ec6d52",
            "value": "config.json: 100%"
          }
        },
        "2479fe19eada4482aacaf08cf9fae412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9562d0b1455c4fdc90292590b1487336",
            "max": 998,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3fb4a92bc26849c390f72b3e0429083c",
            "value": 998
          }
        },
        "1cdc9cd7d23e41c0906381bae172aac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0ef5a6f99d74420b5360ab16548f269",
            "placeholder": "​",
            "style": "IPY_MODEL_725376ff73574b9fb2ab125d8d175f7d",
            "value": " 998/998 [00:00&lt;00:00, 97.8kB/s]"
          }
        },
        "2c00fc903a1441a482dcdaab3b2838d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c23d8532ce4e53a89e2283ad442d82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f838014c304a72a8fef05679ec6d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9562d0b1455c4fdc90292590b1487336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fb4a92bc26849c390f72b3e0429083c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0ef5a6f99d74420b5360ab16548f269": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "725376ff73574b9fb2ab125d8d175f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "deef137d2ca34ed390ffcde72b669fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76764e8c12594d3c9a2b371e7acae375",
              "IPY_MODEL_871614e81f364317932fb8a817afb406",
              "IPY_MODEL_37e8007808054fedb862c9013ef4b2a9"
            ],
            "layout": "IPY_MODEL_8066918117e4489d9128723b83047474"
          }
        },
        "76764e8c12594d3c9a2b371e7acae375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6acf5c7e0458492eb73d6837e1cda691",
            "placeholder": "​",
            "style": "IPY_MODEL_05c57bfb108b4513835417558671d5fb",
            "value": "model.safetensors: 100%"
          }
        },
        "871614e81f364317932fb8a817afb406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33a02a30336548c2a20244b2cf056bb2",
            "max": 1334400964,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_184561bd6c484fefae1c5b8b5810a8e4",
            "value": 1334400964
          }
        },
        "37e8007808054fedb862c9013ef4b2a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1824a259c694415d9d865415abe6b8b1",
            "placeholder": "​",
            "style": "IPY_MODEL_9967707719014d719d29b45f9da337d9",
            "value": " 1.33G/1.33G [00:05&lt;00:00, 218MB/s]"
          }
        },
        "8066918117e4489d9128723b83047474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6acf5c7e0458492eb73d6837e1cda691": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05c57bfb108b4513835417558671d5fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33a02a30336548c2a20244b2cf056bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "184561bd6c484fefae1c5b8b5810a8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1824a259c694415d9d865415abe6b8b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9967707719014d719d29b45f9da337d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c7571318823472292affe7b2550cf11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2b903db56164d4c8ce40dd133b4631e",
              "IPY_MODEL_364fb326fa8d47c4a375f9874c036770",
              "IPY_MODEL_89ca973215b74797ad7aed7a41b626fa"
            ],
            "layout": "IPY_MODEL_0bd6916038574698891b4f07a290f5bb"
          }
        },
        "f2b903db56164d4c8ce40dd133b4631e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_996b42f909154e63a15003f570e6126e",
            "placeholder": "​",
            "style": "IPY_MODEL_fdf084a16abd4b60942f4f6b70929ebf",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "364fb326fa8d47c4a375f9874c036770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34c892e298de432b99ffb301cc506ff3",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab395a0d232f4cb88997e9acfda8d819",
            "value": 60
          }
        },
        "89ca973215b74797ad7aed7a41b626fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cccbb556e01490b82892759e76aee67",
            "placeholder": "​",
            "style": "IPY_MODEL_bb48f2c8a53f4f9fbc96b66cd3a9a6cd",
            "value": " 60.0/60.0 [00:00&lt;00:00, 4.25kB/s]"
          }
        },
        "0bd6916038574698891b4f07a290f5bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "996b42f909154e63a15003f570e6126e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdf084a16abd4b60942f4f6b70929ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34c892e298de432b99ffb301cc506ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab395a0d232f4cb88997e9acfda8d819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cccbb556e01490b82892759e76aee67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb48f2c8a53f4f9fbc96b66cd3a9a6cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd61c4aababb440f8f474dd17f808cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df614718f43a4b229381cbab65d9f95f",
              "IPY_MODEL_021ffbfd6581461b82805053127f4c47",
              "IPY_MODEL_6cbcf0434a814a7eb0fc8a717251267c"
            ],
            "layout": "IPY_MODEL_9a58435ef3ae4f458c647a3f3a4f94c7"
          }
        },
        "df614718f43a4b229381cbab65d9f95f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f610303dcc30485fb9071003c7190e8a",
            "placeholder": "​",
            "style": "IPY_MODEL_81fcc4f188474cf6a9659ba7d5e2d9e4",
            "value": "vocab.txt: 100%"
          }
        },
        "021ffbfd6581461b82805053127f4c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73d1ab38a11a446698b643549941c6fc",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ccb2f00d5d14cc9b216df1634649fee",
            "value": 213450
          }
        },
        "6cbcf0434a814a7eb0fc8a717251267c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f75be3250444d61bf4a37c160726746",
            "placeholder": "​",
            "style": "IPY_MODEL_6217a68e9301412fb32560f21f8112b4",
            "value": " 213k/213k [00:00&lt;00:00, 2.52MB/s]"
          }
        },
        "9a58435ef3ae4f458c647a3f3a4f94c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f610303dcc30485fb9071003c7190e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81fcc4f188474cf6a9659ba7d5e2d9e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73d1ab38a11a446698b643549941c6fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ccb2f00d5d14cc9b216df1634649fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f75be3250444d61bf4a37c160726746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6217a68e9301412fb32560f21f8112b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS1090B Introduction to Data Science\n",
        "\n",
        "## Lab 10: Transformers in HuggingFace [Draft]\n",
        "\n",
        "**Harvard University**<br/>\n",
        "**Spring 2025**<br/>\n",
        "**Instructors**: Pavlos Protopapas, Natesh Pillai, and Chris Gumb<br/>\n",
        "**Authors**: Chris Gumb, Shivas Jayaram, and Guangya Wan\n",
        "\n",
        "<br/>\n",
        "---"
      ],
      "metadata": {
        "id": "RAnQ9EoZKHeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Make a Copy to Edit\n",
        "\n",
        "This notebook is **view-only**. To edit it, follow these steps:\n",
        "\n",
        "1. Click **File** > **Save a copy in Drive**.\n",
        "2. Your own editable copy will open in a new tab.\n",
        "\n",
        "Now you can modify and run the code freely!"
      ],
      "metadata": {
        "id": "Tpu8rBEWkY9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives\n",
        "\n",
        "By the end of this Lab, you will understand:\n",
        "* **HuggingFace** Library and popular **BERT** architecture in **Transformer** library\n",
        "* Basic Functionality of the **Transformer** library\n",
        "* An example for a **Text Classification** task using **Pre-trained BERT** including fine-tuning with  **Keras** (**Super Relevant to HW6!!**)\n",
        "* **Extras**: How to use **GPT2** (Generative Pre-Training) for language generation tasks and how to fine-tune on a custom dataset"
      ],
      "metadata": {
        "id": "IVxj4k4_LDti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# !pip install --upgrade transformers\n",
        "# !pip install tf-keras\n",
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ],
      "metadata": {
        "id": "ajO8V0TcLNLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.utils.layer_utils import count_params\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Transformers\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, GPT2Config\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler"
      ],
      "metadata": {
        "id": "I-kO1ynrKYvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_start = time.time()"
      ],
      "metadata": {
        "id": "OQCvfRuP4k7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI89wl5_RUK3",
        "outputId": "5a55a328-75ad-4bd6-f6fd-7f3023a23f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face and Transformer Overview\n",
        "\n",
        "\n",
        "Hugging Face is an AI research organization and company that focuses on natural language processing (NLP) and natural language understanding (NLU). They are best known for their open-source Transformers library, which is a widely-used, state-of-the-art library for building, training, and deploying transformer-based models. Hugging Face has contributed significantly to the NLP community by providing user-friendly APIs, pre-trained models, and extensive documentation, making it easier for researchers, developers, and practitioners to work with transformer models.\n",
        "\n",
        "The Transformers library, initially released in 2019, provides a comprehensive collection of transformer architectures, such as BERT, GPT, RoBERTa, T5, and many others. The library supports both PyTorch and TensorFlow deep learning frameworks, enabling users to seamlessly switch between them. It also offers a wide range of pre-trained models for various NLP tasks, including text classification, named entity recognition, question answering, and more.\n",
        "\n",
        "One of the key features of the Transformers library is its focus on usability and ease of integration. It offers simple APIs for tokenization, model training, fine-tuning, and inference, along with many useful utilities for handling datasets and evaluation metrics. The library is continually updated and maintained by Hugging Face, with active community contributions, which helps keep it at the forefront of NLP research and development.\n",
        "\n",
        "Refer to the docs for more information: https://huggingface.co/docs"
      ],
      "metadata": {
        "id": "TkRzTTYnsYuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 🔑 Getting Access to Gated Models (e.g., LLaMA 3) on Hugging Face\n",
        "\n",
        "To use powerful models like Meta's **LLaMA 3**, you need to:\n",
        "\n",
        "1. **Create a free Hugging Face account**  \n",
        "   👉 https://huggingface.co/join\n",
        "\n",
        "2. **Request access to gated models**  \n",
        "   Visit the model page (e.g., [LLaMA 3](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)) and click **\"Request Access\"**.  \n",
        "\n",
        "3. **Generate a personal access token**  \n",
        "   - Go to https://huggingface.co/settings/tokens  \n",
        "   - Click **\"New token\"**, give it a name (e.g., `colab_token`), and select **\"Read\"** permissions.  \n",
        "   - Copy the token somewhere safe!\n",
        "---\n",
        "**💡NOTE FOR HW5**\n",
        "\n",
        "You’ll need an HF account, token, and access LLaMA 3 for the final homework assignment. Access can take a few days to be granted, **so please request it today**.\n",
        "\n",
        "---\n",
        "### 🔐 Logging In to Hugging Face from Your Notebook\n",
        "\n",
        "In Colab (or Jupyter), you can authenticate using your token like this:\n",
        "\n",
        "```python\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# This will prompt you to paste your token in a secure input field\n",
        "notebook_login()\n",
        "```\n",
        "---"
      ],
      "metadata": {
        "id": "-9QXJDyBnm5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained Model: BERT vs DistilBERT vs DeBERTa"
      ],
      "metadata": {
        "id": "ZRkulQweyVpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT (Bidirectional Encoder Representations from Transformers)**, **DistilBERT**, and **DeBERTa (Decoding-enhanced BERT with disentangled attention)** are all state-of-the-art transformer-based models for natural language processing tasks. Here's a brief introduction to each model and a comparison of their characteristics:\n",
        "\n",
        "1. **BERT (Bidirectional Encoder Representations from Transformers)**:\n",
        "Introduced by Google AI in 2018, BERT revolutionized NLP by employing bidirectional context in the self-attention mechanism of transformers. BERT is pre-trained on large text corpora using masked language modeling (MLM) and next sentence prediction (NSP) tasks, making it capable of understanding context and generating high-quality embeddings. BERT can be fine-tuned for various downstream tasks such as text classification, named entity recognition, and question answering.\n",
        "\n",
        "2. **DistilBERT**:\n",
        "DistilBERT, developed by Hugging Face, is a smaller, faster, and more efficient version of BERT. It is created using knowledge distillation, a technique that transfers knowledge from a larger teacher model (in this case, BERT) to a smaller student model (DistilBERT). DistilBERT retains about 95% of BERT's performance while having only 50% of its parameters, which makes it more suitable for deployment in resource-constrained environments or real-time applications.\n",
        "\n",
        "3. **DeBERTa (Decoding-enhanced BERT with disentangled attention)**:\n",
        "Introduced by Microsoft Research, DeBERTa is an improved variant of BERT that addresses certain limitations in the original BERT model. DeBERTa introduces disentangled attention, which separates the content and position information during the self-attention computation, leading to a more effective and expressive model. DeBERTa also incorporates relative position embeddings, which improve the model's ability to capture positional information. These enhancements enable DeBERTa to outperform BERT and other transformer models on various NLP benchmarks.\n",
        "\n",
        "**Comparison**:\n",
        "- BERT is the foundational model that introduced bidirectional context and self-attention mechanisms in transformers, setting a new standard for NLP models.\n",
        "- DistilBERT offers a more lightweight, efficient alternative to BERT, with reduced model size and computational requirements while maintaining most of the performance. It is well-suited for deployment in resource-constrained environments.\n",
        "- DeBERTa builds upon BERT by introducing disentangled attention and relative position embeddings, resulting in a more expressive and effective model. DeBERTa often outperforms BERT and other transformer models on NLP benchmarks.\n",
        "\n",
        "All three models are powerful and widely used in NLP tasks. Depending on your specific requirements, such as model size, inference speed, and accuracy, you can choose the appropriate model for your application.\n"
      ],
      "metadata": {
        "id": "CkKelGcF0gBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤗 AutoModel\n",
        "With so many different Transformer architectures, it can be challenging to decide on a single model checkpoint for your use case. As a part of 🤗 Transformers' core philosophy to make the library easy, simple and flexible to use, an AutoModelClass automatically infer and load the correct architecture from a given checkpoint.\n",
        "\n",
        "The from_pretrained() method lets you quickly load a pretrained model for any architecture so you don't have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different.\n",
        "\n",
        "The AutoModel class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It’s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.\n",
        "\n",
        "Side notes: if you know the type of model you want to use, you can use the class that defines its architecture directly. Architecture refers to the skeleton of the model and checkpoints are the weights for a given architecture. For example, BERT is an architecture, while bert-base-uncased is a checkpoint. \"Model\" in this cotext is a general term that can mean either architecture or checkpoint.\n",
        "\n"
      ],
      "metadata": {
        "id": "CzidK79PH29l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model-agnostic AutoModel approach\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHcHcW4rH2iD",
        "outputId": "638f0c03-559c-4188-ad84-edc682d12c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSxwFwybdicl",
        "outputId": "60dcc0d8-a17b-4afd-9893-868da7c4c50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertConfig {\n",
              "  \"activation\": \"gelu\",\n",
              "  \"architectures\": [\n",
              "    \"DistilBertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"dim\": 768,\n",
              "  \"dropout\": 0.1,\n",
              "  \"hidden_dim\": 3072,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"distilbert\",\n",
              "  \"n_heads\": 12,\n",
              "  \"n_layers\": 6,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"qa_dropout\": 0.1,\n",
              "  \"seq_classif_dropout\": 0.2,\n",
              "  \"sinusoidal_pos_embds\": false,\n",
              "  \"tie_weights_\": true,\n",
              "  \"transformers_version\": \"4.50.3\",\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative approach that uses the specific Model module\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "model_2 = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5EsjKVLqt86",
        "outputId": "7012d94f-265a-4a56-fadf-ead6a7b4dc6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The two models are the same!\n",
        "model.get_config() == model_2.get_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABz9g1PGrC0w",
        "outputId": "416b2ab5-6668-4df3-f452-70723b24bafa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoTokenizer"
      ],
      "metadata": {
        "id": "JanxK0o3EeJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model.\n",
        "\n",
        "Loading and saving tokenizers is as simple as it is with models with the AutoTokenizer Class"
      ],
      "metadata": {
        "id": "wW0JT9ffE2Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a tokenizer with AutoTokenizer.from_pretrained():\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "h91afpiAFFT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\"There are only 2 labs left! 😿\") # same as batch_encode_plus and encode_plus, will autotamatically detect which to use\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21MY_98jFScU",
        "outputId": "571d8a7d-8d9a-4fe6-f556-618e2800b784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 2045, 2024, 2069, 1016, 13625, 2187, 999, 100, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the resulting tokenized input is NOT a dictionary\n",
        "# Refer https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__.returns\n",
        "type(encoded_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "wQs-TKYDMLFi",
        "outputId": "7ed0f516-4a13-432b-e0b2-60b3e805e21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.tokenization_utils_base.BatchEncoding"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>transformers.tokenization_utils_base.BatchEncoding</b><br/>def __init__(data: Optional[Dict[str, Any]]=None, encoding: Optional[Union[EncodingFast, Sequence[EncodingFast]]]=None, tensor_type: Union[None, str, TensorType]=None, prepend_batch_axis: bool=False, n_sequences: Optional[int]=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py</a>Holds the output of the [`~tokenization_utils_base.PreTrainedTokenizerBase.__call__`],\n",
              "[`~tokenization_utils_base.PreTrainedTokenizerBase.encode_plus`] and\n",
              "[`~tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus`] methods (tokens, attention_masks, etc).\n",
              "\n",
              "This class is derived from a python dictionary and can be used as a dictionary. In addition, this class exposes\n",
              "utility methods to map from word/character space to token space.\n",
              "\n",
              "Args:\n",
              "    data (`dict`, *optional*):\n",
              "        Dictionary of lists/arrays/tensors returned by the `__call__`/`encode_plus`/`batch_encode_plus` methods\n",
              "        (&#x27;input_ids&#x27;, &#x27;attention_mask&#x27;, etc.).\n",
              "    encoding (`tokenizers.Encoding` or `Sequence[tokenizers.Encoding]`, *optional*):\n",
              "        If the tokenizer is a fast tokenizer which outputs additional information like mapping from word/character\n",
              "        space to token space the `tokenizers.Encoding` instance or list of instance (for batches) hold this\n",
              "        information.\n",
              "    tensor_type (`Union[None, str, TensorType]`, *optional*):\n",
              "        You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at\n",
              "        initialization.\n",
              "    prepend_batch_axis (`bool`, *optional*, defaults to `False`):\n",
              "        Whether or not to add a batch axis when converting to tensors (see `tensor_type` above). Note that this\n",
              "        parameter has an effect if the parameter `tensor_type` is set, *otherwise has no effect*.\n",
              "    n_sequences (`Optional[int]`, *optional*):\n",
              "        You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at\n",
              "        initialization.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 193);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "cqjl2vIjm1cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you can train a model on a dataset, it needs to be preprocessed into the model's expected input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. 🤗 Transformers provides a set of preprocessing classes to help prepare your data for the model.\n",
        "\n",
        "The main tool for preprocessing textual data is the tokenizer. As we saw above, the tokenizer splits text into tokens according to a set of rules. The tokens are converted into numbers and then tensors, which become the model inputs. Any additional inputs required by the model, such as masking, are also created by the tokenizer.\n",
        "\n",
        "Below, we will demonstrate how to use a Tokenizer to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors.\n",
        "\n",
        "The tokenizer returns a dictionary-like obect with three important items:\n",
        "\n",
        "* [input_ids](https://huggingface.co/docs/transformers/main/en/glossary#input-ids) are the indices corresponding to each token in the sentence.\n",
        "* [attention_mask](https://huggingface.co/docs/transformers/main/en/glossary#attention-mask) indicates whether a token should be attended to or not.\n",
        "* [token_type_ids](https://huggingface.co/docs/transformers/main/en/glossary#token-type-ids) identifies which sequence a token belongs to when there is more than one sequence."
      ],
      "metadata": {
        "id": "bPzSX1PUFxe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can convered our previously encoded input text by decoding the encoding's `input_ids`:\n",
        "tokenizer.decode(encoded_input[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kHho4KcuF7W6",
        "outputId": "7d84e6f0-fa99-45ea-a4e1-ed4000124f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] there are only 2 labs left! [UNK] [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier and separator) - to the sentence. Not all models need\n",
        "special tokens, but if they do, the tokenizer automatically adds them for you.\n",
        "\n",
        "It also makes use of an `UNK` token for tokens outside its vocabulary.\n",
        "\n",
        "If there are several sentences you want to preprocess, pass them as a list to the tokenizer:"
      ],
      "metadata": {
        "id": "ZwQJbc20JYYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sentences = [\n",
        "    \"But what about second breakfast?\",\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    \"What about elevensies?\",\n",
        "]\n",
        "encoded_inputs = tokenizer(batch_sentences)\n",
        "print(encoded_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VGfcLe7JZKZ",
        "outputId": "186559a8-de9a-4820-c16b-06e1ce30e622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 2021, 2054, 2055, 2117, 6350, 1029, 102], [101, 2123, 1005, 1056, 2228, 2002, 4282, 2055, 2117, 6350, 1010, 28315, 1012, 102], [101, 2054, 2055, 5408, 14625, 1029, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding"
      ],
      "metadata": {
        "id": "n4_jCGrbJeZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentences aren't always the same length which can be an issue because tensors, the model inputs, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special *padding token* to shorter sentences.\n",
        "\n",
        "Set the `padding` parameter to `True` to pad the shorter sequences in the batch to match the longest sequence:"
      ],
      "metadata": {
        "id": "33D-G5X6tCb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(batch_sentences, padding=True)\n",
        "print(encoded_input) # We now see trailing 0s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtlPOPcrJho5",
        "outputId": "3720f2d5-1577-4a0f-a861-34569e93c74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 2021, 2054, 2055, 2117, 6350, 1029, 102, 0, 0, 0, 0, 0, 0], [101, 2123, 1005, 1056, 2228, 2002, 4282, 2055, 2117, 6350, 1010, 28315, 1012, 102], [101, 2054, 2055, 5408, 14625, 1029, 102, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice also the difference in the attention masks when we use padding!"
      ],
      "metadata": {
        "id": "EcaOnzd6g6YW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Truncation"
      ],
      "metadata": {
        "id": "h3hkvPv4JjzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you'll need to truncate the sequence to a shorter length.\n",
        "\n",
        "Set the `truncation` parameter to `True` to truncate a sequence to the maximum length accepted by the model:"
      ],
      "metadata": {
        "id": "Xkre2JAGtE6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, max_length=8)\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQWXj4OeJmza",
        "outputId": "f5ef4d76-9ab5-4997-ddb3-b4bd14f38569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 2021, 2054, 2055, 2117, 6350, 1029, 102], [101, 2123, 1005, 1056, 2228, 2002, 4282, 102], [101, 2054, 2055, 5408, 14625, 1029, 102, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers have a limit context window size, so you will need to truncate input sequences to respect this limit. The tokenizer will warn you if you go over, but it will not automatically truncate the sequence."
      ],
      "metadata": {
        "id": "CVTVAMFtiOn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text consisting of N words\n",
        "N = 600\n",
        "long_text = \" \".join([\"blah\" for _ in range(N)])\n",
        "print(long_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZVaFa9dheQf",
        "outputId": "9fa7eb52-d2f5-455b-8369-e996d49903b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# On the first run, you get a warning for going over the window size of\n",
        "# model associated with this tokenized (here 512)\n",
        "long_text_encoded = tokenizer(long_text)"
      ],
      "metadata": {
        "id": "3jwPaDMFhn5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918e76ae-6c8d-487b-aac1-4ef603d83c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# But the sequence is not truncated by default\n",
        "# Not that we use len() because the encoding's `input_ids` is actually a list!\n",
        "len(long_text_encoded['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcJA88Vii2Mu",
        "outputId": "b64186c0-732f-4dd2-8cbc-25ba8f2197a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "602"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can always simply set `truncation=True` and the tokenizer will trunkate to the associate model's window size."
      ],
      "metadata": {
        "id": "eZ0tXqIQkl4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncates sequnces to the model's window size\n",
        "long_text_encoded_trunc = tokenizer(long_text, truncation=True)\n",
        "len(long_text_encoded_trunc['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPMgQPH0kk6C",
        "outputId": "321b351f-90f4-407d-e4a2-8b9ab2d9ae59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Tensors"
      ],
      "metadata": {
        "id": "-yROKRYaJtha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you'll want the tokenizer to return the actual tensors that get fed to the model.\n",
        "\n",
        "Set the `return_tensors` parameter to either `pt` for PyTorch, or `tf` for TensorFlow (Keras):"
      ],
      "metadata": {
        "id": "lrjiWTkGJxxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sentences = [\n",
        "    \"But what about second breakfast?\",\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    \"What about elevensies?\",\n",
        "]\n",
        "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRxT-kqjJvTI",
        "outputId": "3ca9aa40-cb47-4d74-c1cb-ed8e74deb8e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(3, 14), dtype=int32, numpy=\n",
            "array([[  101,  2021,  2054,  2055,  2117,  6350,  1029,   102,     0,\n",
            "            0,     0,     0,     0,     0],\n",
            "       [  101,  2123,  1005,  1056,  2228,  2002,  4282,  2055,  2117,\n",
            "         6350,  1010, 28315,  1012,   102],\n",
            "       [  101,  2054,  2055,  5408, 14625,  1029,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 14), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🍿 Sentence Classification Example\n",
        "\n",
        "The revisit the (by now quite familiar) task of classifing IMDb movie reviews as either postive or negative.\n",
        "\n",
        "But this time, we'll use a pre-trained transformer as the foundation of our binary text classification task."
      ],
      "metadata": {
        "id": "Ez-dnJYUyKyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "KmTvFfESn8SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to load and prepare the dataset\n",
        "def load_and_prepare_data(split, samples, batch_size):\n",
        "    # Load the dataset for the given split, and take a specified number of samples\n",
        "    ds = tfds.load('imdb_reviews', split=split, as_supervised=True).take(samples)\n",
        "\n",
        "    # Batch the dataset and prefetch to improve performance\n",
        "    ds = ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    x, y = [], []\n",
        "\n",
        "    # Iterate over the dataset in batches\n",
        "    for batch_reviews, batch_labels in tfds.as_numpy(ds):\n",
        "        # Decode the reviews and extend the lists\n",
        "        decoded_reviews = [review.decode() for review in batch_reviews]\n",
        "        x.extend(decoded_reviews)\n",
        "        y.extend(batch_labels)\n",
        "\n",
        "    # Convert the lists to NumPy arrays and return them\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "train_shape = 1600\n",
        "test_shape = 400\n",
        "batch_size = 4\n",
        "\n",
        "# Load and prepare the train and test datasets\n",
        "train_x, train_y = load_and_prepare_data(tfds.Split.TRAIN, train_shape, batch_size)\n",
        "test_x, test_y = load_and_prepare_data(tfds.Split.TEST, test_shape, batch_size)\n",
        "\n",
        "# Display a few samples from the train dataset\n",
        "print(\"Train samples:\")\n",
        "for i in range(3):\n",
        "    print(f\"Review {i + 1}:\")\n",
        "    print(train_x[i][:50], train_y[i])\n",
        "    print()\n",
        "\n",
        "# Display a few samples from the test dataset\n",
        "print(\"Test samples:\")\n",
        "for i in range(3):\n",
        "    print(f\"Review {i + 1}:\")\n",
        "    print(test_x[i][:50], test_y[i])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "0lzXYakYO4Xu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe66c66-4275-4933-d2fd-c32c674d7632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples:\n",
            "Review 1:\n",
            "This was an absolutely terrible movie. Don't be lu 0\n",
            "\n",
            "Review 2:\n",
            "I have been known to fall asleep during films, but 0\n",
            "\n",
            "Review 3:\n",
            "Mann photographs the Alberta Rocky Mountains in a  0\n",
            "\n",
            "Test samples:\n",
            "Review 1:\n",
            "There are films that make careers. For George Rome 1\n",
            "\n",
            "Review 2:\n",
            "A blackly comic tale of a down-trodden priest, Naz 1\n",
            "\n",
            "Review 3:\n",
            "Scary Movie 1-4, Epic Movie, Date Movie, Meet the  0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model and Tokenizer"
      ],
      "metadata": {
        "id": "z5N-BRBwrAWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  TFAutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCe32I45rD_K",
        "outputId": "cd624830-e6de-4b35-dc46-217601001b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can be up to 512 for BERT, (the self attention mechanism used in the early transformers like BERT scales quadratically in the sequence length)\n",
        "max_length = 512"
      ],
      "metadata": {
        "id": "v33GCqFuUw-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "kjhhiZIysev6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the less-than-best practice of referencing variables outside the\n",
        "# function score\n",
        "def convert_example_to_feature(review):\n",
        "    \"\"\"\n",
        "    Tokenize the input text, truncate or pad to fit the maximum length,\n",
        "    and return the processed input as tensors.\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        review,\n",
        "        add_special_tokens=True,  # Add [CLS] and [SEP] tokens, help the model process and interpret the input data correctly\n",
        "        max_length=max_length,  # Set the maximum input length\n",
        "        truncation=True,  # Truncate input texts longer than the max length\n",
        "        padding=True,  # Pad shorter input texts with [PAD] tokens\n",
        "        return_tensors='tf',  # Return the output as TensorFlow tensors\n",
        "    )   # Review here https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
        "\n",
        "def encode_examples(x, y):\n",
        "    \"\"\"\n",
        "    Encode the input examples using the tokenizer, and return a TensorFlow dataset\n",
        "    containing the processed input data and corresponding labels.\n",
        "    \"\"\"\n",
        "    x_processed = convert_example_to_feature(list(x))\n",
        "    return tf.data.Dataset.from_tensor_slices(((x_processed[\"input_ids\"], x_processed[\"attention_mask\"]), y))"
      ],
      "metadata": {
        "id": "ex2cQOhUU09O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train dataset\n",
        "ds_train_encoded = encode_examples(train_x,train_y).shuffle(buffer_size=train_shape).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# test dataset\n",
        "ds_test_encoded = encode_examples(test_x,test_y).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "z5BpmBt3VF-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Fitting"
      ],
      "metadata": {
        "id": "JHSvo0rbuh_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "\n",
        "# Generally recommended learning rate for Adam 1e-5\n",
        "learning_rate = 1e-5\n",
        "\n",
        "# Number of epochs for training\n",
        "number_of_epochs = 10 # We usually do not need 10, but we have earlystopping!\n",
        "\n",
        "# Learning rate scheduling: warm-up and linear decay\n",
        "def scheduler(epoch, lr):\n",
        "    warmup_epochs = 1\n",
        "    decay_epochs = number_of_epochs - warmup_epochs\n",
        "    if epoch < warmup_epochs:\n",
        "        return lr * (epoch + 1) / warmup_epochs\n",
        "    else:\n",
        "        return lr * (1 - (epoch - warmup_epochs) / decay_epochs)\n",
        "\n",
        "# Advanced Built-in LR scheduler like PolynomialDecay or CosineDecay is also encouraged\n",
        "# See tutorial here for a list of LR scheduler https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/\n",
        "\n",
        "# Model checkpoint callback\n",
        "checkpoint_filepath = 'Demo_DistilBERT_checkpoint.weights.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True) # Recommended to set this to False, if you are facing a hard task and want to optimize it\n",
        "\n",
        "# Early Stopping callback\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss', # You can monitor validation loss or validation accuracy\n",
        "    patience=2, # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True, # Restore the best weights found during training\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Learning rate scheduler callback\n",
        "lr_callback = LearningRateScheduler(scheduler, verbose=1)\n",
        "\n",
        "# choosing Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "\n",
        "# tf.keras.optimizers.experimental.AdamW(lr=learning_rate, weight_decay = 1e-3) is also commonly used\n",
        "# refer https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/AdamW for more details\n",
        "\n",
        "# We do not have one-hot vectors, so we use sparse categorical cross entropy and accuracy\n",
        "# Note that the output from the model is logits, so we need from_logits=True\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "metadata": {
        "id": "j-RBW1fhV23V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model fitting\n",
        "bert_history = model.fit(ds_train_encoded,\n",
        "                         epochs=number_of_epochs,\n",
        "                         validation_data=ds_test_encoded, # because we are lazy\n",
        "                         callbacks=[lr_callback, early_stopping_callback, model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrocaUc8VGFT",
        "outputId": "96c84e92-1e23-4bbc-bf17-d761b36378ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-06.\n",
            "Epoch 1/10\n",
            "400/400 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.7525"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
            "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r400/400 [==============================] - 137s 282ms/step - loss: 0.4801 - accuracy: 0.7525 - val_loss: 0.3015 - val_accuracy: 0.8750 - lr: 1.0000e-05\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-06.\n",
            "Epoch 2/10\n",
            "400/400 [==============================] - 114s 284ms/step - loss: 0.2145 - accuracy: 0.9200 - val_loss: 0.3080 - val_accuracy: 0.8825 - lr: 1.0000e-05\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 8.888888664336668e-06.\n",
            "Epoch 3/10\n",
            "400/400 [==============================] - 116s 289ms/step - loss: 0.1127 - accuracy: 0.9606 - val_loss: 0.3269 - val_accuracy: 0.8875 - lr: 8.8889e-06\n",
            "Epoch 3: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract the history data\n",
        "history_dict = bert_history.history\n",
        "\n",
        "# Extract loss and accuracy values for both training and validation\n",
        "train_loss = history_dict['loss']\n",
        "train_accuracy = history_dict['accuracy']\n",
        "val_loss = history_dict['val_loss']\n",
        "val_accuracy = history_dict['val_accuracy']\n",
        "\n",
        "# Create a range of epoch numbers for the x-axis\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs') # Ouch... floats as x-ticks?\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.plot(epochs, train_accuracy, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "NvewWjxbyxEe",
        "outputId": "c7b65cce-7c5f-4ad5-cc01-1dd8e6af4386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model loading and Evaluation"
      ],
      "metadata": {
        "id": "XLLUpvXG7G3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In case you want to load your trained_model\n",
        "\n",
        "# Model initialization\n",
        "loaded_model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Load the weights from the checkpoint\n",
        "checkpoint_filepath = 'Demo_DistilBERT_checkpoint.weights.h5'\n",
        "loaded_model.load_weights(checkpoint_filepath)\n",
        "\n",
        "# Compile the model with the same optimizer, loss, and metric used during training\n",
        "# (This is because we'd only saved the model weights)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "loaded_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "eval_results = loaded_model.evaluate(ds_test_encoded)\n",
        "\n",
        "print(f\"Test Loss: {eval_results[0]}, Test Accuracy: {eval_results[1]}\")\n",
        "\n",
        "# You can also continue training from here\n",
        "# number_of_epochs = ...\n",
        "# bert_history = loaded_model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded, callbacks=[lr_callback, early_stopping_callback, model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3v0MCKr6QeP",
        "outputId": "f83dca5f-e90e-48e1-d530-a362131418cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 10s 85ms/step - loss: 0.3269 - accuracy: 0.8875\n",
            "Test Loss: 0.3269083499908447, Test Accuracy: 0.887499988079071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "1. **Loading and preprocessing the dataset**: The code begins by importing necessary libraries and loading the IMDB movie reviews dataset from TensorFlow Datasets. It preprocesses the dataset by taking a specified number of samples for training and testing, batching, and prefetching for better performance. The data is then decoded and converted into NumPy arrays.\n",
        "\n",
        "2. **Importing DistilBERT model and tokenizer**: The DistilBERT model is imported using Hugging Face's Transformers library, along with the tokenizer. A custom function `convert_example_to_feature` is defined to tokenize and preprocess the input text using the tokenizer, including adding special tokens, padding, and truncating.\n",
        "\n",
        "3. **Creating TensorFlow datasets**: Another custom function `encode_examples` is defined to create a TensorFlow dataset from the tokenized and preprocessed input text and labels.\n",
        "\n",
        "4. **Encoding and batching the datasets**: The training and test datasets are encoded using the `encode_examples` function and batched, with the training dataset also being shuffled.\n",
        "\n",
        "5. **Compiling the model**: The model is compiled with the Adam optimizer, learning rate 1e-5, and a sparse categorical cross-entropy loss function. A learning rate scheduler with warm-up and linear decay is defined, along with an early stopping and model checkpoint callback.\n",
        "\n",
        "6. **Fine-tuning and evaluation**: The model is fine-tuned on the training data, and its performance is evaluated on the test data. The best model weights are saved in a checkpoint file. Some plots are also generated\n",
        "\n",
        "7. **Loading the trained model**: The code also demonstrates how to load a pretrained DistilBERT model and the saved checkpoint file, compile the model with the same optimizer, loss function, and metric, and evaluate the model on the test dataset. This loaded model can be used for further fine-tuning or predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGPymUDnwZ_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tips for fine-tuning\n",
        "\n",
        "Below is what used:\n",
        "1. **Learning rate scheduling**: A custom learning rate scheduler with a warm-up phase and linear decay was used. This helps the model gradually adapt to the task-specific data and slowly decrease the learning rate as training progresses.\n",
        "\n",
        "2. **Early stopping**: Early stopping with a patience of 2 epochs was implemented to prevent overfitting by stopping the training process if there is no improvement in the validation loss for a specified number of epochs.\n",
        "\n",
        "3. **Model checkpointing**: The best model weights were saved during training based on validation accuracy, enabling the loading of the best-performing model later for further fine-tuning or prediction.\n",
        "\n",
        "\n",
        "Additional techniques that can be helpful in fine-tuning BERT models:\n",
        "\n",
        "1. **Gradient clipping**: Gradient clipping can help prevent exploding gradients during training, which can lead to unstable learning dynamics. This can be added to the optimizer as follows:\n",
        "\n",
        "```\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08, clipnorm=1.0)\n",
        "```\n",
        "\n",
        "\n",
        "2. **Weight decay**: Using the AdamW optimizer, which includes weight decay regularization, can help improve the model's generalization by applies a direct modification to the weights by shrinking them towards zero, independent of the adaptive learning rates:\n",
        "\n",
        "```\n",
        "from tensorflow.keras.optimizers.experimental import AdamW\n",
        "optimizer = AdamW(learning_rate=learning_rate, weight_decay=1e-3)\n",
        "```\n",
        "\n",
        "\n",
        "3. **Reduce learning rate on plateau**: Instead of using a custom learning rate scheduler, the `ReduceLROnPlateau` callback can be used to automatically reduce the learning rate when the validation loss plateaus:\n",
        "\n",
        "```\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1)\n",
        "```\n",
        "This callback can then be added to your list of callbacks during model.fit()\n",
        "\n"
      ],
      "metadata": {
        "id": "_z1nF5dx0aHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer\n",
        "\n",
        "Hugging Face's Trainer (https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer) is a high-level, easy-to-use training and evaluation wrapper designed specifically for fine-tuning and training transformer models. It provides a straightforward way to train, fine-tune, and evaluate models on various tasks, while handling most of the underlying complexities, such as data loading, evaluation metrics, and distributed training.\n",
        "\n",
        "Below is an example:\n",
        "\n",
        "```\n",
        "# Define the metric (accuracy)\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=1e-3,\n",
        "    warmup_steps=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train and evaluate the model\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "```\n",
        "\n",
        "For serious deep learning models with distributed and mixed-precision training, please consider Hugging Face's accelerate https://huggingface.co/docs/accelerate/index"
      ],
      "metadata": {
        "id": "45v4pzI11SPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2\n",
        "# Fine-Tuning an LLM on the Language Model Task Itself\n",
        "\n",
        "Above we saw how to use transfer learning to adapt an encoder model like DistilBERT to a new task such as sentence classification.\n",
        "\n",
        "Below we will see an example of how to fine-tune a decoder-based model like GPT-2. We'll be using model's original language model training task, but now using a new dataset.\n",
        "\n",
        "We'd like to use the DistilGPT-2 model from HuggingFace to generate some text about COVID-19. There is just one problem: this model was pre-trained on text that predates the pandemic and so the model doesn't 'know' anything about COVID.\n",
        "\n",
        "To remedy this, we will fine tune the model on a dataset of COVID related news articles."
      ],
      "metadata": {
        "id": "e6_no5kRyJ1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# Download\n",
        "dataset_path = tf.keras.utils.get_file(\n",
        "    origin=\"https://storage.googleapis.com/cs109b/datasets/covid.zip\",\n",
        "    extract=True)\n",
        "dataset_path = dataset_path.replace(\".zip\",\"\")\n",
        "print(\"dataset_path:\",dataset_path)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Download execution time (mins)\",execution_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoGB5ePcea5v",
        "outputId": "93fdb43e-a7b1-4567-8a19-98f928fcfa5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_path: /root/.keras/datasets/covid\n",
            "Download execution time (mins) 0.007118423779805501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all the paths to text files\n",
        "training_data = []\n",
        "text_files = os.listdir(dataset_path)\n",
        "training_data.extend([os.path.join(dataset_path,f) for f in text_files])\n",
        "\n",
        "# Subset your training data for demo, full datset can take a long time to train\n",
        "subset_data = True\n",
        "if subset_data:\n",
        "  training_data = training_data[:300]\n",
        "\n",
        "# Load the text content\n",
        "for idx, path in enumerate(training_data):\n",
        "  # Load text\n",
        "  with open(path) as file:\n",
        "    training_data[idx] = file.read()\n",
        "\n",
        "print(\"training_data count:\",len(training_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBtr3-G_ekD8",
        "outputId": "f7bce14b-4d97-4500-ded4-b904524d8bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training_data count: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a random sample of index\n",
        "data_samples = np.random.randint(0,high=len(training_data)-1, size=10)\n",
        "for i,data_idx in enumerate(data_samples):\n",
        "  print(\"Text:\",training_data[data_idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIa8sbM3ekGf",
        "outputId": "205539d1-e9a9-4d7a-8810-5a5464380748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Regeneron asks FDA for emergency authorization of its Covid-19 antibody therapy given to Trump last week  (CNN)Regeneron says it has applied to the US Food and Drug Administration for emergency use authorization for its experimental monoclonal antibody therapy, the same antibody cocktail given to President Donald Trump Friday after he was diagnosed with the virus. The biotechnology company confirmed it had submitted the application for the authorization in a statement on its website Wednesday night. \"Under our agreement with the U.S. government for the initial doses of REGN-COV2, if an EUA is granted the government has committed to making these doses available to the American people at no cost and would be responsible for their distribution,\" the statement said. \"At this time, there are doses available for approximately 50,000 patients, and we expect to have doses available for 300,000 patients in total within the next few months. Regeneron's experimental antibody treatment is still in large-scale clinical trials, but has been available for compassionate use, something the FDA has to approve on an individual basis, like it did for the President. The antibody therapy is a combination of two monoclonal antibodies that is designed specifically to block infectivity of SARS-CoV-2, the virus that causes Covid-19, according to the company's statement. A cocktail antibody therapy uses two or more lab-engineered antibodies. Regeneron's cocktail includes a monoclonal antibody that targets the spike protein the virus uses to drill into healthy cells, and another antibody that targets a different part of the novel coronavirus. With two, the hope is to trap and shut down viral replication. \"When you weigh the potential benefit versus the risks, the downsides are very low here because we have not seen any safety concerns,\" Regeneron CEO Dr. Leonard Schleifer told CNN in an interview after Trump received an 8 gram dose of the treatment.  Early data from the company's antibody trials released recently showed it worked fairly safely with few side effects. \"This class of drugs is an extremely safe class,\" Schleifer said. Dr. Richard Besser, a former acting director of the US Centers for Disease Control and Prevention who now heads the Robert Wood Johnson Foundation, said it makes sense that a treatment that gives a patient antibodies would help the immune system, but he said peer review will find any holes or pitfalls. \"I would withhold judgment on this until we see the data,\" Besser said. \"You know these early results that keep coming out from companies in press releases strike me as being ... much more about the stock price than they are about science.\" But Dr. Thomas Frieden, also a former CDC director, disagreed with Besser. Although Regeneron's antibody therapy is unproven, it is \"a promising treatment,\" Frieden told CNN. \"There's a report that only fewer than 300 patients have received it,\" he said. \"It seems to be most effective early in the disease, especially before patients make antibodies of their own.\" \"We don't know if it'll be helpful, but it's something that is not unreasonable to try,\" Frieden said. Regeneron isn't the only company working on antibody therapies for the coronavirus. There are at least 70 different antibody treatments for Covid-19 under investigation. CNN's Jen Christensen and Amanda Sealy contributed to this story. \n",
            "Text: Questions remain on J&J Covid-19 vaccine rollout as authorization decision approaches  (CNN)Johnson & Johnson says that it has 4 million doses of its Covid-19 vaccine ready to ship \"immediately\" once it receives emergency use authorization, which could happen this week. But as the decision nears, it's unclear when or where those doses will be delivered. During a briefing on Wednesday, White House Covid coordinator Jeff Zients said the J&J vaccine's distribution would be similar to the current allocations process across jurisdictions, pharmacies and community health centers. When asked earlier in the week, the White House did not disclose its plans, although an administration official said they have worked closely with the company to formulate a distribution outline.    \"If authorized, we are ready to roll out this vaccine without delay,\" Zients said Wednesday. But federal officials have also expressed disappointment that fewer doses than expected will be ready. Zients said about 2 million doses will go to states after the vaccine is authorized, and the remaining amount will be allocated to pharmacies and community health centers.  States and professional associations say they would like some direction about the coming doses but they haven't received detailed information yet. One health industry source told CNN the lack of information has made it difficult for providers to prepare for the vaccine -- unsure if they should be building doses into their electronic medical records or educating their clinicians on how to administer the vaccine.   Still, some cities and states are making big plans for how they will administer the single-dose vaccine -- many focusing on their most vulnerable communities -- and they acknowledge they don't know how many doses to expect. \"I think Johnson & Johnson is going to be the difference maker because it's single dose, easier refrigeration standards. I think this is what will supercharge our effort,\" New York City Mayor Bill de Blasio said during a news briefing on Tuesday.  \"In terms of getting very precisely what we expect for New York City, I'm still waiting to hear that report,\" he added. \"But I'm very hopeful about this new development.\"  When the vaccine could be available Despite the excitement and anticipation, the Johnson & Johnson Covid-19 vaccine has several steps to go before it's available. The FDA's Vaccines and Related Biological Products Advisory Committee meets on Friday to review data on the vaccine and decide whether to recommend it for emergency use authorization in adults 18 and older.  Once the vaccine is authorized, the US Centers for Disease Control and Prevention Advisory Committee on Immunization Practices, or ACIP, makes recommendations to the CDC on how the vaccine should be used, such as among what age groups and on what type of schedule. An emergency meeting of ACIP is scheduled for February 28 and March 1.  Typically, the CDC director accepts the committee's recommendation. Shortly after that, the vaccine can be distributed, and can start going into arms. What everyone wants to know is where the federal government will send those doses, and who they'll be steered toward. \"We're hoping that ACIP will provide some guidance on the most effective use of the J&J one dose vaccine,\" Claire Hannan, executive director of the Association of Immunization Managers, wrote in an email to CNN on Tuesday. \"States are looking for guidance for use of the vaccine in populations which might really benefit from the one dose series complete, such as transient populations,\" Hannan said. \"We aren't hearing much on this.\" Biden administration waiting to disclose its plan Asked Monday about their plan to distribute the J&J vaccine, White House officials said they were waiting for FDA authorization.   \"So I think we're going to wait on that until after the FDA does its work and then after the ACIP does its work, all consistent with our philosophy of letting the science go first,\" Andy Slavitt, senior adviser of the White House coronavirus response, said during a briefing on Monday. \"Once we understand what the scientific community has to say, we will then be in a position to answer, I think, the very relevant questions that we need scientists' advice on.  And again, this is the President's directive to let science lead, and then we will rapidly follow that up with a strategy, both distribution and targeting and otherwise, that reflects that.\" The chief medical adviser to the White House, Dr. Anthony Fauci, said that it is possible the FDA may find details in the trial data that may give federal leaders a better sense of how the vaccine should be distributed. \"In the data there may be hints about just how this would be used most appropriately and most effectively. When we find that out, then the policy of the distribution would become much more scientifically based,\" said Fauci, director of the National Institute of Allergy and Infectious Diseases. While J&J has been manufacturing the vaccine as it was tested, there are still a limited number of doses.  A White House official confirmed this week, \"If an EUA is issued, we anticipate jurisdictions will get an allocation of around 2 million doses of J&J supply next week.\"  On Tuesday, Johnson & Johnson's Dr. Richard Nettles specified the company has 4 million doses ready to ship immediately, and it plans to \"deliver enough single-doses by the end of March to enable the vaccination of more than 20 million Americans,\" according to remarks delivered before a House Committee on Energy & Commerce subcommittee. The US government has ordered 100 million doses to be delivered by June, and the company says it can meet that commitment.  \"What we're seeing is that instead of being front loaded with a number of doses that are coming out, it very likely will be back loaded,\" Fauci said on CNN on Tuesday.  \"It's just a matter of what happened with their production capability and how they are now going to be revving up, and then soon after that, they're going to have a lot of doses,\" he said. \"But, it's not going to be front loaded.\"  'An orange with two apples' The J&J vaccine rollout was never going to be the same as the Moderna and Pfizer vaccines launch. It's different from the earlier mRNA vaccines and coming at a different time in the pandemic.  \"The Moderna and Pfizer were so similar,\" said Daniel Polsky, the Bloomberg Distinguished Professor of Health Policy and Economics at Johns Hopkins University. \"Now we have kind of an orange with two apples. So how do you manage anything? Can you give people choices? What do you send where and why and what are the implications of people getting those?\"  The earlier vaccines were the first; everything was new, and doses were more limited than they are now. They tested new distribution networks and required complex cold storage. Vaccines were directed toward the most vulnerable, going directly to hospitals and long-term care facilities.  Now there are more places to send vaccines and more people who are eligible to get them. The single-dose Johnson & Johnson vaccine developed by Janssen, the company's vaccine arm, should be easier to distribute. Polsky thinks Johnson & Johnson is doing its job. It created a vaccine and communicated what it does. Now, federal and local governments will need to \"connect the dots\" and determine where the vaccine fits best - and that could be bumpy for this vaccine and others, Polsky said. \"The J&J and whatever else follows, everything you add, adds to the complexity of distribution and delivery and communication and all that is, this exponential function of what you are managing,\" Polsky said. \"A state or agency, whoever has the authority to decide what goes where, how they make those decisions, I think that's going to be really complex. \"And come on, it doesn't even have an approval yet.\" Health officials already making plans Around the country, state public health departments were eager to be able to use the vaccine -- but thin on specifics of when they expect to receive doses and how many. The Texas Department of State Health Services said Monday it had not been given any information about the J&J allocation timeline or any other details about the vaccine. Arkansas' department of public health said Tuesday it is still in the planning stages right now, but won't have details to share until it knows more about the characteristics of the vaccine and it is authorized by the FDA.    Maryland Gov. Larry Hogan said Tuesday that White House officials suggested an EUA could happen as early as Friday and shipments of an initial 2 million doses could begin soon after. \"They didn't commit to next week but they said it could happen as early as next week,\" Hogan said, noting that Maryland typically receives about 2% of the nationwide vaccine supply. Some cities, such as New York, have plans in place for the Johnson & Johnson's Covid-19 vaccine rollout, but they still do not know how many vaccine doses they will be allocated. As soon as New York City receives doses of the vaccine, the city plans to start an initiative in which medical personnel will vaccinate homebound older adults in their apartments. The initiative is part of a larger effort to expand vaccinations for seniors. \"The big piece we want to get to is the, literally, in-home vaccination of folks who cannot leave their home. That really requires a Johnson & Johnson vaccine,\" de Blasio, the city's mayor, said during a news briefing on Tuesday. As a single-dose vaccine stored at refrigerator temperatures, the Johnson & Johnson vaccine may be easier to transport for such in-home vaccination efforts. De Blasio said that the Johnson & Johnson vaccine will make a difference in New York City, should it receive an emergency use authorization -- but there are still some questions left to answer, specifically around supply. \"Clearly supply continues to be the single most important issue and that supply is dictated by the federal government and then, of course, by the manufacturers themselves,\" Dr. Jay Varma, senior adviser for public health at the mayor's office, said during Tuesday's briefing.   \"The J&J vaccine is just being considered this week in front of the FDA, so we're looking forward to seeing the data -- both the data that's analyzed completely by J & J because all we have up until now was the press release, and second, the independent analysis that's done by the FDA,\" Varma said. \"The next big challenge becomes the one that we've been talking about all the time, which is supply, and of course we're kind of at the whims of what the federal government is able to allocate to New York City specifically, as well as of course what the manufacturers can actually produce themselves.\" In Colorado, the state hopes to make the Johnson & Johnson vaccine available at mass vaccination sites, said Brig. Gen. Scott Sherman, director of the state's unified coordination center vaccine task force. \"With the Janssen vaccine being a one-dose vaccine, it's good for those types of mass vaccination sites. We don't have to schedule people for a second clinic to come back and get a second dose,\" Sherman said. \"The beauty of putting this in our larger mass vaccination sites is that people will have the choice.\"   Sherman added that the ACIP report on how the vaccine will be recommended for use will help inform the state's planning. They'll also need to know the initial number of doses that will be allocated for the state and how soon those doses may arrive. That remains unclear. \"I don't have a whole lot of information on it. So it'll be interesting what the allocation is. It'll be interesting to know what the storage requirements are,\" Sherman said. \"And then what the quantities look like moving forward.\" CNN's Nadia Kounang, Amanda Sealy, Kaitlan Collins, Kristen Holmes, Maggie Fox and Nicholas Neville contributed to this report. \n",
            "Text: Our pandemic back-to-school supply list  (CNN)Pencils, pens, highlighters, paper, folders: Normally around this time of year, parents and kids would be stocking up on school supplies and scrambling to get ready to return to the classroom. This year, however, is anything but normal. In this year of the pandemic, schools and school districts in much of the country are kicking off the year with virtual education, a continuation of (and hopefully improvement upon) some of the remote learning capabilities rolled out in the immediate aftermath of shelter-in-place orders this spring.  Elsewhere, in states such as Texas and Florida, schools are reopening with new rules and regulations — protocols that embrace social distancing yet seriously transform day-to-day experiences for students. To be clear, stark socioeconomic differences across the country mean that different families have vastly different needs for supplies — some kids need nothing more than headphones to listen to virtual classes on Zoom, while others require basic Wi-Fi, pens and a quiet space in a crowded apartment to work. Still, the needs are real. We interviewed parents, teachers and administrators for insights about which supplies would be most helpful to the greatest number of kids given the current reality of pandemic-era education in America. Here are some of their suggestions. The basics  While Covid-19 has changed the way we socialize, recreate and dine out of the house, it hasn't changed the most basic list of stuff kids need to do school right. This means parents should stock up on staples such as pens, pencils, crayons, markers, paper, erasers and scissors — both for kids who will be learning from home and for those who will be learning from school.  Ashley Fry, a sixth grade math teacher at West Jackson Middle School in Hoschton, Georgia, recommended that kids who will attend any in-real-life instruction should keep these items in a washable pouch. She knows that sharing likely won't be allowed. \"Middle schoolers are known for forgetting supplies, which is normally fine because they just borrow from each other or from their teacher,\" she said. \"Now, however, it will be potentially dangerous to do so with the current pandemic going on.\" The new basics  Two other must-haves for school in 2020: hand sanitizer and face coverings.  While these items are more important for students attending school outside the home, they're important for virtual learners, too, since these kids will spend parts of their days out and about. Students should buy or make 10 to 15 masks they are comfortable wearing, Fry recommended, so they can have two or three face coverings available every day. That's because masks can get sweaty or dirty, and kids need to have a backup mask, just in case. \"Having (multiple) masks will make it so parents don't have to wash masks each evening,\" Fry said. Mask insurance  Having masks is one thing; making sure they don't fall off is another. This is particularly an issue for little kids — the US Centers for Disease Control and Prevention advises that children 2 and older wear masks in settings where it's difficult to practice social distancing. Lanyards or other cords to keep masks around a child's neck are good solutions to this problem, said Elizabeth Goldman, a fourth grade teacher at Jackson Avenue School in Mineola, New York. \"(A lanyard) keeps the mask close enough and affords easy access in case of any sudden social distance deviation, which is guaranteed with children,\" Goldman wrote via text message. \"It also prevents the mask from falling on the floor and getting dirty, in addition to keeping the mask within (the child's) personal space.\" Parents can even leverage the opportunity to spark creativity, Goldman added. \"I would imagine the mask lanyard could also be a means for easing kids' anxieties about wearing masks; they could be decorative and personalized, which basically makes any item fun,\" she said. Confirming connectivity  With a significant amount of learning happening online, it's important for students to be able to access the internet from home. This makes connectivity — reliable Wi-Fi or hard-wired Ethernet — a necessity. The problem? Not all students in America have access to the internet outside of school. One in five parents said it was likely that their child or children would have to use public Wi-Fi to complete classwork because they didn't have reliable connectivity at home, according to an April 2020 survey by Pew Research Center in Washington, D.C. The findings were even more salient for lower-income parents, who reported that 40% of their kids would have to use public Wi-Fi, said Monica Anderson, Pew's associate director of research. \"Unfortunately, even in 2020, internet is not a given for everyone,\" said Anderson, who noted that more than 4,900 respondents participated in the survey. To solve this dilemma, many schools and school districts are doling out Wi-Fi hotspots to families in need or setting up makeshift access points at various positions in the community. Other institutions are leveraging technologies that would enable students to complete schoolwork on smartphones over the cellular network. One device per child  Still other schools and school districts are focusing on the devices themselves, equipping students with mobile devices that also are critically important to putting students in the best position to succeed. Many schools and school districts have programs through which they provide a device to every student for free. Some, however, do not. Lori Lyn is a second grade teacher at Hicks Elementary School in Houston, where every child gets a laptop. She noted that families of students who don't receive free devices from their schools should try everything possible to obtain one for each child to use as his or her own. Devices such as Kindles, smartphones and many tablets simply don't have the computing power necessary for virtual education, Lyn added. \"We don't want them to be sharing with parents or amongst each other,\" Lyn explained. \"As difficult as it might be for some families to manage, if you're a family with four kids, it's crucial that each kid has their own device.\" Tune in by tuning out  Another item that many teachers consider to be a must-have for the coming school year: headphones. Particularly for students who must endure at least another semester of virtual learning, these tools will enable kids to log on to class sessions, tune out distractions and focus on the lessons at hand. Mark Kirlough, who teaches engineering at Lower Richland High School in Hopkins, South Carolina, said the headphones don't have to be expensive, so long as they work with the requisite devices and get loud enough to tune out other sounds. \"If you have siblings or if your parents are home working, distractions can be a real problem,\" he said. \"No matter what the situation is in a student's house, (headphones) are one way to help them focus.\" A place to work  Families with enough space and resources also may want to consider getting each child a desk. For Las Vegas resident Stacy Hamilton, a single mother in Las Vegas, this was priority No. 1 for the summer. In the spring, when the city's schools shut down, she set up a work area for her 12-year-old daughter and 10-year-old son at the kitchen table. Hamilton, who was working from home, used the same spot as her office. In July, Hamilton purchased a stand-alone desk for her son. She said she plans to put it and a desk for her daughter in a separate room that will effectively become their classroom until they can return to their respective schools. \"I wanted to get something that was functional for the current moment but could then get moved to his room so he can use it for years to come,\" she said. List the day  Parents said another strategy for putting at-home learners in a position to succeed is by listing the daily schedule and objectives on a whiteboard or flip-chart paper that is easily visible from wherever students plan to complete their work. Jill Murphy, a resident of Sonoma County, California, said she deployed this strategy in the spring, and it gave everyone in her family the opportunity to get on the same page about how each day would go. \"We all need to be on the same page regarding when math or humanities Zoom classes are starting versus when we've agreed on video games or outdoor time,\" said Murphy, who has a 12-year-old son. \"Otherwise it's a constant negotiation, particularly when both parents are working and trading off as remote school 'supervisors.'\" Murphy added that her plan for the fall is to buy a whiteboard to hang in the corner of the family room. Air purifier  Finally, some parents said they are considering purchasing air purifiers to help protect their kids while they learn from home during the coming school year. These devices work to remove most contaminants from the air in a room. While none of the tools have been proven to eliminate coronavirus, many of them have received positive ratings from the US Environmental Protection Agency, and work to improve overall air quality by a significant percent.  For Dana Freeman, who lives in Burlington, Vermont, any improvement to the air her kids breathe is worth the investment. \"I'm just trying to do everything I can to help protect my kids,\" said Freeman, who purchased one purifier for each of her children to take to their respective college apartments. \"If a portable air purifier helps to keep the air in their rooms a little cleaner, then I am all for it.\" Matt Villano is a freelance writer and editor based in Northern California. \n",
            "Text: Cats can infect other cats with coronavirus, researchers find  (CNN)Cats can infect other cats with the novel coronavirus, but they may not show any symptoms, according to a study published Wednesday.  The findings add to a growing body of research showing how cats, big and small, can contract the virus. But experts say there is no evidence felines are contributing to the spread of Covid-19. In the study, researchers infected three cats and found that all of them were shedding the virus after three days. When the infected cats were paired with healthy ones -- housed together for a few days -- the healthy cats developed the virus, too. None of the animals in the study showed any symptoms such as abnormal temperatures or substantial weight loss. The first two cats in the United States to test positive -- both New Yorkers -- showed mild respiratory symptoms, although both were expected to make a full recovery. Eight lions and tigers at the Bronx Zoo also tested positive. The team behind the new study, led by international virus expert Yoshihiro Kawaoka of the University of Tokyo and scientists from the University of Wisconsin, said more research is needed to better understand whether cats could transmit the virus to humans as well. So far, there is no evidence that they can. \"Given the need to stop the coronavirus disease 2019 pandemic through various mechanisms ... a better understanding of the role cats may play in the transmission of SARS-CoV-2 to humans is needed,\" they wrote in a letter to the New England Journal of Medicine. The American Veterinary Medical Association noted that the new research was conducted in a lab -- and it's unclear whether cats can be as easily infected in the real world. A small number of animals worldwide are confirmed to have the virus, the group said in a statement. \"There is no evidence to date that these relatively few naturally infected animals have played any role in transmitting COVID-19 to humans,\" it said. While officials are still learning more about coronavirus and pets, the US Centers for Disease Control and Prevention recommends limiting interactions between pets and people or animals outside of a household. Keep cats indoors whenever possible, the CDC says, and walk dogs on a leash, maintaining a distance of at least six feet from other people and animals. The agency also recommends keeping dogs away from public places where large numbers of people and animals gather, such as dog parks. If someone is ill with Covid-19 -- whether suspected or confirmed -- officials recommend another member of the household care for pets. If that's not possible, people should wear cloth face coverings around animals, making sure to wash their hands before and after any interactions. And when people are sick, officials say they should refrain from petting or snuggling with their pets -- and avoid being kissed or licked by them.  \n",
            "Text: Covid-19's effects include seizures and movement disorders -- even in some moderate cases, study finds  (CNN)Covid-19 can lead to neurological complications, including strokes, seizures and movement disorders, researchers have found. The complications, which go well beyond cognitive impairment, can occur even in moderate cases, according to a study published Wednesday in the journal Neurology: Clinical Practice. \"These particular complications of Covid, and neurological disorders more generally, are about your ability to interact meaningfully with the world,\" said lead study author Dr. Pria Anand, an assistant professor of neurology at Boston University School of Medicine.  \"I think that's one of the unique and devastating things about this (virus).\" A range of dysfunction While Covid-19 can cause headaches and altered mental status in a large number of patients, there are also more serious complications that can require specialized management by neurologists, the study found.  The current study looked at data from 921 adults hospitalized at Boston Medical Center for Covid-19 from April 15 to July 1, with 74 requiring examination by a neurologist. The patients involved in the study were  average age of 64 years, and 47 already had a history of neurologic disease. Focusing on patients with existing neurological manifestations, the researchers found that 18 had been admitted to the hospital due to strokes after contracting Covid-19, 15 had seizures and 26 had a form of brain dysfunction causing confusion and delirium. Another seven had movement disorders, and three patients had suffered traumatic brain injuries related to falls at home after coming down with Covid-19. Ten of the people in the study group died, from causes including multi-organ failure. Many of the survivors who had entered the hospital with a mild neurological disability were discharged with moderately severe disability, the study found. A study in a low-income community One important aspect of the study was in portraying how Covid-19 and its aftermath impact low-income areas and communities of color, said Anand, who is a member of the American Academy of Neurology. Boston Medical Center, which is located in the city's South End, serves a neighborhood where over half of patients live in households with income less than $25,000, two-thirds are racial or ethnic minorities and 72% of hospital visits are by low-income or elderly patients who rely on government-provided insurance coverage. \"We were in the unique position of caring for patients who were disproportionately affected by Covid-19 and just wanted to paint a fuller picture of what the neurologic complications are in this critical set of patients,\" Anand said. During the time covered by the study, the hospital had the second-highest Covid-19 case count of any hospital  in Massachusetts, a state that itself was third in the nation for overall cases and cases per capita. \"Neurological complications of Covid-19 may arise from direct effects of the virus, but are more typically a reflection of the nervous system's response to the infection,\" said Dr. Orly Avitzur, the president-elect of the American Academy of Neurology and editor in chief of the organization's Brain & Life magazine, via email. She was not involved in the study. For instance, low oxygen levels, which are common in Covid-19 patients, can cause encephalopathy, or brain damage that could prove permanent, she explained. And those with severe cases of Covid-19 cases can have surges of immune signaling proteins called cytokines, which can cause alteration of consciousness of confusion with effects lasting months, if not longer, after the initial infection. Avitzur noted that the study at Boston Medical Center only analyzed neurological patients during their hospitalization or 30 days prior to admission, so future research is required to assess the long-term consequences of these effects on Covid-19 survivors.  \"With nine months into this pandemic, we cannot yet predict the course of the long-term effects,\" she said. \"We are likely to be gathering these data for years to come.\" She cited a similar study published by Spanish researchers in August showing that 57% of those with Covid-19 experienced at least one form of neurologic symptom, including myalgias, headaches and disorders of consciousness. Disorders that change you as a person Of the 64 survivors with neurological complications that were discharged from the hospital in the Boston study, 27 of them returned home. Some 20 patients required ongoing care at skilled nursing facilities, including 11 who had been living at home prior to hospitalization. Another nine were sent to acute rehab centers, three needed care at long-term acute hospitals and five patients entered hospice care. \"A lot of patients had Covid-caused complications of critical illness systemically,\" Anand said. \"People's journey is not over when they leave the hospital.\" And it's unclear how long these alterations to their lives might last. \"Beyond just the question of surviving or not surviving, there's this kind of vast grey area of when do you get to be the person you were before you were ill?\" Anand said.  It's unclear whether some will get their life as they knew it back again. \"For so many people, that's just as important as the question of survival or not,\" she said.  \n",
            "Text: You can't hide your stress from your kids, study says  (CNN)Parenting in the age of coronavirus is packed with stress. After all, you're juggling home schooling, work (or the lack of it) and the new realities of social distancing and isolation while trying to keep your family healthy and safe. But if you're trying to hide that stress from your children -- even with the best intentions of protecting them from the pressure -- it's not going to work, according to a new study published Wednesday in the journal Family Psychology. \"If you're stressed and just say, 'Oh, I'm fine,' that only makes you less available to your child,\" said study author Sara Waters, an assistant professor in the department of human development at Washington State University. We found that the kids picked up on that and reciprocated, which becomes a self-fulfilling dynamic,\" Waters said in a statement. \"These are fascinating findings about the way our bodies' physiology links up with our children's -- for good or bad,\" said Dr. Jenny Radesky, a developmental behavioral pediatrician who teaches at the University of Michigan.  \"These are the unspoken ways that children feed off of us, sense our emotional states, and react emotionally themselves,\" she said, which are \"incredibly relevant\" to stress from today's Covid-19 pandemic. \"There are so many things about Covid-19-related stresses that we may not want to express to our children, such as worries about relatives' health or finances,\" Radesky added.  \"These results suggest that stifling emotions don't get rid of them -- they stay under our skin in the form of changes in our heart and nervous system functioning,\" she said. \"And as most parents know, they can pop out later in the form of irritability, overreacting to our children or yelling.\" What lies beneath The study put sensors on the bodies of 107 parents, nearly half of whom were dads, and their 7 to 11-year-old children. Parents were asked to list five topics of frequent conflict with their kids, and then they were given activities designed to create stress, such as public speaking. Once back in the room with their child, parents were asked to talk about one of the family's top conflicts: Half of the 107 parents were asked to suppress their feelings of stress; the other half were told not to do so. Interactions were recorded on video and scored by reviewers who did not know which group the parents had been assigned. Results showed that when parents repressed their stressful feelings, both the parents and the children were rated as \"less warm\" and \"less engaged\" with each other. \"That makes sense for a parent distracted by trying to keep their stress hidden, but the kids very quickly changed their behavior to match the parent,\" Waters said.  In fact, sensors on the child's body recorded a physical response when the parent hid their emotions, the study found. \"The response happens under the skin,\" Waters said. Differences in parent-child dynamics One interesting finding was a difference between mothers and fathers. When moms were told to hide their emotions, their children showed even more signs of stress on the physiological sensor and in their outward behavior. That didn't happen with dads, however. \"We were looking for a physiological response, but there wasn't one in either the control or the experimental condition where dads transmitted stress to their kids,\" Waters said. Perhaps that's because dads may suppress emotions around their children more than mothers do, Waters said.  \"The kids have experience with their dad saying things are fine even when they're not,\" she said. \"But it was more abnormal for kids to see their mom suppressing their emotions, and they reacted to that.\" Don't guilt yourself over stress The takeaway from the study for parents, Waters said, is to not stress about your stress. \"Honor your feelings and your child's feelings,\" she said. \"Kids will work their way through it; they're good at it. Giving yourself permission to feel opens up your mind to more and better problem solving. It's a good thing.\" Radesky agreed: \"It's important for parents to know that this isn't meant to 'blame' them, but to give them the power to know that children can feed off of parents' emotional state in positive ways, too.\"   \"When we cope in a healthy way, they see us,\" she said. \"When we express our emotions and show that they are mentionable and manageable, as Mister Rogers said, we teach them that they can do the same. \"  \n",
            "Text: US tops 1,000 coronavirus deaths 4 days in a row as experts urge the country to shut down  (CNN)As more Covid-19 records get broken, debates on whether to send kids back to school or to shut down the economy again are coming to a head. More than 1,000 people died every day for four straight days last week due to Covid-19. That brings the total US death toll  from the virus to more than 146,000 as of Sunday, according to data from Johns Hopkins University.  And researchers project up to 175,000 deaths linked to the virus by August 15, according to an ensemble forecast published by the US Centers for Disease Control and Prevention.  With overwhelmed hospitals and lengthy delays in testing, some local leaders -- including Houston Mayor Sylvester Turner and Los Angeles Mayor Eric Garcetti -- said a second stay-at-home order might be possible. That kind of drastic measure is supported by more than 150 prominent medical experts, scientists, teachers, nurses and other experts who signed a letter urging leaders to shut the country down and start over to contain the rampant spread of the virus.  \"Right now, we are on a path to lose more than 200,000 American lives by November 1st. Yet, in many states people can drink in bars, get a haircut, eat inside a restaurant, get a tattoo, get a massage, and do myriad other normal, pleasant, but non-essential activities,\" read the letter, which was sent to the Trump administration, members of Congress and state governors.  What we can learn from states that got Covid-19 under control  In the past two days, these states broke records:  As the country's caseload and death toll climbs, at least four states reported record-breaking numbers since Friday.  California, which is leading the nation with the most recorded coronavirus cases, reported 159 deaths linked to the virus Friday -- the highest number recorded in a single day since the start of the pandemic. More than half of all virus-related deaths in the state come from Los Angeles County, where more than 4,260 deaths have been reported. The state has had more than 446,450 reported infections, according to Johns Hopkins.  Georgia also broke a new single-day record Friday, reporting at least 4,813 new coronavirus cases. Health officials reported 3,787 new cases Saturday. More than 165,180 people have tested positive in the state, according to Johns Hopkins.  Oregon reported nine new coronavirus-related deaths Friday, breaking its record for most reported fatalities in a single day since the pandemic began. Health officials in the state reported 396 new cases, bringing the state's total to more than 16,100.  For the second day in a row, Hawaii reported a record number of new cases, identifying 60 new positive tests Friday, according to health officials. On Thursday, Hawaii reported 55 new cases. the state has had at least 1,620 reported infections, according to Johns Hopkins. Track the virus in your state and across the US Florida's youngest victim was a 9-year-old girl In Florida, a state that's broken its record of new cases several times in recent weeks, Covid-19 hospitalizations have jumped by nearly 80% since July 4, according to data from the state's Agency for Health Care Administration. Across the state, at least 50 hospitals have no ICU beds available. Last week, a 9-year-old girl with no pre-existing conditions became the state's youngest coronavirus victim. Kimora \"Kimmie\" Lynum was taken to a local hospital to treat a \"very high\" fever, her family said. Kimmie's cousin and family spokesman Dejeon Cain said the hospital told the family to return home. After doing so, the young girl complained of not feeling well and collapsed, Cain said. She didn't have a detectable heartbeat.  Her family said they don't know how Kimmie contracted the virus, as she had appeared healthy and had spent the summer at home.  Texas hospital could send patients home to die In Texas, which broke a record for virus-related deaths last week, doctors inside one hospital may decide to send patients home to \"die by their loved ones\" due to limited resources, officials said.  Officials in Starr County announced they're creating committees to review patients' cases at the Starr County Memorial Hospital -- where at least 50% of patients admitted to the hospital's emergency room have tested positive for coronavirus.  \"Unfortunately, Starr County Memorial Hospital has limited resources and our doctors are going to have to decide who receives treatment, and who is sent home to die by their loved ones,\" Starr County Judge Eloy Vera wrote in a Facebook post on Thursday. \"This is what we did not want our community to experience.\" Earlier this month, the governor ordered a statewide mask mandate and urged residents to heed the precautions in order for businesses and the economy to remain open.  As the state grapples with coronavirus, Gov. Greg Abbott also had to issue a disaster declaration for 32 counties after Hurricane Hanna made landfall Saturday evening.  CDC in favor of reopening schools  Many teachers and staff members across the US have strongly opposed returning to classrooms next month as the virus runs uncontrolled across American communities.  But some parents -- including some in a Georgia county where coronavirus numbers are among the highest in the state -- are demanding schools reopen.  President Donald Trump has said he's putting pressure on governors to reopen schools in a push to get the US back to business as usual.  In new guidelines posted last week on education and childcare, the CDC came down hard in favor of reopening schools, saying children don't suffer much from coronavirus and are less likely to spread it.  But research from South Korea found that older children (between the ages of 10 and 19) can transmit the coronavirus within a household just as much as adults.  The CDC guidelines recommended local officials consider closing schools -- or keeping them closed -- if there is substantial, uncontrolled spread of the virus.  \"It is critically important for our public health to open schools this fall,\" CDC Director Dr. Robert Redfield said.  \"School closures have disrupted normal ways of life for children and parents, and they have had negative health consequences on our youth. CDC is prepared to work with K-12 schools to safely reopen while protecting the most vulnerable.\" Your top coronavirus questions, answered CNN's Andy Rose, Nicole Chavez, Kay Jones, Alta Spells, Sarah Moon, Rosa Flores, Denise Royal, Natasha Chen and Chandler Thornton contributed to this report.  \n",
            "Text: Technology companies can help fight Covid-19 Dr. Farzad Mostashari is co-founder and CEO of Aledade. He is former National Coordinator for Health IT at the Department of Health and Human Services. Dr. Tom Frieden, former director of the US Centers for Disease Control and Prevention, is currently president and CEO of Resolve to Save Lives, a global non-profit initiative that is part of the global public health organization Vital Strategies. The views expressed in this commentary are solely those of the authors.  (CNN)The Covid-19 pandemic is a global, all-hands-on-deck moment. Every day, it asks a great deal from all of us, from policymakers and epidemiologists to health care professionals to every person who stays at home to protect others. It's forcing us to find new ways to work together to flatten the curve, protect our health care workers, strengthen public health and bring us all closer to the end of this crisis. It's only natural that the biggest tech companies, including Amazon, Apple, Google, Facebook and Microsoft, are stepping up as well, not only by donating to Covid-19 relief efforts, but also by harnessing their unique ability to rapidly solve problems and scale solutions. In the pandemic, the commitment to apply technology solutions to societal problems can help save lives and restore the economy, if it is channeled into the right solutions. That means these tech titans should focus on their expertise and share what they know. They should provide solutions that support our existing public health infrastructure. And most fundamentally, they should know when not to step up, sometimes resisting the temptation to help in order to follow that age-old promise in the Hippocratic oath, do no harm -- especially to the public trust that is crucial for effective response to the pandemic. Crises can provide fertile soil for a bias toward action, even when an action isn't needed. In a global pandemic, that bias often takes the form of an overreaction of surveillance. Ideas such as using apps for proximity-based automated contact tracing, as attempted in China and South Korea, and as Google and Apple have begun to explore, make assumptions about uptake and accuracy that are implausible. Worse, to make them more feasible could mean making participation hidden or coercive, threatening the very trust that an effective and democratic public health system needs. We've learned that lesson of surveillance too many times in the post-9/11 world. Proximity-based tracing applications may have some role supporting comprehensive contact tracing programs in the future, but only if they avoid over-promising, address privacy and feasibility barriers, and demonstrate accuracy and value. What tech companies can do is navigate the waters they know and focus on what they do best. Facebook, Twitter and Google, for example, have become not just ways to stay connected to people -- especially important in the midst of physical isolation -- but also places people turn to for information. That's why we were pleased to see these companies quickly elevate information from trusted sources to the top of news feeds and search pages, and for YouTube to aggressively take down harmful misinformation. Pinterest, Apple and others have released apps that facilitate health education and self assessment. There is still room for improvement, but these have been promising steps. For the next steps, tech companies can help by telling us what they've learned. The expertise tech companies bring to the fight against Covid-19 is powered by data insights that could be transformational if opened to the public. Google and Facebook, for example, recently announced their work to make Community Mobility Reports and a Mobility Dashboard available to the public, providing de-identified, aggregate data on how communities are responding to shelter-at-home orders and physical distancing. Big tech is learning more and benefits us all by sharing what it knows freely and publicly. These companies can also help by empowering us to do our jobs better. Not by creating something new, but by empowering the people who have been doing this work for decades. Carnegie Mellon University's Delphi epidemiological research center worked to develop a way to forecast where Covid-19 may hit next. They searched for a survey tool broad enough to launch their new symptom mapping project and found a partner in Facebook, which opened the survey to users of its platform. With this massive community of users, these epidemiologists were able to connect with a wealth of information that can inform our responses on the front lines and help us prepare for and possibly prevent future outbreaks. As epidemiologists, we are enthusiastic about this new data feed. It has the potential to serve as a crucial early warning system that can help save lives and preserve economic activity. One of our most important public health activities moving forward will be contact tracing. Apple, Google and Microsoft can contribute by helping public health workers do their job more effectively. Partnerships between our public health experts and our nation's leading technologists can be transformational in ways that will continue improving our health long after Covid-19 is no longer the emergency it is today. We are at a unique moment in human history, but not just because of this pandemic. Outbreaks of disease have left their scars throughout history. What makes this time unique is that we have, at our fingertips, a wealth of information, data and insights that would have been unimaginable in pandemics of the past. If we and the companies that hold this data use it well by building on expertise, sharing de-identified data widely, supporting public health and focusing on doing no harm, we can turn a new chapter in how humanity fights these invisible viral enemies with more interconnected human intelligence.  \n",
            "Text: FDA authorizes first rapid Covid-19 self-testing kit for at-home diagnosis  (CNN)The US Food and Drug Administration has issued an emergency use authorization for the first self-test for Covid-19 that can provide rapid results at home. The Lucira COVID-19 All-In-One Test Kit is a molecular single-use test available by prescription for self-diagnosis of the coronavirus, the agency said Tuesday. The rapid test utilizes a molecular amplification technology to detect the virus in people with known or suspected Covid-19 and can return results in 30 minutes, the FDA said. A molecular Covid-19 test searches for signs of the coronavirus genetic material. \"While COVID-19 diagnostic tests have been authorized for at-home collection, this is the first that can be fully self-administered and provide results at home,\" FDA Commissioner Dr. Stephen Hahn said in a statement. The new test, which uses self-collected nasal swab samples, is authorized for people 14 and older with suspected Covid-19 and people under 13 when performed by a health care provider. It is also authorized for use in point-of-care settings, such as doctor's offices, hospitals, urgent care centers and emergency rooms for all ages but must be collected by a health care provider, the FDA said. \"This new testing option is an important diagnostic advancement to address the pandemic and reduce the public burden of disease transmission,\" Hahn added. Health and Human Services Secretary Alex Azar agreed. \"Making it possible for Americans to do their own rapid COVID-19 self-test at home by prescription is the latest addition to our constantly expanding arsenal of COVID-19 testing options,\" Azar said in a statement. But some health experts urged caution. \"The data is just still emerging, Tom Bollyky, the director of the global health program and senior fellow for global health, economics, and development at the Council on Foreign Relations, told CNN. \"Obviously with some past emergency use authorizations it pays to be cautious with what the FDA has put out here, but it's certainly a promising sign.\"  The self Covid-19 home test could be one more step that gets the country closer to a new normality. \"Expanded testing, rapid testing really could be one more tool that brings us back to a life that approaches more normalcy. If we're able to identify at least on, perhaps even a daily basis, what our current viral load is and it helps us make ourselves safer and others,\" he said. The new self-testing kit includes a sterile swab, a sample vial, a test unit, batteries and a plastic disposal bag. The sample collected on the nasal swab is inserted into the vial which then enters the test unit where it is analyzed. The results are displayed on the test unit by a color change in the LED indicators, according to the FDA. Instructions on how to use the test are included with the prescription.  \n",
            "Text: Health care workers and long-term care facility residents should get Covid-19 vaccine first, CDC vaccine advisers say  (CNN)Vaccine advisers to the US Centers for Disease Control and Prevention voted 13-1 on Tuesday to recommend that both health care workers and residents of long-term care facilities be first in line for any coronavirus vaccines that get emergency authorization from the US Food and Drug Administration.   The Advisory Committee for Immunization Practices voted to include both groups in what they're calling Phase 1a of the CDC's coronavirus vaccine distribution plan.  They are at \"exceptionally high risk,\" Dr. Jose Romero, who chairs ACIP, said.   \"Long term care facility residents are defined as adults who reside in facilities that provide a variety of services, including medical and personal care, to persons who are unable to live independently,\" the CDC said.   These very frail people account for 40% of coronavirus deaths in the US and the ACIP committee members felt strongly they need to be protected. So far, the CDC's Sara Oliver told the meeting, 100,000 long term care facility residents have died from Covid-19.  ACIP members also agreed it would be efficient to vaccinate the staff working in nursing homes and similar long term care facilities and the residents at the same time.  And no one had doubts about the need to protect health care workers.  More than 240,000 health care workers have been infected with coronavirus and 858 have died, the CDC says. \"Anybody that works within a health care institution that could have contact with an individual who has Covid should receive vaccination,\" Romero  told CNN before the meeting started. \"That includes individuals such as the persons delivering food, those persons in housekeeping who rapidly turn over rooms in the emergency room or who perform cleaning in the patient's rooms. Those individuals will be included.\"  Just before the vote, the American Health Care Association and National Center for Assisted Living released a report showing nursing homes have recorded their highest weekly count of coronavirus cases since the spring. \"Given the asymptomatic and pre-symptomatic spread of this virus combined with the explosion of community spread across the U.S., we are extremely hopeful this vaccine will literally be a lifesaver for thousands of residents and expedite the reopening of our facilities to family members and loved ones,\" Mark Parkinson, president and CEO of AHCA/NCAL, said in a statement after the vote. The single vote against the recommendation came from Dr. Helen Keipp Talbot of Vanderbilt University, who said she was worried that the vaccine had not been studied in residents of long-term care facilities. \"We hope it works and we hope it's safe. That concerns me on many levels,\" Talbot told the meeting.  Later, she added: \"I have no reservations for health care workers taking this vaccine.\"  Early data on the Pfizer and Moderna vaccines suggest both are safe and highly effective, with each preventing 95% of symptomatic infections in the people who have volunteered to test them according to the companies. \"Our discussions have been transparent and our motives have been clear,\" Romero said after the vote. \"We see the growing number of health care providers that have become infected, some of which have, unfortunately, passed away,\" added Romero, who is secretary for health for the Arkansas Department of Health.  \"I believe my vote reflects maximum benefit, minimum harm, promoting justice and mitigating that health inequalities that exist with regard to distribution of this vaccine.\" Dr. Robert Atmar of the Baylor College of Medicine said he initially had qualms about putting long-term care facility residents in the first group. \"Ultimately, I was persuaded by the tremendous burden in terms of mortality and hospitalizations that the residents of these facilities bear, the remarkable efficacy that has initially been reported and that ultimately we will have a chance to review ... and the plans for monitoring of safety in this population and the extra mile that will be pursued to make sure that the residents and their families will be fully informed about the amount of evidence that is available before the residents receive the vaccine.\"  The next meeting will come after the FDA's advisers, known as the Vaccines and Related Biological Products Advisory Committee or VRBPAC, meet on December 10 to decide on Pfizer's application for an EUA, said ACIP's executive secretary, Dr. Amanda Cohn. \"We anticipate that the next ACIP meeting will occur sometime after the VRPBAC meeting,\" Cohn said. ACIP will vote to recommend whether any vaccine the FDA authorizes should actually be given to anyone in the United States. The CDC and ACIP are considering a four-phase plan for allocating vaccines eventually. Phases 1b and 1c will likely include essential workers such as food production workers who are at high risk of infection, as well as emergency personnel and perhaps people at highest risk of coronavirus complications and death. The federal government anticipates that 40 million doses of vaccine could be available in the United States by the end of December if both the Pfizer and Moderna vaccines are approved.   But all 40 million doses would not be available right away, Oliver told Tuesday's meeting. \"We expect a constrained supply environment,\" Oliver said. Oliver said the CDC expects between 5 million and 10 million doses will become available each week for the first few months as vaccine makers ramp up manufacturing.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "# Tokenize data\n",
        "training_data_tokenized = []\n",
        "for data in training_data:\n",
        "  tokenized_text = tokenizer.encode(data)\n",
        "  training_data_tokenized.append(tokenized_text)\n",
        "\n",
        "print(len(training_data_tokenized))\n",
        "print(len(training_data_tokenized[0]),training_data_tokenized[0][:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wiNKGMrekIx",
        "outputId": "942147b6-35d9-4af1-d411-d92c4fb96771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1611 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n",
            "593 [31439, 64, 13423, 12319, 481, 307, 14153, 1028, 17670, 11, 475, 481, 1332, 47949, 284, 2987, 16989, 220, 357, 18474]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into blocks\n",
        "training_chunks = []\n",
        "block_size = 100\n",
        "for tokenized_text in training_data_tokenized:\n",
        "  for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
        "      training_chunks.append(tokenized_text[i:i + block_size])\n",
        "\n",
        "# Generate inputs and labels\n",
        "inputs = []\n",
        "labels = []\n",
        "for ex in training_chunks:\n",
        "    inputs.append(ex[:-1])\n",
        "    labels.append(ex[1:])\n",
        "\n",
        "print(\"inputs length:\",len(inputs))\n",
        "print(\"labels length:\",len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gabMwrVeesf1",
        "outputId": "b549c8f1-53c7-4080-a040-b2466c32a692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs length: 3788\n",
            "labels length: 3788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"input:\",len(inputs[0]),inputs[0][:20])\n",
        "print(\"labels:\",len(labels[0]),labels[0][:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7om_xVjewug",
        "outputId": "d88c0c5d-3c4f-48ab-9906-b59d536b3d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: 99 [31439, 64, 13423, 12319, 481, 307, 14153, 1028, 17670, 11, 475, 481, 1332, 47949, 284, 2987, 16989, 220, 357, 18474]\n",
            "labels: 99 [64, 13423, 12319, 481, 307, 14153, 1028, 17670, 11, 475, 481, 1332, 47949, 284, 2987, 16989, 220, 357, 18474, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 12\n",
        "TRAIN_SHUFFLE_BUFFER_SIZE = len(inputs)\n",
        "\n",
        "# Create TF Dataset\n",
        "train_data = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "\n",
        "#############\n",
        "# Train data\n",
        "#############\n",
        "train_data = train_data.shuffle(buffer_size=TRAIN_SHUFFLE_BUFFER_SIZE)\n",
        "train_data = train_data.batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_data = train_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"train_data\",train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HInr8mweyEp",
        "outputId": "5496be07-a3e2-4eb5-9f10-adf76b08bdd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data <_PrefetchDataset element_spec=(TensorSpec(shape=(12, 99), dtype=tf.int32, name=None), TensorSpec(shape=(12, 99), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelWithLMHead\n",
        "############################\n",
        "# Training Params\n",
        "############################\n",
        "learning_rate = 3e-5\n",
        "epsilon=1e-08\n",
        "clipnorm=1.0\n",
        "epochs = 30\n",
        "\n",
        "# Free up memory\n",
        "K.clear_session()\n",
        "\n",
        "# Build the model\n",
        "model = TFAutoModelWithLMHead.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "# Print the model architecture\n",
        "print(model.summary())\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon, clipnorm=clipnorm)\n",
        "# Loss\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "# Compile\n",
        "model.compile(loss=[loss, *[None] * model.config.n_layer],\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=[metric])\n",
        "\n",
        "# Train model\n",
        "start_time = time.time()\n",
        "training_results = model.fit(\n",
        "        train_data, # train_data.take(1000) for testing\n",
        "        epochs=epochs,\n",
        "        verbose=1)\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Training execution time (mins)\",execution_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "256db42191074e088700bcd91ab6806e",
            "9bff89d6196d4b2fa03b9b7a4bc1fe11",
            "8662c747c5c245f0be04920157220207",
            "c350e8fd93d94ec98176d3478b3ee466",
            "cacf2894023c40d096973f4cacb3c362",
            "34adce56001c41fabfb4d1716b429f06",
            "89ff48a748ad4492b60ef261226cebed",
            "3e1260c562574a878e5b9b12d015b017",
            "36e8e74970ba41efa4264e20577c4984",
            "fa2f0350c6444083bbdff7f54b6e3a92",
            "da68a6387c9c4986b396fcc01cf42751"
          ]
        },
        "id": "jqTQ095fezco",
        "outputId": "c3486ea9-cd28-4c57-d45d-ae9dd6df079e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/modeling_tf_auto.py:721: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "256db42191074e088700bcd91ab6806e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLay  multiple                  81912576  \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 81912576 (312.47 MB)\n",
            "Trainable params: 81912576 (312.47 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "315/315 [==============================] - 121s 328ms/step - loss: 3.5611 - accuracy: 0.3340\n",
            "Epoch 2/30\n",
            "315/315 [==============================] - 102s 325ms/step - loss: 3.3356 - accuracy: 0.3553\n",
            "Epoch 3/30\n",
            "315/315 [==============================] - 102s 325ms/step - loss: 3.2254 - accuracy: 0.3678\n",
            "Epoch 4/30\n",
            "315/315 [==============================] - 102s 324ms/step - loss: 3.1388 - accuracy: 0.3773\n",
            "Epoch 5/30\n",
            "315/315 [==============================] - 103s 327ms/step - loss: 3.0625 - accuracy: 0.3860\n",
            "Epoch 6/30\n",
            "315/315 [==============================] - 102s 325ms/step - loss: 2.9919 - accuracy: 0.3948\n",
            "Epoch 7/30\n",
            "315/315 [==============================] - 102s 325ms/step - loss: 2.9237 - accuracy: 0.4027\n",
            "Epoch 8/30\n",
            "315/315 [==============================] - 103s 327ms/step - loss: 2.8617 - accuracy: 0.4108\n",
            "Epoch 9/30\n",
            "315/315 [==============================] - 102s 325ms/step - loss: 2.8011 - accuracy: 0.4180\n",
            "Epoch 10/30\n",
            "315/315 [==============================] - 102s 325ms/step - loss: 2.7408 - accuracy: 0.4258\n",
            "Epoch 11/30\n",
            "315/315 [==============================] - 102s 324ms/step - loss: 2.6874 - accuracy: 0.4332\n",
            "Epoch 12/30\n",
            "315/315 [==============================] - 102s 324ms/step - loss: 2.6312 - accuracy: 0.4395\n",
            "Epoch 13/30\n",
            "315/315 [==============================] - 103s 327ms/step - loss: 2.5783 - accuracy: 0.4468\n",
            "Epoch 14/30\n",
            "315/315 [==============================] - 102s 325ms/step - loss: 2.5262 - accuracy: 0.4550\n",
            "Epoch 15/30\n",
            "315/315 [==============================] - 102s 324ms/step - loss: 2.4739 - accuracy: 0.4612\n",
            "Epoch 16/30\n",
            "315/315 [==============================] - 103s 327ms/step - loss: 2.4238 - accuracy: 0.4689\n",
            "Epoch 17/30\n",
            "315/315 [==============================] - 103s 327ms/step - loss: 2.3756 - accuracy: 0.4763\n",
            "Epoch 18/30\n",
            "315/315 [==============================] - 102s 325ms/step - loss: 2.3257 - accuracy: 0.4831\n",
            "Epoch 19/30\n",
            "315/315 [==============================] - 103s 327ms/step - loss: 2.2784 - accuracy: 0.4908\n",
            "Epoch 20/30\n",
            "315/315 [==============================] - 102s 324ms/step - loss: 2.2303 - accuracy: 0.4976\n",
            "Epoch 21/30\n",
            "315/315 [==============================] - 102s 325ms/step - loss: 2.1847 - accuracy: 0.5051\n",
            "Epoch 22/30\n",
            "315/315 [==============================] - 102s 324ms/step - loss: 2.1389 - accuracy: 0.5117\n",
            "Epoch 23/30\n",
            "315/315 [==============================] - 102s 324ms/step - loss: 2.0922 - accuracy: 0.5192\n",
            "Epoch 24/30\n",
            "315/315 [==============================] - 102s 324ms/step - loss: 2.0499 - accuracy: 0.5259\n",
            "Epoch 25/30\n",
            "315/315 [==============================] - 102s 324ms/step - loss: 2.0057 - accuracy: 0.5335\n",
            "Epoch 26/30\n",
            "315/315 [==============================] - 103s 327ms/step - loss: 1.9628 - accuracy: 0.5407\n",
            "Epoch 27/30\n",
            "315/315 [==============================] - 102s 325ms/step - loss: 1.9213 - accuracy: 0.5477\n",
            "Epoch 28/30\n",
            "315/315 [==============================] - 103s 327ms/step - loss: 1.8775 - accuracy: 0.5547\n",
            "Epoch 29/30\n",
            "315/315 [==============================] - 102s 325ms/step - loss: 1.8365 - accuracy: 0.5629\n",
            "Epoch 30/30\n",
            "315/315 [==============================] - 103s 327ms/step - loss: 1.7979 - accuracy: 0.5697\n",
            "Training execution time (mins) 58.151370477676394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "input_text = \"how is covid affecting\"\n",
        "\n",
        "# Tokenize Input\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
        "print(\"input_ids\",input_ids)\n",
        "\n",
        "# Generate outout\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=75,\n",
        "    top_p=0.80,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Generated text:\")\n",
        "display(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "Gmdy1KBmfkig",
        "outputId": "956fdbce-91f2-4020-f471-dfe272da1ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids tf.Tensor([[ 4919   318 39849   312 13891]], shape=(1, 5), dtype=int32)\n",
            "Generated text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'how is covid affecting children? The key is not to politicize the conversation, but to really politicize the conversation.\"  The study \"explained the impact of covid in young children, adolescents and young adults in different settings,\" said Jenni Wiley, a research assistant professor at the University of Minnesota.  Wiley said the researchers found that Black children were the \"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "input_text = \"the global pandemic\"\n",
        "\n",
        "# Tokenize Input\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
        "\n",
        "# Generate outout\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=75,\n",
        "    top_p=0.8,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Generated text:\")\n",
        "display(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "f3wR39RZflnY",
        "outputId": "f6f4d7ce-85e9-413b-d4c7-828e99236ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'the global pandemic,\" Francis said in a media briefing on Friday. \"It is in our national interest to open our economy to the best of our ability and to prevent further spread of this virus in our communities.\" Francis also said that although he had not personally contracted the virus, he still had a close relationship with the woman.  \"I would urge all US citizens to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚽 Pipelines\n",
        "\n",
        "| **Task**                     | **Description**                                                                                              | **Modality**    |\n",
        "|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|\n",
        "| Text classification          | assign a label to a given sequence of text                                                                   | NLP             |\n",
        "| Text generation              | generate text given a prompt                                                                                 | NLP             |\n",
        "| Summarization                | generate a summary of a sequence of text or document                                                         | NLP             |\n",
        "\n",
        "<Tip>\n",
        "\n",
        "For a complete list of available tasks, check out the [pipeline API reference](https://huggingface.co/docs/transformers/main/en/./main_classes/pipelines).\n",
        "\n",
        "</Tip>"
      ],
      "metadata": {
        "id": "oowf-aFOnwtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "L0410g1OfnuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls = pipeline('ner')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "b174a8bbc0bb434b9afb38ae8f76722a",
            "fc5d72e2f6eb411fa9428b7b67387e80",
            "2479fe19eada4482aacaf08cf9fae412",
            "1cdc9cd7d23e41c0906381bae172aac8",
            "2c00fc903a1441a482dcdaab3b2838d0",
            "c0c23d8532ce4e53a89e2283ad442d82",
            "c5f838014c304a72a8fef05679ec6d52",
            "9562d0b1455c4fdc90292590b1487336",
            "3fb4a92bc26849c390f72b3e0429083c",
            "e0ef5a6f99d74420b5360ab16548f269",
            "725376ff73574b9fb2ab125d8d175f7d",
            "deef137d2ca34ed390ffcde72b669fa0",
            "76764e8c12594d3c9a2b371e7acae375",
            "871614e81f364317932fb8a817afb406",
            "37e8007808054fedb862c9013ef4b2a9",
            "8066918117e4489d9128723b83047474",
            "6acf5c7e0458492eb73d6837e1cda691",
            "05c57bfb108b4513835417558671d5fb",
            "33a02a30336548c2a20244b2cf056bb2",
            "184561bd6c484fefae1c5b8b5810a8e4",
            "1824a259c694415d9d865415abe6b8b1",
            "9967707719014d719d29b45f9da337d9",
            "0c7571318823472292affe7b2550cf11",
            "f2b903db56164d4c8ce40dd133b4631e",
            "364fb326fa8d47c4a375f9874c036770",
            "89ca973215b74797ad7aed7a41b626fa",
            "0bd6916038574698891b4f07a290f5bb",
            "996b42f909154e63a15003f570e6126e",
            "fdf084a16abd4b60942f4f6b70929ebf",
            "34c892e298de432b99ffb301cc506ff3",
            "ab395a0d232f4cb88997e9acfda8d819",
            "1cccbb556e01490b82892759e76aee67",
            "bb48f2c8a53f4f9fbc96b66cd3a9a6cd",
            "dd61c4aababb440f8f474dd17f808cba",
            "df614718f43a4b229381cbab65d9f95f",
            "021ffbfd6581461b82805053127f4c47",
            "6cbcf0434a814a7eb0fc8a717251267c",
            "9a58435ef3ae4f458c647a3f3a4f94c7",
            "f610303dcc30485fb9071003c7190e8a",
            "81fcc4f188474cf6a9659ba7d5e2d9e4",
            "73d1ab38a11a446698b643549941c6fc",
            "9ccb2f00d5d14cc9b216df1634649fee",
            "5f75be3250444d61bf4a37c160726746",
            "6217a68e9301412fb32560f21f8112b4"
          ]
        },
        "id": "iX-W4BhEWhfX",
        "outputId": "df4be255-b668-4cea-d92b-172778a43486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b174a8bbc0bb434b9afb38ae8f76722a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "deef137d2ca34ed390ffcde72b669fa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c7571318823472292affe7b2550cf11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd61c4aababb440f8f474dd17f808cba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls(\"Pavlos is one of the teachers here at Harvard\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjALJJ78WlT4",
        "outputId": "77d9206c-43b9-488a-9c03-8f9c01c8d24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER',\n",
              "  'score': np.float32(0.9992362),\n",
              "  'index': 1,\n",
              "  'word': 'Pa',\n",
              "  'start': 0,\n",
              "  'end': 2},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.99177223),\n",
              "  'index': 2,\n",
              "  'word': '##v',\n",
              "  'start': 2,\n",
              "  'end': 3},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.995366),\n",
              "  'index': 3,\n",
              "  'word': '##los',\n",
              "  'start': 3,\n",
              "  'end': 6},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': np.float32(0.972195),\n",
              "  'index': 11,\n",
              "  'word': 'Harvard',\n",
              "  'start': 38,\n",
              "  'end': 45}]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_end = time.time()\n",
        "print(f\"It took {(time_end - time_start)/60:.2f} minutes for this notebook to run\")"
      ],
      "metadata": {
        "id": "iCOedqOQWt1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12182048-fa1e-4fa3-f1b8-147e057a1432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It took 65.27 minutes for this notebook to run\n"
          ]
        }
      ]
    }
  ]
}