{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \u003cimg style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"\u003e Data Science 2: Advanced Topics in Data Science \n",
                "## Lab 8: Recurrent Neural Networks\n",
                "\n",
                "\n",
                "**Harvard University**\u003cbr/\u003e\n",
                "**Spring 2025**\u003cbr/\u003e\n",
                "**Instructors**: Pavlos Protopapas, Natesh Pillai, and Chris Gumb\n",
                "\n",
                "\u003chr style=\"height:2pt\"\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\u003cstyle\u003e\n",
                            "blockquote { background: #AEDE94; }\n",
                            "h1 { \n",
                            "    padding-top: 25px;\n",
                            "    padding-bottom: 25px;\n",
                            "    text-align: left; \n",
                            "    padding-left: 10px;\n",
                            "    background-color: #DDDDDD; \n",
                            "    color: black;\n",
                            "}\n",
                            "h2 { \n",
                            "    padding-top: 10px;\n",
                            "    padding-bottom: 10px;\n",
                            "    text-align: left; \n",
                            "    padding-left: 5px;\n",
                            "    background-color: #EEEEEE; \n",
                            "    color: black;\n",
                            "}\n",
                            "\n",
                            "div.exercise {\n",
                            "\tbackground-color: #ffcccc;\n",
                            "\tborder-color: #E9967A; \t\n",
                            "\tborder-left: 5px solid #800080; \n",
                            "\tpadding: 0.5em;\n",
                            "}\n",
                            "div.discussion {\n",
                            "\tbackground-color: #ccffcc;\n",
                            "\tborder-color: #88E97A;\n",
                            "\tborder-left: 5px solid #0A8000; \n",
                            "\tpadding: 0.5em;\n",
                            "}\n",
                            "div.theme {\n",
                            "\tbackground-color: #DDDDDD;\n",
                            "\tborder-color: #E9967A; \t\n",
                            "\tborder-left: 5px solid #800080; \n",
                            "\tpadding: 0.5em;\n",
                            "\tfont-size: 18pt;\n",
                            "}\n",
                            "div.gc { \n",
                            "\tbackground-color: #AEDE94;\n",
                            "\tborder-color: #E9967A; \t \n",
                            "\tborder-left: 5px solid #800080; \n",
                            "\tpadding: 0.5em;\n",
                            "\tfont-size: 12pt;\n",
                            "}\n",
                            "p.q1 { \n",
                            "    padding-top: 5px;\n",
                            "    padding-bottom: 5px;\n",
                            "    text-align: left; \n",
                            "    padding-left: 5px;\n",
                            "    background-color: #EEEEEE; \n",
                            "    color: black;\n",
                            "}\n",
                            "header {\n",
                            "   padding-top: 35px;\n",
                            "    padding-bottom: 35px;\n",
                            "    text-align: left; \n",
                            "    padding-left: 10px;\n",
                            "    background-color: #DDDDDD; \n",
                            "    color: black;\n",
                            "}\n",
                            "\u003c/style\u003e\n",
                            "\n"
                        ],
                        "text/plain": [
                            "\u003cIPython.core.display.HTML object\u003e"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
                "import requests\n",
                "from IPython.core.display import HTML\n",
                "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text\n",
                "HTML(styles)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Learning Objectives\n",
                "\n",
                "By the end of this lab, you should understand:\n",
                "- how to perform basic preprocessing on text data\n",
                "- the layers used in `keras` to construct RNNs and its variants (GRU, LSTM)\n",
                "- how the model's task (i.e., many-to-1, many-to-many) affects architecture choices"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id=\"contents\"\u003e\u003c/a\u003e\n",
                "\n",
                "## Notebook Contents\n",
                "- [**IMDB Review Dataset**](#imdb)\n",
                "- [**Preprocessing Text Data**](#prep)\n",
                "    - [Tokenization](#token)\n",
                "    - [Padding](#pad)\n",
                "    - [Numerical Encoding](#encode)\n",
                "- [**Movie Review Sentiment Analysis**](#models)\n",
                "    - [Naive FFNN](#ffnn)\n",
                "    - [Embedding Layer](#embed)\n",
                "    - [1D CNN](#cnn)\n",
                "    - [Vanilla RNN](#rnn)\n",
                "    - [Vanishing/Exploding Gradients](#vanish)\n",
                "    - [GRU](#gru)\n",
                "    - [LSTM](#lstm)\n",
                "    - [BiDirectional Layer](#bidir)\n",
                "    - [Deep RNNs](#deep)\n",
                "    - [TimeDistributed Layer](#timedis)\n",
                "    - [RepeatVector Layer](#repeatvec)\n",
                "    - [CNN + RNN](#cnnrnn)\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-04-04 00:51:30.002183: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-04 00:51:30.002220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-04 00:51:30.002927: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-04-04 00:51:30.007737: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\nYour GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA L4, compute capability 8.9\n"
                }
            ],
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras.datasets import imdb\n",
                "from tensorflow.keras.models import Sequential, Model, load_model\n",
                "from tensorflow.keras.layers import BatchNormalization, Bidirectional, Dense, Embedding, GRU, LSTM, SimpleRNN,\\\n",
                "                                    Input, TimeDistributed, Dropout, RepeatVector\n",
                "from tensorflow.keras.layers import Conv1D, Conv2D, Flatten, MaxPool1D, MaxPool2D, Lambda\n",
                "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback, ModelCheckpoint\n",
                "from tensorflow.keras.initializers import Constant\n",
                "from tensorflow.keras.preprocessing import sequence\n",
                "from sklearn.model_selection import train_test_split\n",
                "import tensorflow_datasets\n",
                "from matplotlib import pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os, re, sys\n",
                "# fix random seed for (some) reproducibility\n",
                "np.random.seed(109)\n",
                "# Use mixed precision for optimal performance\n",
                "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Case Study: IMDB Review Classifier \n",
                "\u003cimg src='fig/manyto1.png' width='300px'\u003e\n",
                "\n",
                "Let's frame our discussion of RNNS around the example of a text classifier. Specifically, We'll build and evaluate various models that all attempt to descriminate between positive and negative reviews through the Internet Movie Database (IMDB). The dataset is again made available to us through the tensorflow datasets API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow_datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-04-03 21:07:45.800951: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Could not resolve hostname', error details: Could not resolve host: metadata.google.internal\".\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /shared/home/cwg194/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9ba6ddea925749fd86bc56bfc4f48b3f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Dl Completed...: 0 url [00:00, ? url/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ca5249a2244e4092b20848fb0c954ffb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Dl Size...: 0 MiB [00:00, ? MiB/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating splits...:   0%|          | 0/3 [00:00\u003c?, ? splits/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train examples...:   0%|          | 0/25000 [00:00\u003c?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Shuffling /shared/home/cwg194/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteYEFXG9/imdb_reviews-‚Ä¶"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating test examples...:   0%|          | 0/25000 [00:00\u003c?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Shuffling /shared/home/cwg194/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteYEFXG9/imdb_reviews-‚Ä¶"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating unsupervised examples...:   0%|          | 0/50000 [00:00\u003c?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Shuffling /shared/home/cwg194/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteYEFXG9/imdb_reviews-‚Ä¶"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u001b[1mDataset imdb_reviews downloaded and prepared to /shared/home/cwg194/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-04-03 21:08:12.788878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20763 MB memory:  -\u003e device: 0, name: NVIDIA L4, pci bus id: 0000:31:00.0, compute capability: 8.9\n"
                }
            ],
            "source": [
                "(train, test), info = tensorflow_datasets.load('imdb_reviews',\n",
                "                                               split=['train', 'test'],\n",
                "                                               with_info=True)\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The helpful `info` object provides details about the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tfds.core.DatasetInfo(\n",
                            "    name='imdb_reviews',\n",
                            "    full_name='imdb_reviews/plain_text/1.0.0',\n",
                            "    description=\"\"\"\n",
                            "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
                            "    classification containing substantially more data than previous benchmark\n",
                            "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
                            "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
                            "    \"\"\",\n",
                            "    config_description=\"\"\"\n",
                            "    Plain text\n",
                            "    \"\"\",\n",
                            "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
                            "    data_dir=PosixGPath('/tmp/tmp4nakntiotfds'),\n",
                            "    file_format=tfrecord,\n",
                            "    download_size=80.23 MiB,\n",
                            "    dataset_size=129.83 MiB,\n",
                            "    features=FeaturesDict({\n",
                            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
                            "        'text': Text(shape=(), dtype=string),\n",
                            "    }),\n",
                            "    supervised_keys=('text', 'label'),\n",
                            "    disable_shuffling=False,\n",
                            "    splits={\n",
                            "        'test': \u003cSplitInfo num_examples=25000, num_shards=1\u003e,\n",
                            "        'train': \u003cSplitInfo num_examples=25000, num_shards=1\u003e,\n",
                            "        'unsupervised': \u003cSplitInfo num_examples=50000, num_shards=1\u003e,\n",
                            "    },\n",
                            "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
                            "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
                            "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
                            "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
                            "      month     = {June},\n",
                            "      year      = {2011},\n",
                            "      address   = {Portland, Oregon, USA},\n",
                            "      publisher = {Association for Computational Linguistics},\n",
                            "      pages     = {142--150},\n",
                            "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
                            "    }\"\"\",\n",
                            ")"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "info"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We see that the dataset consists of text reviews and binary good/bad labels. Here are two examples:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "text:\nThis was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n\nlabel: bad\n\ntext:\nThis is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.\n\nlabel: good\n\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-04-03 21:08:13.230255: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
                }
            ],
            "source": [
                "# display the first 'bad' and 'good' movie reviews in the training data\n",
                "labels = {0: 'bad', 1: 'good'}\n",
                "seen = {'bad': False, 'good': False}\n",
                "for review in train:\n",
                "    label = review['label'].numpy()\n",
                "    if not seen[labels[label]]:\n",
                "        print(f\"text:\\n{review['text'].numpy().decode()}\\n\")\n",
                "        print(f\"label: {labels[label]}\\n\")\n",
                "        seen[labels[label]] = True\n",
                "    if all(val == True for val in seen.values()):\n",
                "        break"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Great! But unfortunately, computers can read! üìñ--ü§ñ‚ùì"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing Text Data \u003cdiv id='prep'\u003e\n",
                "\n",
                "Computers have no built-in knowledge of language and cannot understand text data in the way that humans do -- at least not without some help! The first crucial step in natural language processing is to clean and preprocess your data so that your algorithms and models can make use of it.\n",
                "    \n",
                "We'll look at a few preprocess steps:\n",
                "- tokenization\n",
                "- padding\n",
                "- numerical encoding\n",
                "        \n",
                "Depending on your NLP task, you may want to take additional preprocessing steps which we will not cover here. These can include:\n",
                "- converting all characters to lowercase\n",
                "- treating each punctuation mark as a token (e.g., , . ! ? are each separate tokens)\n",
                "- removing punctuation altogether\n",
                "- separating each sentence with a unique symbol (e.g., \u003cS\u003e and \u003c/S\u003e)\n",
                "- removing words that are incredibly common (e.g., function words, (in)definite articles). These are referred to as 'stopwords').\n",
                "- Lemmatizing (replacing words with their 'dictionary entry form')\n",
                "- Stemming (removing grammatical morphemes)\n",
                "    \n",
                "Useful NLP Python libraries such as [NLTK](https://www.nltk.org/) and [spaCy](https://spacy.io/) provide built in methods for many of these preprocessing steps."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='token'\u003e\u003cb\u003eTokenization\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "**Tokens** are the atomic units of meaning which our model will be working with. What should these units be? These could be characters, words, or even sentences. For our movie review classifier we will be working at the word level."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For this example we will process just a subset of the original dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-04-03 21:08:13.283452: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
                },
                {
                    "data": {
                        "text/plain": [
                            "{'label': \u003ctf.Tensor: shape=(), dtype=int64, numpy=0\u003e,\n",
                            " 'text': \u003ctf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\u003e}"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "SAMPLE_SIZE = 10\n",
                "subset = train.take(SAMPLE_SIZE)\n",
                "next(iter(subset))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The TFDS format allows for the construction of efficient preprocessing pipelines. But for our own preprocessing example we will be primarily working with Python `list` objects. This gives us a chance to practice the Python **list comprehension** which is a powerful tool to have at your disposal. It will serve you well when processing arbitrary text which may not already be in a nice TFDS format (such as in the HW üòâ).\n",
                "\n",
                "We'll convert our data subset into X and y lists."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-04-03 21:08:13.351532: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n2025-04-03 21:08:13.374922: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
                }
            ],
            "source": [
                "X = [x['text'].numpy().decode() for x in subset]\n",
                "y = [x['label'].numpy() for x in subset]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "X has 10 reviews\ny has 10 labels\n"
                }
            ],
            "source": [
                "print(f'X has {len(X)} reviews')\n",
                "print(f'y has {len(y)} labels')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "First 20 characters of all reviews:\n['This was an absolute...', 'I have been known to...', 'Mann photographs the...', 'This is the kind of ...', 'As others have menti...', 'This is a film which...', 'Okay, you have:\u003cbr /...', 'The film is based on...', 'I really love the se...', \"Sure, this one isn't...\"]\n\nAll labels:\n[0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n"
                }
            ],
            "source": [
                "N_CHARS = 20\n",
                "print(f'First {N_CHARS} characters of all reviews:\\n{[x[:20]+\"...\" for x in X]}\\n')\n",
                "print(f'All labels:\\n{y}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Each observation in `X` is a review. A review is a `str` object which we can think of as a sequence of characters. This is indeed how Python treats strings as made clear by how we are printing 'slices' of each review in the code cell above.\u003cbr\u003e\n",
                "\n",
                "We'll see a bit later that you can in fact sucessfully train a neural network on text data at the character level.\n",
                "\n",
                "But for the moment we will work at the word level, treating the word level. This means our observations should be organized as **sequences of words** rather than sequences of characters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# list comprehensions again to the rescue!\n",
                "X = [x.split() for x in X]\n",
                "# The same thing can be accomplished with:\n",
                "# list(map(str.split, X))\n",
                "# but that is much harder to parse! O_o"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's look at the first 10 **tokens** in the first 2 reviews."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(['This',\n",
                            "  'was',\n",
                            "  'an',\n",
                            "  'absolutely',\n",
                            "  'terrible',\n",
                            "  'movie.',\n",
                            "  \"Don't\",\n",
                            "  'be',\n",
                            "  'lured',\n",
                            "  'in'],\n",
                            " ['I',\n",
                            "  'have',\n",
                            "  'been',\n",
                            "  'known',\n",
                            "  'to',\n",
                            "  'fall',\n",
                            "  'asleep',\n",
                            "  'during',\n",
                            "  'films,',\n",
                            "  'but'])"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "X[0][:10], X[1][:10]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='pad'\u003e\u003cb\u003ePadding\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "Let's take a look at the lengths of the reviews in our subset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[116, 112, 132, 88, 81, 289, 557, 111, 223, 127]"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "[len(x) for x in X]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If we were training our RNN one sentence at a time, it would be okay to have sentences of varying lengths. However, as with any neural network, it can be sometimes be advantageous to train inputs in batches. When doing so with RNNs, our input tensors need to be of the same length/dimensions."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here are two examples of tokenized reviews padded to have a length of 5.\n",
                "```\n",
                "['I', 'loved', 'it', '\u003cPAD\u003e', '\u003cPAD\u003e']\n",
                "['It', 'stinks', '\u003cPAD\u003e', '\u003cPAD\u003e', '\u003cPAD\u003e']\n",
                "```\n",
                "Now let's pad our own examples. Note that 'padding' in this context also means truncating sequences that are longer than our specified max length."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "MAX_LEN = 500\n",
                "PAD = '\u003cPAD\u003e'\n",
                "# truncate\n",
                "X = [x[:MAX_LEN] for x in X]\n",
                "# pad\n",
                "# note: prepadding is usually preferable\n",
                "for x in X:\n",
                "    while len(x) \u003c MAX_LEN:\n",
                "        x.append(PAD)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[500, 500, 500, 500, 500, 500, 500, 500, 500, 500]"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "[len(x) for x in X]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now all reviews are of a uniform length!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='encode'\u003e\u003cb\u003eNumerical Encoding\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "If each review in our dataset is an observation, then the features of each observation are the tokens, in this case, words. But these words are still strings. Our machine learning methods require us to be able to multiple our features by weights. If we want to use these words as inputs for a neural network we'll have to convert them into some numerical representation.\n",
                "\n",
                "One solution is to create a one-to-one mapping between unique words and integers.\n",
                "\n",
                "If the five sentences below were our entire corpus, our conversion would look this:\n",
                "\n",
                "1. i have books - [1, 4, 2]\n",
                "2. interesting books are useful [11,2,9,8]\n",
                "3. i have computers [1,4,3]\n",
                "4. computers are interesting and useful [3,5,11,10,8]\n",
                "5. books and computers are both valuable. [2,10,3,9,13,12]\n",
                "6. bye bye [7,7]\n",
                "\n",
                "I-1, books-2, computers-3, have-4, are-5, computers-6,bye-7, useful-8, are-9, and-10,interesting-11, valuable-12, both-13\n",
                "\n",
                "To accomplish this we'll first need to know what all the unique words are in our dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_tokens = [word for review in X for word in review]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(5000, 5000)"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# sanity check\n",
                "len(all_tokens), sum([len(x) for x in X])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Casting our `list` of words into a `set` is a great way to get all the *unique* words in the data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Unique Words: 892\n"
                }
            ],
            "source": [
                "vocab = sorted(set(all_tokens))\n",
                "print('Unique Words:', len(vocab))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we need to create a mapping from words to integers. For this will a **dictionary comprehension**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "word2idx = {word: idx for idx, word in enumerate(vocab)}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'\"Absolute': 0,\n",
                            " '\"Bohlen\"-Fan': 1,\n",
                            " '\"Brideshead': 2,\n",
                            " '\"Candy\"?).': 3,\n",
                            " '\"City': 4,\n",
                            " '\"Dieter': 5,\n",
                            " '\"Dieter\"': 6,\n",
                            " '\"Dragonfly\"': 7,\n",
                            " '\"I\\'ve': 8,\n",
                            " '\"Lady.\"\u003cbr': 9,\n",
                            " '\"London': 10,\n",
                            " '\"Make': 11,\n",
                            " '\"Miss\"': 12,\n",
                            " '\"Mr': 13,\n",
                            " '\"Mrs.\"': 14,\n",
                            " '\"actors\"': 15,\n",
                            " '\"dewy-eyed.\"\u003cbr': 16,\n",
                            " '\"hey,': 17,\n",
                            " '\"meanwhile,\")': 18,\n",
                            " '\"men\"': 19,\n",
                            " \"'Where\": 20,\n",
                            " \"'em\": 21,\n",
                            " \"'round\": 22,\n",
                            " '(Backbone': 23,\n",
                            " '(Barbarella).': 24,\n",
                            " '(Brit': 25,\n",
                            " '(I': 26,\n",
                            " '(Jeremy': 27,\n",
                            " '(Remember': 28,\n",
                            " '(YOU': 29,\n",
                            " '(as': 30,\n",
                            " '(at': 31,\n",
                            " '(not': 32,\n",
                            " '(the': 33,\n",
                            " \"(there's\": 34,\n",
                            " '(they': 35,\n",
                            " '(we': 36,\n",
                            " '(what': 37,\n",
                            " '(when,': 38,\n",
                            " '(yes': 39,\n",
                            " '-': 40,\n",
                            " '.': 41,\n",
                            " '/\u003e\u003cbr': 42,\n",
                            " '/\u003eAh,': 43,\n",
                            " '/\u003eAnd': 44,\n",
                            " '/\u003eBut': 45,\n",
                            " '/\u003eCanadian': 46,\n",
                            " '/\u003eDavid': 47,\n",
                            " '/\u003eFirst': 48,\n",
                            " '/\u003eHenceforth,': 49,\n",
                            " '/\u003eJoanna': 50,\n",
                            " '/\u003eJournalist': 51,\n",
                            " '/\u003eNothing': 52,\n",
                            " '/\u003eOK,': 53,\n",
                            " '/\u003ePenelope': 54,\n",
                            " '/\u003ePeter': 55,\n",
                            " '/\u003eSecond': 56,\n",
                            " '/\u003eSo': 57,\n",
                            " '/\u003eSusan': 58,\n",
                            " '/\u003eThank': 59,\n",
                            " '/\u003eThird': 60,\n",
                            " '/\u003eTo': 61,\n",
                            " '/\u003eWhen': 62,\n",
                            " '/\u003eWrong!': 63,\n",
                            " '/\u003eand': 64,\n",
                            " '1-dimensional': 65,\n",
                            " '14': 66,\n",
                            " '1950s': 67,\n",
                            " '20': 68,\n",
                            " '\u003cPAD\u003e': 69,\n",
                            " '\u003cbr': 70,\n",
                            " 'A': 71,\n",
                            " 'After': 72,\n",
                            " 'Alberta': 73,\n",
                            " 'Alison': 74,\n",
                            " 'Alonso': 75,\n",
                            " 'American': 76,\n",
                            " 'And': 77,\n",
                            " 'As': 78,\n",
                            " 'B.B.E.': 79,\n",
                            " 'Beginners\",': 80,\n",
                            " 'Boarding-School,': 81,\n",
                            " 'Bohlen\"': 82,\n",
                            " 'Both': 83,\n",
                            " 'Brennan': 84,\n",
                            " 'Bulimia': 85,\n",
                            " 'But': 86,\n",
                            " 'Cage': 87,\n",
                            " 'Canadian': 88,\n",
                            " 'Cher': 89,\n",
                            " 'Christmas!)': 90,\n",
                            " 'Christopher': 91,\n",
                            " 'Circus.\u003cbr': 92,\n",
                            " 'City': 93,\n",
                            " 'City,': 94,\n",
                            " 'Col.': 95,\n",
                            " 'Colin': 96,\n",
                            " 'Colonel': 97,\n",
                            " 'Columbian': 98,\n",
                            " 'Conchita': 99,\n",
                            " 'Constantly': 100,\n",
                            " 'Coppola': 101,\n",
                            " 'Cricket': 102,\n",
                            " \"Cricket's\": 103,\n",
                            " 'Davies)': 104,\n",
                            " 'Dawson': 105,\n",
                            " 'Deadwood,': 106,\n",
                            " 'Dean': 107,\n",
                            " 'Depardieu,': 108,\n",
                            " \"Don't\": 109,\n",
                            " 'Dragonfly': 110,\n",
                            " 'Emily': 111,\n",
                            " 'England)': 112,\n",
                            " 'England.)': 113,\n",
                            " 'European': 114,\n",
                            " 'Even': 115,\n",
                            " 'Film': 116,\n",
                            " 'First': 117,\n",
                            " 'Gerard': 118,\n",
                            " 'German': 119,\n",
                            " 'Giancarlo': 120,\n",
                            " 'Giannini': 121,\n",
                            " \"Girls'\": 122,\n",
                            " 'Hampshire': 123,\n",
                            " 'He': 124,\n",
                            " 'Headmistress': 125,\n",
                            " 'Her': 126,\n",
                            " 'Herringbone-Tweed,': 127,\n",
                            " 'Hollywood': 128,\n",
                            " 'Home': 129,\n",
                            " 'However': 130,\n",
                            " 'I': 131,\n",
                            " \"I'll\": 132,\n",
                            " \"I'm\": 133,\n",
                            " 'II': 134,\n",
                            " 'If': 135,\n",
                            " 'Ironside.': 136,\n",
                            " 'It': 137,\n",
                            " \"It's\": 138,\n",
                            " 'Japanese': 139,\n",
                            " 'Jimmy': 140,\n",
                            " 'John': 141,\n",
                            " 'Justice\".': 142,\n",
                            " 'Katt': 143,\n",
                            " 'Keith': 144,\n",
                            " 'Klondike': 145,\n",
                            " 'L.L.': 146,\n",
                            " 'L.L.\u003cbr': 147,\n",
                            " 'Lady': 148,\n",
                            " 'Law': 149,\n",
                            " 'Leading': 150,\n",
                            " \"Lies'.\": 151,\n",
                            " 'Lohman': 152,\n",
                            " \"Lohman's.\": 153,\n",
                            " 'Lohman,': 154,\n",
                            " 'London,': 155,\n",
                            " 'Lord': 156,\n",
                            " 'Love': 157,\n",
                            " 'Lumley': 158,\n",
                            " 'Madness': 159,\n",
                            " 'Mann': 160,\n",
                            " 'Manor,': 161,\n",
                            " 'Manor.\u003cbr': 162,\n",
                            " 'Mare': 163,\n",
                            " 'Maria': 164,\n",
                            " 'McCallum': 165,\n",
                            " 'McInnes': 166,\n",
                            " \"McInnes's\": 167,\n",
                            " 'Meanwhile': 168,\n",
                            " 'Michael': 169,\n",
                            " 'Miss': 170,\n",
                            " 'Mortimer': 171,\n",
                            " 'Mountains': 172,\n",
                            " 'Mountie': 173,\n",
                            " 'Mr.': 174,\n",
                            " 'Nancherrow': 175,\n",
                            " 'New': 176,\n",
                            " 'Nicolas': 177,\n",
                            " 'North': 178,\n",
                            " \"O'Toole\": 179,\n",
                            " 'OK-movie.': 180,\n",
                            " 'Okay,': 181,\n",
                            " \"Ol'\": 182,\n",
                            " 'Our': 183,\n",
                            " 'Phillip': 184,\n",
                            " 'Pilcher': 185,\n",
                            " 'Polonia': 186,\n",
                            " 'Reefer': 187,\n",
                            " 'Revisited,\"': 188,\n",
                            " 'Rocky': 189,\n",
                            " 'Roman': 190,\n",
                            " \"She's\": 191,\n",
                            " 'Shea': 192,\n",
                            " 'So': 193,\n",
                            " 'Spades\"': 194,\n",
                            " 'Speaking': 195,\n",
                            " 'Stately': 196,\n",
                            " 'Stewart': 197,\n",
                            " 'Still,': 198,\n",
                            " 'Stockwell': 199,\n",
                            " 'Sunday': 200,\n",
                            " 'Sure,': 201,\n",
                            " 'Susan': 202,\n",
                            " 'Teacups,': 203,\n",
                            " 'Technically': 204,\n",
                            " \"That's\": 205,\n",
                            " 'The': 206,\n",
                            " 'There': 207,\n",
                            " 'They': 208,\n",
                            " \"They're\": 209,\n",
                            " 'Things': 210,\n",
                            " 'This': 211,\n",
                            " 'Together,': 212,\n",
                            " 'Truth': 213,\n",
                            " 'US': 214,\n",
                            " 'Venerable': 215,\n",
                            " 'Walken': 216,\n",
                            " \"Walken's\": 217,\n",
                            " 'Walter': 218,\n",
                            " 'War': 219,\n",
                            " 'Well,': 220,\n",
                            " 'West.\u003cbr': 221,\n",
                            " 'When': 222,\n",
                            " 'Wild': 223,\n",
                            " 'Winningham': 224,\n",
                            " 'Wonderful': 225,\n",
                            " 'World': 226,\n",
                            " 'York': 227,\n",
                            " 'Yukon': 228,\n",
                            " 'a': 229,\n",
                            " 'ability': 230,\n",
                            " 'ably': 231,\n",
                            " 'about.': 232,\n",
                            " 'absolutely': 233,\n",
                            " 'acclaimed;': 234,\n",
                            " 'accord': 235,\n",
                            " 'accuracy.': 236,\n",
                            " 'accurate': 237,\n",
                            " 'achievement,': 238,\n",
                            " 'act': 239,\n",
                            " 'acting': 240,\n",
                            " 'action': 241,\n",
                            " 'actor': 242,\n",
                            " \"actor's\": 243,\n",
                            " 'actors': 244,\n",
                            " 'actors,': 245,\n",
                            " \"actress's\": 246,\n",
                            " 'actresses': 247,\n",
                            " 'actually': 248,\n",
                            " 'admit,': 249,\n",
                            " 'advice': 250,\n",
                            " 'advice:': 251,\n",
                            " 'affair': 252,\n",
                            " 'after': 253,\n",
                            " 'afternoon': 254,\n",
                            " 'age': 255,\n",
                            " 'age!).': 256,\n",
                            " 'ago': 257,\n",
                            " 'ahead': 258,\n",
                            " 'air': 259,\n",
                            " 'alive': 260,\n",
                            " 'all': 261,\n",
                            " 'all,': 262,\n",
                            " 'all.': 263,\n",
                            " 'all.\u003cbr': 264,\n",
                            " 'along.': 265,\n",
                            " 'alright,': 266,\n",
                            " 'always': 267,\n",
                            " 'always)': 268,\n",
                            " 'am': 269,\n",
                            " 'amazingly': 270,\n",
                            " 'amusing': 271,\n",
                            " 'an': 272,\n",
                            " 'and': 273,\n",
                            " 'anguish': 274,\n",
                            " 'animal': 275,\n",
                            " 'another!)\u003cbr': 276,\n",
                            " 'another.': 277,\n",
                            " 'any': 278,\n",
                            " 'anybody': 279,\n",
                            " 'anything': 280,\n",
                            " 'apology': 281,\n",
                            " 'appear': 282,\n",
                            " 'appeared': 283,\n",
                            " 'are': 284,\n",
                            " 'arm-chair': 285,\n",
                            " 'around': 286,\n",
                            " 'around,': 287,\n",
                            " 'as': 288,\n",
                            " 'asleep': 289,\n",
                            " 'aspiring': 290,\n",
                            " 'astonishing': 291,\n",
                            " 'at': 292,\n",
                            " 'autobiography': 293,\n",
                            " 'award': 294,\n",
                            " 'baby': 295,\n",
                            " 'backbone!\u003cbr': 296,\n",
                            " 'backlighting.': 297,\n",
                            " 'badly-acted,': 298,\n",
                            " 'barely': 299,\n",
                            " 'barometers': 300,\n",
                            " 'based': 301,\n",
                            " 'battling': 302,\n",
                            " 'be': 303,\n",
                            " 'beautiful': 304,\n",
                            " 'because': 305,\n",
                            " 'become': 306,\n",
                            " 'becomes': 307,\n",
                            " 'been': 308,\n",
                            " 'before': 309,\n",
                            " 'behaviour': 310,\n",
                            " 'being': 311,\n",
                            " 'best': 312,\n",
                            " 'best,': 313,\n",
                            " 'best.': 314,\n",
                            " 'better': 315,\n",
                            " 'big': 316,\n",
                            " 'bit': 317,\n",
                            " 'blockbuster,': 318,\n",
                            " 'body': 319,\n",
                            " 'border': 320,\n",
                            " 'boring.': 321,\n",
                            " 'boy': 322,\n",
                            " 'boy.': 323,\n",
                            " 'brilliant': 324,\n",
                            " 'bulimia': 325,\n",
                            " 'business': 326,\n",
                            " 'but': 327,\n",
                            " 'but,': 328,\n",
                            " 'by': 329,\n",
                            " 'by,': 330,\n",
                            " 'called': 331,\n",
                            " 'cameos': 332,\n",
                            " 'camps': 333,\n",
                            " 'can': 334,\n",
                            " 'can\"!': 335,\n",
                            " \"can't\": 336,\n",
                            " 'cant': 337,\n",
                            " 'captures': 338,\n",
                            " 'cases': 339,\n",
                            " 'cast': 340,\n",
                            " 'catatonic.': 341,\n",
                            " 'causes': 342,\n",
                            " 'causing': 343,\n",
                            " 'character': 344,\n",
                            " 'choice': 345,\n",
                            " 'cinema': 346,\n",
                            " 'colonel': 347,\n",
                            " 'combination': 348,\n",
                            " 'come': 349,\n",
                            " 'comes': 350,\n",
                            " 'comfortable': 351,\n",
                            " 'company': 352,\n",
                            " 'complete': 353,\n",
                            " 'compulsive': 354,\n",
                            " 'concern': 355,\n",
                            " 'consent': 356,\n",
                            " 'considerate': 357,\n",
                            " 'constant.': 358,\n",
                            " 'contacts?)\u003cbr': 359,\n",
                            " 'content': 360,\n",
                            " 'continue': 361,\n",
                            " 'control': 362,\n",
                            " 'convey': 363,\n",
                            " 'could': 364,\n",
                            " \"couldn't\": 365,\n",
                            " 'couple': 366,\n",
                            " 'courage': 367,\n",
                            " 'course)': 368,\n",
                            " 'cover': 369,\n",
                            " 'cries': 370,\n",
                            " 'criticize.': 371,\n",
                            " 'cross,': 372,\n",
                            " 'crush': 373,\n",
                            " 'crying,': 374,\n",
                            " 'dangerous': 375,\n",
                            " 'dash': 376,\n",
                            " 'days': 377,\n",
                            " 'dead': 378,\n",
                            " 'decide': 379,\n",
                            " 'deep,': 380,\n",
                            " 'degree': 381,\n",
                            " 'depends': 382,\n",
                            " 'depths': 383,\n",
                            " 'descend': 384,\n",
                            " 'deserves': 385,\n",
                            " 'despair,': 386,\n",
                            " 'despair.': 387,\n",
                            " 'desperation': 388,\n",
                            " 'destroy': 389,\n",
                            " 'development': 390,\n",
                            " 'devoid': 391,\n",
                            " 'did': 392,\n",
                            " \"didn't\": 393,\n",
                            " 'directed': 394,\n",
                            " 'director': 395,\n",
                            " 'disappointed': 396,\n",
                            " 'disgust.': 397,\n",
                            " 'disorder.': 398,\n",
                            " 'do.': 399,\n",
                            " 'does': 400,\n",
                            " \"don't\": 401,\n",
                            " 'done,': 402,\n",
                            " 'done.\u003cbr': 403,\n",
                            " 'due': 404,\n",
                            " 'dumb': 405,\n",
                            " 'during': 406,\n",
                            " 'each': 407,\n",
                            " 'early': 408,\n",
                            " 'eaten': 409,\n",
                            " 'eating': 410,\n",
                            " 'edge.': 411,\n",
                            " 'effected': 412,\n",
                            " 'either': 413,\n",
                            " 'elect': 414,\n",
                            " 'else.': 415,\n",
                            " 'emblazered': 416,\n",
                            " 'emotional': 417,\n",
                            " 'emotions': 418,\n",
                            " 'end,': 419,\n",
                            " 'enforce': 420,\n",
                            " 'enjoyable': 421,\n",
                            " 'enough': 422,\n",
                            " 'enough.': 423,\n",
                            " 'ensweatered': 424,\n",
                            " 'equally': 425,\n",
                            " 'even': 426,\n",
                            " 'every': 427,\n",
                            " 'everything': 428,\n",
                            " 'exactly': 429,\n",
                            " 'excellent.': 430,\n",
                            " 'experiment': 431,\n",
                            " 'explanation': 432,\n",
                            " 'exploits': 433,\n",
                            " 'extreme': 434,\n",
                            " 'eyes': 435,\n",
                            " 'fall': 436,\n",
                            " 'family': 437,\n",
                            " 'fashion,': 438,\n",
                            " 'fast': 439,\n",
                            " 'fathers': 440,\n",
                            " 'fell': 441,\n",
                            " 'female': 442,\n",
                            " 'few': 443,\n",
                            " 'filled': 444,\n",
                            " 'film': 445,\n",
                            " 'film,': 446,\n",
                            " 'filmmaker': 447,\n",
                            " 'films': 448,\n",
                            " 'films,': 449,\n",
                            " 'films.': 450,\n",
                            " 'finally:\u003cbr': 451,\n",
                            " 'find': 452,\n",
                            " 'finds': 453,\n",
                            " 'finely': 454,\n",
                            " 'fired.': 455,\n",
                            " 'first': 456,\n",
                            " 'folks;': 457,\n",
                            " 'for': 458,\n",
                            " 'found': 459,\n",
                            " 'fourth': 460,\n",
                            " 'frenzy': 461,\n",
                            " 'from': 462,\n",
                            " 'fruits': 463,\n",
                            " 'full': 464,\n",
                            " 'gal!)\u003cbr': 465,\n",
                            " 'gave': 466,\n",
                            " 'gently': 467,\n",
                            " 'genuine': 468,\n",
                            " 'get': 469,\n",
                            " 'gets': 470,\n",
                            " 'girl': 471,\n",
                            " 'girl,': 472,\n",
                            " 'give': 473,\n",
                            " 'glamourous': 474,\n",
                            " 'glitzy,': 475,\n",
                            " 'go': 476,\n",
                            " 'going': 477,\n",
                            " 'gold': 478,\n",
                            " 'good': 479,\n",
                            " 'goodness': 480,\n",
                            " 'gorgeous.': 481,\n",
                            " 'gradations': 482,\n",
                            " 'graduation.': 483,\n",
                            " 'great': 484,\n",
                            " 'guess': 485,\n",
                            " 'gunfighters': 486,\n",
                            " 'guy': 487,\n",
                            " 'had': 488,\n",
                            " 'happen': 489,\n",
                            " 'happen,': 490,\n",
                            " 'happened': 491,\n",
                            " 'happening': 492,\n",
                            " 'has': 493,\n",
                            " 'hated': 494,\n",
                            " 'have': 495,\n",
                            " 'have:\u003cbr': 496,\n",
                            " 'having': 497,\n",
                            " 'head': 498,\n",
                            " 'her': 499,\n",
                            " 'here': 500,\n",
                            " 'highly': 501,\n",
                            " 'him': 502,\n",
                            " 'his': 503,\n",
                            " 'history.': 504,\n",
                            " 'homage': 505,\n",
                            " 'home': 506,\n",
                            " 'hours.': 507,\n",
                            " 'how': 508,\n",
                            " 'howl': 509,\n",
                            " 'humor': 510,\n",
                            " 'hush-hush': 511,\n",
                            " 'hypocrisy': 512,\n",
                            " 'hysteria': 513,\n",
                            " 'i': 514,\n",
                            " 'if': 515,\n",
                            " 'immense': 516,\n",
                            " 'in': 517,\n",
                            " 'in,': 518,\n",
                            " 'including,': 519,\n",
                            " 'individual': 520,\n",
                            " 'inside.': 521,\n",
                            " 'intensity.': 522,\n",
                            " 'interested': 523,\n",
                            " 'into': 524,\n",
                            " 'is': 525,\n",
                            " \"isn't\": 526,\n",
                            " 'it': 527,\n",
                            " \"it's\": 528,\n",
                            " 'it.': 529,\n",
                            " 'its': 530,\n",
                            " 'job': 531,\n",
                            " 'just': 532,\n",
                            " 'killed': 533,\n",
                            " 'kind': 534,\n",
                            " 'knowledge': 535,\n",
                            " 'known': 536,\n",
                            " 'last,': 537,\n",
                            " 'lastly,': 538,\n",
                            " 'later': 539,\n",
                            " 'law': 540,\n",
                            " 'least': 541,\n",
                            " 'let': 542,\n",
                            " 'libido.': 543,\n",
                            " 'life': 544,\n",
                            " 'like': 545,\n",
                            " 'live': 546,\n",
                            " 'local': 547,\n",
                            " 'loss,': 548,\n",
                            " 'lost': 549,\n",
                            " 'lot.': 550,\n",
                            " 'lots': 551,\n",
                            " 'love': 552,\n",
                            " 'love.': 553,\n",
                            " 'loved': 554,\n",
                            " 'lucky': 555,\n",
                            " 'ludicrous': 556,\n",
                            " 'lured': 557,\n",
                            " 'made': 558,\n",
                            " 'majority': 559,\n",
                            " 'make': 560,\n",
                            " 'making': 561,\n",
                            " 'man': 562,\n",
                            " 'marshal': 563,\n",
                            " 'marshal!)': 564,\n",
                            " 'marvels': 565,\n",
                            " 'matter': 566,\n",
                            " 'may': 567,\n",
                            " 'me': 568,\n",
                            " 'meaning.': 569,\n",
                            " 'meant': 570,\n",
                            " 'measured': 571,\n",
                            " 'mellow': 572,\n",
                            " 'men': 573,\n",
                            " 'mentioned,': 574,\n",
                            " 'microscopically': 575,\n",
                            " 'mind': 576,\n",
                            " 'mine)': 577,\n",
                            " 'missed': 578,\n",
                            " 'mistaken': 579,\n",
                            " 'moist.': 580,\n",
                            " 'more.': 581,\n",
                            " 'most': 582,\n",
                            " 'mostly': 583,\n",
                            " 'mother,': 584,\n",
                            " 'movie': 585,\n",
                            " \"movie's\": 586,\n",
                            " 'movie.': 587,\n",
                            " 'movies': 588,\n",
                            " 'much': 589,\n",
                            " 'musical': 590,\n",
                            " 'musician,': 591,\n",
                            " 'must': 592,\n",
                            " 'name': 593,\n",
                            " 'name.': 594,\n",
                            " 'nature': 595,\n",
                            " 'never': 596,\n",
                            " 'nightmare': 597,\n",
                            " 'nineties': 598,\n",
                            " 'no': 599,\n",
                            " 'nor': 600,\n",
                            " 'nostalgic': 601,\n",
                            " 'not': 602,\n",
                            " 'not.': 603,\n",
                            " 'nothing': 604,\n",
                            " 'novel.\u003cbr': 605,\n",
                            " \"novelist's\": 606,\n",
                            " 'novels\":': 607,\n",
                            " 'novels:': 608,\n",
                            " 'now': 609,\n",
                            " 'nude': 610,\n",
                            " 'occasion': 611,\n",
                            " 'of': 612,\n",
                            " 'off': 613,\n",
                            " 'off.': 614,\n",
                            " 'offensive': 615,\n",
                            " 'office': 616,\n",
                            " 'old': 617,\n",
                            " 'oldest': 618,\n",
                            " 'on': 619,\n",
                            " 'on-screen': 620,\n",
                            " 'once.': 621,\n",
                            " 'one': 622,\n",
                            " \"one's\": 623,\n",
                            " 'one-dimensional': 624,\n",
                            " 'only': 625,\n",
                            " 'or': 626,\n",
                            " 'or,': 627,\n",
                            " 'oscillators': 628,\n",
                            " 'other': 629,\n",
                            " 'others': 630,\n",
                            " 'out': 631,\n",
                            " 'outdoors': 632,\n",
                            " 'outside': 633,\n",
                            " 'outstanding,': 634,\n",
                            " 'own': 635,\n",
                            " 'paddle': 636,\n",
                            " 'paid': 637,\n",
                            " 'pair': 638,\n",
                            " 'parents': 639,\n",
                            " 'part': 640,\n",
                            " 'past': 641,\n",
                            " 'pathetic': 642,\n",
                            " 'people': 643,\n",
                            " 'people.': 644,\n",
                            " 'perfect': 645,\n",
                            " 'performances': 646,\n",
                            " 'perhaps': 647,\n",
                            " 'phony,': 648,\n",
                            " 'photographs': 649,\n",
                            " 'picture.': 650,\n",
                            " 'piece.': 651,\n",
                            " 'play': 652,\n",
                            " 'played': 653,\n",
                            " 'plays': 654,\n",
                            " 'plot': 655,\n",
                            " 'plot,': 656,\n",
                            " 'plug': 657,\n",
                            " 'poignant': 658,\n",
                            " 'pointlessly': 659,\n",
                            " 'popular': 660,\n",
                            " 'portrayal': 661,\n",
                            " 'position.': 662,\n",
                            " 'praise': 663,\n",
                            " 'precise,': 664,\n",
                            " 'prepared': 665,\n",
                            " 'pressure': 666,\n",
                            " 'pressure,': 667,\n",
                            " 'prime': 668,\n",
                            " 'prison': 669,\n",
                            " 'producers': 670,\n",
                            " 'propaganda': 671,\n",
                            " 'proud': 672,\n",
                            " 'pseudo-love': 673,\n",
                            " 'pursue': 674,\n",
                            " 'pursued,': 675,\n",
                            " 'pursuers': 676,\n",
                            " 'pursuing': 677,\n",
                            " 'puts': 678,\n",
                            " 'quite': 679,\n",
                            " 'range': 680,\n",
                            " 'rapids': 681,\n",
                            " 're-makings': 682,\n",
                            " 'reaches': 683,\n",
                            " 'read': 684,\n",
                            " 'reading': 685,\n",
                            " 'real': 686,\n",
                            " 'really': 687,\n",
                            " 'reason': 688,\n",
                            " 'rebels': 689,\n",
                            " 'received.': 690,\n",
                            " 'recommend': 691,\n",
                            " 'redeem': 692,\n",
                            " 'remotely': 693,\n",
                            " 'resembling': 694,\n",
                            " 'resonance': 695,\n",
                            " 'rest': 696,\n",
                            " 'revolutions.': 697,\n",
                            " 'ridiculous': 698,\n",
                            " 'right,': 699,\n",
                            " 'right?\u003cbr': 700,\n",
                            " 'rightly': 701,\n",
                            " 'rising': 702,\n",
                            " 'role': 703,\n",
                            " 'roles': 704,\n",
                            " 'romance': 705,\n",
                            " 'row': 706,\n",
                            " 'rubbish.': 707,\n",
                            " 'ruining': 708,\n",
                            " 'running': 709,\n",
                            " 'rush.': 710,\n",
                            " 'said': 711,\n",
                            " 'same': 712,\n",
                            " 'say': 713,\n",
                            " 'says': 714,\n",
                            " 'scale': 715,\n",
                            " 'scene': 716,\n",
                            " 'scenes': 717,\n",
                            " 'schools,': 718,\n",
                            " 'sci-fi': 719,\n",
                            " 'script.': 720,\n",
                            " 'see': 721,\n",
                            " 'seeing': 722,\n",
                            " 'seem': 723,\n",
                            " 'seemed': 724,\n",
                            " 'seen': 725,\n",
                            " 'sense': 726,\n",
                            " 'sensitive': 727,\n",
                            " 'serving': 728,\n",
                            " 'set': 729,\n",
                            " 'sette': 730,\n",
                            " 'setting': 731,\n",
                            " 'sexiest': 732,\n",
                            " 'sexual': 733,\n",
                            " 'sexy': 734,\n",
                            " 'shake': 735,\n",
                            " 'shall': 736,\n",
                            " 'sharp': 737,\n",
                            " 'she': 738,\n",
                            " \"she's\": 739,\n",
                            " 'shooting': 740,\n",
                            " 'should': 741,\n",
                            " 'show': 742,\n",
                            " 'show.': 743,\n",
                            " 'shown': 744,\n",
                            " 'shows': 745,\n",
                            " 'side': 746,\n",
                            " 'side.': 747,\n",
                            " 'sign:': 748,\n",
                            " 'simply': 749,\n",
                            " 'sis': 750,\n",
                            " 'sit': 751,\n",
                            " 'sixties': 752,\n",
                            " 'sixties.': 753,\n",
                            " 'slow': 754,\n",
                            " 'snowy': 755,\n",
                            " 'so': 756,\n",
                            " 'so.': 757,\n",
                            " 'some': 758,\n",
                            " 'somehow,': 759,\n",
                            " 'soporific': 760,\n",
                            " 'sort.\u003cbr': 761,\n",
                            " 'soul': 762,\n",
                            " 'speaking': 763,\n",
                            " 'spectacular': 764,\n",
                            " 'spelling,': 765,\n",
                            " 'spirit': 766,\n",
                            " 'spotlight': 767,\n",
                            " 'squeeze.': 768,\n",
                            " 'standard)': 769,\n",
                            " 'stars': 770,\n",
                            " 'still': 771,\n",
                            " 'story': 772,\n",
                            " 'story,': 773,\n",
                            " 'storyline.': 774,\n",
                            " 'streets': 775,\n",
                            " 'subject.': 776,\n",
                            " 'such': 777,\n",
                            " 'suffering': 778,\n",
                            " 'sum': 779,\n",
                            " 'sunlight!\"\u003cbr': 780,\n",
                            " 'superb': 781,\n",
                            " 'supporting': 782,\n",
                            " 'sympathise': 783,\n",
                            " 'sympathy': 784,\n",
                            " 'symptoms.': 785,\n",
                            " 'target': 786,\n",
                            " 'tea': 787,\n",
                            " 'teenage': 788,\n",
                            " 'tell': 789,\n",
                            " 'telling': 790,\n",
                            " 'tells': 791,\n",
                            " 'temp-jobs': 792,\n",
                            " 'terrible': 793,\n",
                            " 'than': 794,\n",
                            " 'that': 795,\n",
                            " \"that's\": 796,\n",
                            " 'that.': 797,\n",
                            " 'thats': 798,\n",
                            " 'the': 799,\n",
                            " 'their': 800,\n",
                            " 'them': 801,\n",
                            " 'them.': 802,\n",
                            " 'themselves': 803,\n",
                            " 'themselves,': 804,\n",
                            " 'then': 805,\n",
                            " 'there': 806,\n",
                            " 'these': 807,\n",
                            " 'they': 808,\n",
                            " 'things': 809,\n",
                            " 'this': 810,\n",
                            " 'this,': 811,\n",
                            " 'those': 812,\n",
                            " 'thought': 813,\n",
                            " 'three': 814,\n",
                            " 'three.': 815,\n",
                            " 'through': 816,\n",
                            " 'time': 817,\n",
                            " 'tired,': 818,\n",
                            " 'to': 819,\n",
                            " 'today,': 820,\n",
                            " 'took': 821,\n",
                            " 'tormented': 822,\n",
                            " 'town?': 823,\n",
                            " 'toy-boy': 824,\n",
                            " 'toy-boy,': 825,\n",
                            " 'trauma,': 826,\n",
                            " 'true': 827,\n",
                            " 'tuned': 828,\n",
                            " 'turned': 829,\n",
                            " 'two': 830,\n",
                            " 'type': 831,\n",
                            " 'ultimate': 832,\n",
                            " 'unbearable': 833,\n",
                            " 'up': 834,\n",
                            " 'up:': 835,\n",
                            " 'us': 836,\n",
                            " 'us.\u003cbr': 837,\n",
                            " 'use': 838,\n",
                            " 'uses': 839,\n",
                            " 'usually': 840,\n",
                            " 'version': 841,\n",
                            " 'very': 842,\n",
                            " 'vibrating': 843,\n",
                            " 'viewers': 844,\n",
                            " 'waiting': 845,\n",
                            " 'waiting!': 846,\n",
                            " 'want': 847,\n",
                            " 'warm': 848,\n",
                            " 'warn': 849,\n",
                            " 'warning': 850,\n",
                            " 'was': 851,\n",
                            " 'was,': 852,\n",
                            " \"wasn't\": 853,\n",
                            " 'watched': 854,\n",
                            " 'waters,': 855,\n",
                            " 'we': 856,\n",
                            " 'well.': 857,\n",
                            " 'were': 858,\n",
                            " 'what': 859,\n",
                            " 'when': 860,\n",
                            " 'which': 861,\n",
                            " 'who': 862,\n",
                            " 'whole': 863,\n",
                            " 'whom': 864,\n",
                            " 'why.': 865,\n",
                            " 'will': 866,\n",
                            " 'wish': 867,\n",
                            " 'with': 868,\n",
                            " 'within': 869,\n",
                            " 'without': 870,\n",
                            " 'witness': 871,\n",
                            " 'witty': 872,\n",
                            " 'woman': 873,\n",
                            " 'women': 874,\n",
                            " 'wonder': 875,\n",
                            " 'work': 876,\n",
                            " 'working': 877,\n",
                            " 'world': 878,\n",
                            " 'world.\"': 879,\n",
                            " 'worst': 880,\n",
                            " 'would': 881,\n",
                            " 'write': 882,\n",
                            " 'written': 883,\n",
                            " 'wrote': 884,\n",
                            " 'year': 885,\n",
                            " 'years': 886,\n",
                            " 'you': 887,\n",
                            " 'young': 888,\n",
                            " 'younger': 889,\n",
                            " 'your': 890,\n",
                            " 'yours.\u003cbr': 891}"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "word2idx"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We repeat the process, this time mapping integers to words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "idx2word = {idx: word for idx, word in enumerate(vocab)}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{0: '\"Absolute',\n",
                            " 1: '\"Bohlen\"-Fan',\n",
                            " 2: '\"Brideshead',\n",
                            " 3: '\"Candy\"?).',\n",
                            " 4: '\"City',\n",
                            " 5: '\"Dieter',\n",
                            " 6: '\"Dieter\"',\n",
                            " 7: '\"Dragonfly\"',\n",
                            " 8: '\"I\\'ve',\n",
                            " 9: '\"Lady.\"\u003cbr',\n",
                            " 10: '\"London',\n",
                            " 11: '\"Make',\n",
                            " 12: '\"Miss\"',\n",
                            " 13: '\"Mr',\n",
                            " 14: '\"Mrs.\"',\n",
                            " 15: '\"actors\"',\n",
                            " 16: '\"dewy-eyed.\"\u003cbr',\n",
                            " 17: '\"hey,',\n",
                            " 18: '\"meanwhile,\")',\n",
                            " 19: '\"men\"',\n",
                            " 20: \"'Where\",\n",
                            " 21: \"'em\",\n",
                            " 22: \"'round\",\n",
                            " 23: '(Backbone',\n",
                            " 24: '(Barbarella).',\n",
                            " 25: '(Brit',\n",
                            " 26: '(I',\n",
                            " 27: '(Jeremy',\n",
                            " 28: '(Remember',\n",
                            " 29: '(YOU',\n",
                            " 30: '(as',\n",
                            " 31: '(at',\n",
                            " 32: '(not',\n",
                            " 33: '(the',\n",
                            " 34: \"(there's\",\n",
                            " 35: '(they',\n",
                            " 36: '(we',\n",
                            " 37: '(what',\n",
                            " 38: '(when,',\n",
                            " 39: '(yes',\n",
                            " 40: '-',\n",
                            " 41: '.',\n",
                            " 42: '/\u003e\u003cbr',\n",
                            " 43: '/\u003eAh,',\n",
                            " 44: '/\u003eAnd',\n",
                            " 45: '/\u003eBut',\n",
                            " 46: '/\u003eCanadian',\n",
                            " 47: '/\u003eDavid',\n",
                            " 48: '/\u003eFirst',\n",
                            " 49: '/\u003eHenceforth,',\n",
                            " 50: '/\u003eJoanna',\n",
                            " 51: '/\u003eJournalist',\n",
                            " 52: '/\u003eNothing',\n",
                            " 53: '/\u003eOK,',\n",
                            " 54: '/\u003ePenelope',\n",
                            " 55: '/\u003ePeter',\n",
                            " 56: '/\u003eSecond',\n",
                            " 57: '/\u003eSo',\n",
                            " 58: '/\u003eSusan',\n",
                            " 59: '/\u003eThank',\n",
                            " 60: '/\u003eThird',\n",
                            " 61: '/\u003eTo',\n",
                            " 62: '/\u003eWhen',\n",
                            " 63: '/\u003eWrong!',\n",
                            " 64: '/\u003eand',\n",
                            " 65: '1-dimensional',\n",
                            " 66: '14',\n",
                            " 67: '1950s',\n",
                            " 68: '20',\n",
                            " 69: '\u003cPAD\u003e',\n",
                            " 70: '\u003cbr',\n",
                            " 71: 'A',\n",
                            " 72: 'After',\n",
                            " 73: 'Alberta',\n",
                            " 74: 'Alison',\n",
                            " 75: 'Alonso',\n",
                            " 76: 'American',\n",
                            " 77: 'And',\n",
                            " 78: 'As',\n",
                            " 79: 'B.B.E.',\n",
                            " 80: 'Beginners\",',\n",
                            " 81: 'Boarding-School,',\n",
                            " 82: 'Bohlen\"',\n",
                            " 83: 'Both',\n",
                            " 84: 'Brennan',\n",
                            " 85: 'Bulimia',\n",
                            " 86: 'But',\n",
                            " 87: 'Cage',\n",
                            " 88: 'Canadian',\n",
                            " 89: 'Cher',\n",
                            " 90: 'Christmas!)',\n",
                            " 91: 'Christopher',\n",
                            " 92: 'Circus.\u003cbr',\n",
                            " 93: 'City',\n",
                            " 94: 'City,',\n",
                            " 95: 'Col.',\n",
                            " 96: 'Colin',\n",
                            " 97: 'Colonel',\n",
                            " 98: 'Columbian',\n",
                            " 99: 'Conchita',\n",
                            " 100: 'Constantly',\n",
                            " 101: 'Coppola',\n",
                            " 102: 'Cricket',\n",
                            " 103: \"Cricket's\",\n",
                            " 104: 'Davies)',\n",
                            " 105: 'Dawson',\n",
                            " 106: 'Deadwood,',\n",
                            " 107: 'Dean',\n",
                            " 108: 'Depardieu,',\n",
                            " 109: \"Don't\",\n",
                            " 110: 'Dragonfly',\n",
                            " 111: 'Emily',\n",
                            " 112: 'England)',\n",
                            " 113: 'England.)',\n",
                            " 114: 'European',\n",
                            " 115: 'Even',\n",
                            " 116: 'Film',\n",
                            " 117: 'First',\n",
                            " 118: 'Gerard',\n",
                            " 119: 'German',\n",
                            " 120: 'Giancarlo',\n",
                            " 121: 'Giannini',\n",
                            " 122: \"Girls'\",\n",
                            " 123: 'Hampshire',\n",
                            " 124: 'He',\n",
                            " 125: 'Headmistress',\n",
                            " 126: 'Her',\n",
                            " 127: 'Herringbone-Tweed,',\n",
                            " 128: 'Hollywood',\n",
                            " 129: 'Home',\n",
                            " 130: 'However',\n",
                            " 131: 'I',\n",
                            " 132: \"I'll\",\n",
                            " 133: \"I'm\",\n",
                            " 134: 'II',\n",
                            " 135: 'If',\n",
                            " 136: 'Ironside.',\n",
                            " 137: 'It',\n",
                            " 138: \"It's\",\n",
                            " 139: 'Japanese',\n",
                            " 140: 'Jimmy',\n",
                            " 141: 'John',\n",
                            " 142: 'Justice\".',\n",
                            " 143: 'Katt',\n",
                            " 144: 'Keith',\n",
                            " 145: 'Klondike',\n",
                            " 146: 'L.L.',\n",
                            " 147: 'L.L.\u003cbr',\n",
                            " 148: 'Lady',\n",
                            " 149: 'Law',\n",
                            " 150: 'Leading',\n",
                            " 151: \"Lies'.\",\n",
                            " 152: 'Lohman',\n",
                            " 153: \"Lohman's.\",\n",
                            " 154: 'Lohman,',\n",
                            " 155: 'London,',\n",
                            " 156: 'Lord',\n",
                            " 157: 'Love',\n",
                            " 158: 'Lumley',\n",
                            " 159: 'Madness',\n",
                            " 160: 'Mann',\n",
                            " 161: 'Manor,',\n",
                            " 162: 'Manor.\u003cbr',\n",
                            " 163: 'Mare',\n",
                            " 164: 'Maria',\n",
                            " 165: 'McCallum',\n",
                            " 166: 'McInnes',\n",
                            " 167: \"McInnes's\",\n",
                            " 168: 'Meanwhile',\n",
                            " 169: 'Michael',\n",
                            " 170: 'Miss',\n",
                            " 171: 'Mortimer',\n",
                            " 172: 'Mountains',\n",
                            " 173: 'Mountie',\n",
                            " 174: 'Mr.',\n",
                            " 175: 'Nancherrow',\n",
                            " 176: 'New',\n",
                            " 177: 'Nicolas',\n",
                            " 178: 'North',\n",
                            " 179: \"O'Toole\",\n",
                            " 180: 'OK-movie.',\n",
                            " 181: 'Okay,',\n",
                            " 182: \"Ol'\",\n",
                            " 183: 'Our',\n",
                            " 184: 'Phillip',\n",
                            " 185: 'Pilcher',\n",
                            " 186: 'Polonia',\n",
                            " 187: 'Reefer',\n",
                            " 188: 'Revisited,\"',\n",
                            " 189: 'Rocky',\n",
                            " 190: 'Roman',\n",
                            " 191: \"She's\",\n",
                            " 192: 'Shea',\n",
                            " 193: 'So',\n",
                            " 194: 'Spades\"',\n",
                            " 195: 'Speaking',\n",
                            " 196: 'Stately',\n",
                            " 197: 'Stewart',\n",
                            " 198: 'Still,',\n",
                            " 199: 'Stockwell',\n",
                            " 200: 'Sunday',\n",
                            " 201: 'Sure,',\n",
                            " 202: 'Susan',\n",
                            " 203: 'Teacups,',\n",
                            " 204: 'Technically',\n",
                            " 205: \"That's\",\n",
                            " 206: 'The',\n",
                            " 207: 'There',\n",
                            " 208: 'They',\n",
                            " 209: \"They're\",\n",
                            " 210: 'Things',\n",
                            " 211: 'This',\n",
                            " 212: 'Together,',\n",
                            " 213: 'Truth',\n",
                            " 214: 'US',\n",
                            " 215: 'Venerable',\n",
                            " 216: 'Walken',\n",
                            " 217: \"Walken's\",\n",
                            " 218: 'Walter',\n",
                            " 219: 'War',\n",
                            " 220: 'Well,',\n",
                            " 221: 'West.\u003cbr',\n",
                            " 222: 'When',\n",
                            " 223: 'Wild',\n",
                            " 224: 'Winningham',\n",
                            " 225: 'Wonderful',\n",
                            " 226: 'World',\n",
                            " 227: 'York',\n",
                            " 228: 'Yukon',\n",
                            " 229: 'a',\n",
                            " 230: 'ability',\n",
                            " 231: 'ably',\n",
                            " 232: 'about.',\n",
                            " 233: 'absolutely',\n",
                            " 234: 'acclaimed;',\n",
                            " 235: 'accord',\n",
                            " 236: 'accuracy.',\n",
                            " 237: 'accurate',\n",
                            " 238: 'achievement,',\n",
                            " 239: 'act',\n",
                            " 240: 'acting',\n",
                            " 241: 'action',\n",
                            " 242: 'actor',\n",
                            " 243: \"actor's\",\n",
                            " 244: 'actors',\n",
                            " 245: 'actors,',\n",
                            " 246: \"actress's\",\n",
                            " 247: 'actresses',\n",
                            " 248: 'actually',\n",
                            " 249: 'admit,',\n",
                            " 250: 'advice',\n",
                            " 251: 'advice:',\n",
                            " 252: 'affair',\n",
                            " 253: 'after',\n",
                            " 254: 'afternoon',\n",
                            " 255: 'age',\n",
                            " 256: 'age!).',\n",
                            " 257: 'ago',\n",
                            " 258: 'ahead',\n",
                            " 259: 'air',\n",
                            " 260: 'alive',\n",
                            " 261: 'all',\n",
                            " 262: 'all,',\n",
                            " 263: 'all.',\n",
                            " 264: 'all.\u003cbr',\n",
                            " 265: 'along.',\n",
                            " 266: 'alright,',\n",
                            " 267: 'always',\n",
                            " 268: 'always)',\n",
                            " 269: 'am',\n",
                            " 270: 'amazingly',\n",
                            " 271: 'amusing',\n",
                            " 272: 'an',\n",
                            " 273: 'and',\n",
                            " 274: 'anguish',\n",
                            " 275: 'animal',\n",
                            " 276: 'another!)\u003cbr',\n",
                            " 277: 'another.',\n",
                            " 278: 'any',\n",
                            " 279: 'anybody',\n",
                            " 280: 'anything',\n",
                            " 281: 'apology',\n",
                            " 282: 'appear',\n",
                            " 283: 'appeared',\n",
                            " 284: 'are',\n",
                            " 285: 'arm-chair',\n",
                            " 286: 'around',\n",
                            " 287: 'around,',\n",
                            " 288: 'as',\n",
                            " 289: 'asleep',\n",
                            " 290: 'aspiring',\n",
                            " 291: 'astonishing',\n",
                            " 292: 'at',\n",
                            " 293: 'autobiography',\n",
                            " 294: 'award',\n",
                            " 295: 'baby',\n",
                            " 296: 'backbone!\u003cbr',\n",
                            " 297: 'backlighting.',\n",
                            " 298: 'badly-acted,',\n",
                            " 299: 'barely',\n",
                            " 300: 'barometers',\n",
                            " 301: 'based',\n",
                            " 302: 'battling',\n",
                            " 303: 'be',\n",
                            " 304: 'beautiful',\n",
                            " 305: 'because',\n",
                            " 306: 'become',\n",
                            " 307: 'becomes',\n",
                            " 308: 'been',\n",
                            " 309: 'before',\n",
                            " 310: 'behaviour',\n",
                            " 311: 'being',\n",
                            " 312: 'best',\n",
                            " 313: 'best,',\n",
                            " 314: 'best.',\n",
                            " 315: 'better',\n",
                            " 316: 'big',\n",
                            " 317: 'bit',\n",
                            " 318: 'blockbuster,',\n",
                            " 319: 'body',\n",
                            " 320: 'border',\n",
                            " 321: 'boring.',\n",
                            " 322: 'boy',\n",
                            " 323: 'boy.',\n",
                            " 324: 'brilliant',\n",
                            " 325: 'bulimia',\n",
                            " 326: 'business',\n",
                            " 327: 'but',\n",
                            " 328: 'but,',\n",
                            " 329: 'by',\n",
                            " 330: 'by,',\n",
                            " 331: 'called',\n",
                            " 332: 'cameos',\n",
                            " 333: 'camps',\n",
                            " 334: 'can',\n",
                            " 335: 'can\"!',\n",
                            " 336: \"can't\",\n",
                            " 337: 'cant',\n",
                            " 338: 'captures',\n",
                            " 339: 'cases',\n",
                            " 340: 'cast',\n",
                            " 341: 'catatonic.',\n",
                            " 342: 'causes',\n",
                            " 343: 'causing',\n",
                            " 344: 'character',\n",
                            " 345: 'choice',\n",
                            " 346: 'cinema',\n",
                            " 347: 'colonel',\n",
                            " 348: 'combination',\n",
                            " 349: 'come',\n",
                            " 350: 'comes',\n",
                            " 351: 'comfortable',\n",
                            " 352: 'company',\n",
                            " 353: 'complete',\n",
                            " 354: 'compulsive',\n",
                            " 355: 'concern',\n",
                            " 356: 'consent',\n",
                            " 357: 'considerate',\n",
                            " 358: 'constant.',\n",
                            " 359: 'contacts?)\u003cbr',\n",
                            " 360: 'content',\n",
                            " 361: 'continue',\n",
                            " 362: 'control',\n",
                            " 363: 'convey',\n",
                            " 364: 'could',\n",
                            " 365: \"couldn't\",\n",
                            " 366: 'couple',\n",
                            " 367: 'courage',\n",
                            " 368: 'course)',\n",
                            " 369: 'cover',\n",
                            " 370: 'cries',\n",
                            " 371: 'criticize.',\n",
                            " 372: 'cross,',\n",
                            " 373: 'crush',\n",
                            " 374: 'crying,',\n",
                            " 375: 'dangerous',\n",
                            " 376: 'dash',\n",
                            " 377: 'days',\n",
                            " 378: 'dead',\n",
                            " 379: 'decide',\n",
                            " 380: 'deep,',\n",
                            " 381: 'degree',\n",
                            " 382: 'depends',\n",
                            " 383: 'depths',\n",
                            " 384: 'descend',\n",
                            " 385: 'deserves',\n",
                            " 386: 'despair,',\n",
                            " 387: 'despair.',\n",
                            " 388: 'desperation',\n",
                            " 389: 'destroy',\n",
                            " 390: 'development',\n",
                            " 391: 'devoid',\n",
                            " 392: 'did',\n",
                            " 393: \"didn't\",\n",
                            " 394: 'directed',\n",
                            " 395: 'director',\n",
                            " 396: 'disappointed',\n",
                            " 397: 'disgust.',\n",
                            " 398: 'disorder.',\n",
                            " 399: 'do.',\n",
                            " 400: 'does',\n",
                            " 401: \"don't\",\n",
                            " 402: 'done,',\n",
                            " 403: 'done.\u003cbr',\n",
                            " 404: 'due',\n",
                            " 405: 'dumb',\n",
                            " 406: 'during',\n",
                            " 407: 'each',\n",
                            " 408: 'early',\n",
                            " 409: 'eaten',\n",
                            " 410: 'eating',\n",
                            " 411: 'edge.',\n",
                            " 412: 'effected',\n",
                            " 413: 'either',\n",
                            " 414: 'elect',\n",
                            " 415: 'else.',\n",
                            " 416: 'emblazered',\n",
                            " 417: 'emotional',\n",
                            " 418: 'emotions',\n",
                            " 419: 'end,',\n",
                            " 420: 'enforce',\n",
                            " 421: 'enjoyable',\n",
                            " 422: 'enough',\n",
                            " 423: 'enough.',\n",
                            " 424: 'ensweatered',\n",
                            " 425: 'equally',\n",
                            " 426: 'even',\n",
                            " 427: 'every',\n",
                            " 428: 'everything',\n",
                            " 429: 'exactly',\n",
                            " 430: 'excellent.',\n",
                            " 431: 'experiment',\n",
                            " 432: 'explanation',\n",
                            " 433: 'exploits',\n",
                            " 434: 'extreme',\n",
                            " 435: 'eyes',\n",
                            " 436: 'fall',\n",
                            " 437: 'family',\n",
                            " 438: 'fashion,',\n",
                            " 439: 'fast',\n",
                            " 440: 'fathers',\n",
                            " 441: 'fell',\n",
                            " 442: 'female',\n",
                            " 443: 'few',\n",
                            " 444: 'filled',\n",
                            " 445: 'film',\n",
                            " 446: 'film,',\n",
                            " 447: 'filmmaker',\n",
                            " 448: 'films',\n",
                            " 449: 'films,',\n",
                            " 450: 'films.',\n",
                            " 451: 'finally:\u003cbr',\n",
                            " 452: 'find',\n",
                            " 453: 'finds',\n",
                            " 454: 'finely',\n",
                            " 455: 'fired.',\n",
                            " 456: 'first',\n",
                            " 457: 'folks;',\n",
                            " 458: 'for',\n",
                            " 459: 'found',\n",
                            " 460: 'fourth',\n",
                            " 461: 'frenzy',\n",
                            " 462: 'from',\n",
                            " 463: 'fruits',\n",
                            " 464: 'full',\n",
                            " 465: 'gal!)\u003cbr',\n",
                            " 466: 'gave',\n",
                            " 467: 'gently',\n",
                            " 468: 'genuine',\n",
                            " 469: 'get',\n",
                            " 470: 'gets',\n",
                            " 471: 'girl',\n",
                            " 472: 'girl,',\n",
                            " 473: 'give',\n",
                            " 474: 'glamourous',\n",
                            " 475: 'glitzy,',\n",
                            " 476: 'go',\n",
                            " 477: 'going',\n",
                            " 478: 'gold',\n",
                            " 479: 'good',\n",
                            " 480: 'goodness',\n",
                            " 481: 'gorgeous.',\n",
                            " 482: 'gradations',\n",
                            " 483: 'graduation.',\n",
                            " 484: 'great',\n",
                            " 485: 'guess',\n",
                            " 486: 'gunfighters',\n",
                            " 487: 'guy',\n",
                            " 488: 'had',\n",
                            " 489: 'happen',\n",
                            " 490: 'happen,',\n",
                            " 491: 'happened',\n",
                            " 492: 'happening',\n",
                            " 493: 'has',\n",
                            " 494: 'hated',\n",
                            " 495: 'have',\n",
                            " 496: 'have:\u003cbr',\n",
                            " 497: 'having',\n",
                            " 498: 'head',\n",
                            " 499: 'her',\n",
                            " 500: 'here',\n",
                            " 501: 'highly',\n",
                            " 502: 'him',\n",
                            " 503: 'his',\n",
                            " 504: 'history.',\n",
                            " 505: 'homage',\n",
                            " 506: 'home',\n",
                            " 507: 'hours.',\n",
                            " 508: 'how',\n",
                            " 509: 'howl',\n",
                            " 510: 'humor',\n",
                            " 511: 'hush-hush',\n",
                            " 512: 'hypocrisy',\n",
                            " 513: 'hysteria',\n",
                            " 514: 'i',\n",
                            " 515: 'if',\n",
                            " 516: 'immense',\n",
                            " 517: 'in',\n",
                            " 518: 'in,',\n",
                            " 519: 'including,',\n",
                            " 520: 'individual',\n",
                            " 521: 'inside.',\n",
                            " 522: 'intensity.',\n",
                            " 523: 'interested',\n",
                            " 524: 'into',\n",
                            " 525: 'is',\n",
                            " 526: \"isn't\",\n",
                            " 527: 'it',\n",
                            " 528: \"it's\",\n",
                            " 529: 'it.',\n",
                            " 530: 'its',\n",
                            " 531: 'job',\n",
                            " 532: 'just',\n",
                            " 533: 'killed',\n",
                            " 534: 'kind',\n",
                            " 535: 'knowledge',\n",
                            " 536: 'known',\n",
                            " 537: 'last,',\n",
                            " 538: 'lastly,',\n",
                            " 539: 'later',\n",
                            " 540: 'law',\n",
                            " 541: 'least',\n",
                            " 542: 'let',\n",
                            " 543: 'libido.',\n",
                            " 544: 'life',\n",
                            " 545: 'like',\n",
                            " 546: 'live',\n",
                            " 547: 'local',\n",
                            " 548: 'loss,',\n",
                            " 549: 'lost',\n",
                            " 550: 'lot.',\n",
                            " 551: 'lots',\n",
                            " 552: 'love',\n",
                            " 553: 'love.',\n",
                            " 554: 'loved',\n",
                            " 555: 'lucky',\n",
                            " 556: 'ludicrous',\n",
                            " 557: 'lured',\n",
                            " 558: 'made',\n",
                            " 559: 'majority',\n",
                            " 560: 'make',\n",
                            " 561: 'making',\n",
                            " 562: 'man',\n",
                            " 563: 'marshal',\n",
                            " 564: 'marshal!)',\n",
                            " 565: 'marvels',\n",
                            " 566: 'matter',\n",
                            " 567: 'may',\n",
                            " 568: 'me',\n",
                            " 569: 'meaning.',\n",
                            " 570: 'meant',\n",
                            " 571: 'measured',\n",
                            " 572: 'mellow',\n",
                            " 573: 'men',\n",
                            " 574: 'mentioned,',\n",
                            " 575: 'microscopically',\n",
                            " 576: 'mind',\n",
                            " 577: 'mine)',\n",
                            " 578: 'missed',\n",
                            " 579: 'mistaken',\n",
                            " 580: 'moist.',\n",
                            " 581: 'more.',\n",
                            " 582: 'most',\n",
                            " 583: 'mostly',\n",
                            " 584: 'mother,',\n",
                            " 585: 'movie',\n",
                            " 586: \"movie's\",\n",
                            " 587: 'movie.',\n",
                            " 588: 'movies',\n",
                            " 589: 'much',\n",
                            " 590: 'musical',\n",
                            " 591: 'musician,',\n",
                            " 592: 'must',\n",
                            " 593: 'name',\n",
                            " 594: 'name.',\n",
                            " 595: 'nature',\n",
                            " 596: 'never',\n",
                            " 597: 'nightmare',\n",
                            " 598: 'nineties',\n",
                            " 599: 'no',\n",
                            " 600: 'nor',\n",
                            " 601: 'nostalgic',\n",
                            " 602: 'not',\n",
                            " 603: 'not.',\n",
                            " 604: 'nothing',\n",
                            " 605: 'novel.\u003cbr',\n",
                            " 606: \"novelist's\",\n",
                            " 607: 'novels\":',\n",
                            " 608: 'novels:',\n",
                            " 609: 'now',\n",
                            " 610: 'nude',\n",
                            " 611: 'occasion',\n",
                            " 612: 'of',\n",
                            " 613: 'off',\n",
                            " 614: 'off.',\n",
                            " 615: 'offensive',\n",
                            " 616: 'office',\n",
                            " 617: 'old',\n",
                            " 618: 'oldest',\n",
                            " 619: 'on',\n",
                            " 620: 'on-screen',\n",
                            " 621: 'once.',\n",
                            " 622: 'one',\n",
                            " 623: \"one's\",\n",
                            " 624: 'one-dimensional',\n",
                            " 625: 'only',\n",
                            " 626: 'or',\n",
                            " 627: 'or,',\n",
                            " 628: 'oscillators',\n",
                            " 629: 'other',\n",
                            " 630: 'others',\n",
                            " 631: 'out',\n",
                            " 632: 'outdoors',\n",
                            " 633: 'outside',\n",
                            " 634: 'outstanding,',\n",
                            " 635: 'own',\n",
                            " 636: 'paddle',\n",
                            " 637: 'paid',\n",
                            " 638: 'pair',\n",
                            " 639: 'parents',\n",
                            " 640: 'part',\n",
                            " 641: 'past',\n",
                            " 642: 'pathetic',\n",
                            " 643: 'people',\n",
                            " 644: 'people.',\n",
                            " 645: 'perfect',\n",
                            " 646: 'performances',\n",
                            " 647: 'perhaps',\n",
                            " 648: 'phony,',\n",
                            " 649: 'photographs',\n",
                            " 650: 'picture.',\n",
                            " 651: 'piece.',\n",
                            " 652: 'play',\n",
                            " 653: 'played',\n",
                            " 654: 'plays',\n",
                            " 655: 'plot',\n",
                            " 656: 'plot,',\n",
                            " 657: 'plug',\n",
                            " 658: 'poignant',\n",
                            " 659: 'pointlessly',\n",
                            " 660: 'popular',\n",
                            " 661: 'portrayal',\n",
                            " 662: 'position.',\n",
                            " 663: 'praise',\n",
                            " 664: 'precise,',\n",
                            " 665: 'prepared',\n",
                            " 666: 'pressure',\n",
                            " 667: 'pressure,',\n",
                            " 668: 'prime',\n",
                            " 669: 'prison',\n",
                            " 670: 'producers',\n",
                            " 671: 'propaganda',\n",
                            " 672: 'proud',\n",
                            " 673: 'pseudo-love',\n",
                            " 674: 'pursue',\n",
                            " 675: 'pursued,',\n",
                            " 676: 'pursuers',\n",
                            " 677: 'pursuing',\n",
                            " 678: 'puts',\n",
                            " 679: 'quite',\n",
                            " 680: 'range',\n",
                            " 681: 'rapids',\n",
                            " 682: 're-makings',\n",
                            " 683: 'reaches',\n",
                            " 684: 'read',\n",
                            " 685: 'reading',\n",
                            " 686: 'real',\n",
                            " 687: 'really',\n",
                            " 688: 'reason',\n",
                            " 689: 'rebels',\n",
                            " 690: 'received.',\n",
                            " 691: 'recommend',\n",
                            " 692: 'redeem',\n",
                            " 693: 'remotely',\n",
                            " 694: 'resembling',\n",
                            " 695: 'resonance',\n",
                            " 696: 'rest',\n",
                            " 697: 'revolutions.',\n",
                            " 698: 'ridiculous',\n",
                            " 699: 'right,',\n",
                            " 700: 'right?\u003cbr',\n",
                            " 701: 'rightly',\n",
                            " 702: 'rising',\n",
                            " 703: 'role',\n",
                            " 704: 'roles',\n",
                            " 705: 'romance',\n",
                            " 706: 'row',\n",
                            " 707: 'rubbish.',\n",
                            " 708: 'ruining',\n",
                            " 709: 'running',\n",
                            " 710: 'rush.',\n",
                            " 711: 'said',\n",
                            " 712: 'same',\n",
                            " 713: 'say',\n",
                            " 714: 'says',\n",
                            " 715: 'scale',\n",
                            " 716: 'scene',\n",
                            " 717: 'scenes',\n",
                            " 718: 'schools,',\n",
                            " 719: 'sci-fi',\n",
                            " 720: 'script.',\n",
                            " 721: 'see',\n",
                            " 722: 'seeing',\n",
                            " 723: 'seem',\n",
                            " 724: 'seemed',\n",
                            " 725: 'seen',\n",
                            " 726: 'sense',\n",
                            " 727: 'sensitive',\n",
                            " 728: 'serving',\n",
                            " 729: 'set',\n",
                            " 730: 'sette',\n",
                            " 731: 'setting',\n",
                            " 732: 'sexiest',\n",
                            " 733: 'sexual',\n",
                            " 734: 'sexy',\n",
                            " 735: 'shake',\n",
                            " 736: 'shall',\n",
                            " 737: 'sharp',\n",
                            " 738: 'she',\n",
                            " 739: \"she's\",\n",
                            " 740: 'shooting',\n",
                            " 741: 'should',\n",
                            " 742: 'show',\n",
                            " 743: 'show.',\n",
                            " 744: 'shown',\n",
                            " 745: 'shows',\n",
                            " 746: 'side',\n",
                            " 747: 'side.',\n",
                            " 748: 'sign:',\n",
                            " 749: 'simply',\n",
                            " 750: 'sis',\n",
                            " 751: 'sit',\n",
                            " 752: 'sixties',\n",
                            " 753: 'sixties.',\n",
                            " 754: 'slow',\n",
                            " 755: 'snowy',\n",
                            " 756: 'so',\n",
                            " 757: 'so.',\n",
                            " 758: 'some',\n",
                            " 759: 'somehow,',\n",
                            " 760: 'soporific',\n",
                            " 761: 'sort.\u003cbr',\n",
                            " 762: 'soul',\n",
                            " 763: 'speaking',\n",
                            " 764: 'spectacular',\n",
                            " 765: 'spelling,',\n",
                            " 766: 'spirit',\n",
                            " 767: 'spotlight',\n",
                            " 768: 'squeeze.',\n",
                            " 769: 'standard)',\n",
                            " 770: 'stars',\n",
                            " 771: 'still',\n",
                            " 772: 'story',\n",
                            " 773: 'story,',\n",
                            " 774: 'storyline.',\n",
                            " 775: 'streets',\n",
                            " 776: 'subject.',\n",
                            " 777: 'such',\n",
                            " 778: 'suffering',\n",
                            " 779: 'sum',\n",
                            " 780: 'sunlight!\"\u003cbr',\n",
                            " 781: 'superb',\n",
                            " 782: 'supporting',\n",
                            " 783: 'sympathise',\n",
                            " 784: 'sympathy',\n",
                            " 785: 'symptoms.',\n",
                            " 786: 'target',\n",
                            " 787: 'tea',\n",
                            " 788: 'teenage',\n",
                            " 789: 'tell',\n",
                            " 790: 'telling',\n",
                            " 791: 'tells',\n",
                            " 792: 'temp-jobs',\n",
                            " 793: 'terrible',\n",
                            " 794: 'than',\n",
                            " 795: 'that',\n",
                            " 796: \"that's\",\n",
                            " 797: 'that.',\n",
                            " 798: 'thats',\n",
                            " 799: 'the',\n",
                            " 800: 'their',\n",
                            " 801: 'them',\n",
                            " 802: 'them.',\n",
                            " 803: 'themselves',\n",
                            " 804: 'themselves,',\n",
                            " 805: 'then',\n",
                            " 806: 'there',\n",
                            " 807: 'these',\n",
                            " 808: 'they',\n",
                            " 809: 'things',\n",
                            " 810: 'this',\n",
                            " 811: 'this,',\n",
                            " 812: 'those',\n",
                            " 813: 'thought',\n",
                            " 814: 'three',\n",
                            " 815: 'three.',\n",
                            " 816: 'through',\n",
                            " 817: 'time',\n",
                            " 818: 'tired,',\n",
                            " 819: 'to',\n",
                            " 820: 'today,',\n",
                            " 821: 'took',\n",
                            " 822: 'tormented',\n",
                            " 823: 'town?',\n",
                            " 824: 'toy-boy',\n",
                            " 825: 'toy-boy,',\n",
                            " 826: 'trauma,',\n",
                            " 827: 'true',\n",
                            " 828: 'tuned',\n",
                            " 829: 'turned',\n",
                            " 830: 'two',\n",
                            " 831: 'type',\n",
                            " 832: 'ultimate',\n",
                            " 833: 'unbearable',\n",
                            " 834: 'up',\n",
                            " 835: 'up:',\n",
                            " 836: 'us',\n",
                            " 837: 'us.\u003cbr',\n",
                            " 838: 'use',\n",
                            " 839: 'uses',\n",
                            " 840: 'usually',\n",
                            " 841: 'version',\n",
                            " 842: 'very',\n",
                            " 843: 'vibrating',\n",
                            " 844: 'viewers',\n",
                            " 845: 'waiting',\n",
                            " 846: 'waiting!',\n",
                            " 847: 'want',\n",
                            " 848: 'warm',\n",
                            " 849: 'warn',\n",
                            " 850: 'warning',\n",
                            " 851: 'was',\n",
                            " 852: 'was,',\n",
                            " 853: \"wasn't\",\n",
                            " 854: 'watched',\n",
                            " 855: 'waters,',\n",
                            " 856: 'we',\n",
                            " 857: 'well.',\n",
                            " 858: 'were',\n",
                            " 859: 'what',\n",
                            " 860: 'when',\n",
                            " 861: 'which',\n",
                            " 862: 'who',\n",
                            " 863: 'whole',\n",
                            " 864: 'whom',\n",
                            " 865: 'why.',\n",
                            " 866: 'will',\n",
                            " 867: 'wish',\n",
                            " 868: 'with',\n",
                            " 869: 'within',\n",
                            " 870: 'without',\n",
                            " 871: 'witness',\n",
                            " 872: 'witty',\n",
                            " 873: 'woman',\n",
                            " 874: 'women',\n",
                            " 875: 'wonder',\n",
                            " 876: 'work',\n",
                            " 877: 'working',\n",
                            " 878: 'world',\n",
                            " 879: 'world.\"',\n",
                            " 880: 'worst',\n",
                            " 881: 'would',\n",
                            " 882: 'write',\n",
                            " 883: 'written',\n",
                            " 884: 'wrote',\n",
                            " 885: 'year',\n",
                            " 886: 'years',\n",
                            " 887: 'you',\n",
                            " 888: 'young',\n",
                            " 889: 'younger',\n",
                            " 890: 'your',\n",
                            " 891: 'yours.\u003cbr'}"
                        ]
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "idx2word"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, perform the mapping to encode the observations in our subset. Note the use of ***nested list comprehensions***!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "([211, 851, 272, 233, 793, 587, 109, 303, 557, 517],\n",
                            " [131, 495, 308, 536, 819, 436, 289, 406, 449, 327])"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "X_proc = [[word2idx[word] for word in review] for review in X]\n",
                "X_proc[0][:10], X_proc[1][:10]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`X_proc` is a list of lists but if we are going to feed it into a `keras` model we should convert both it and `y` into `numpy` arrays."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(array([[211, 851, 272, ...,  69,  69,  69],\n",
                            "        [131, 495, 308, ...,  69,  69,  69],\n",
                            "        [160, 649, 799, ...,  69,  69,  69],\n",
                            "        ...,\n",
                            "        [206, 445, 525, ...,  69,  69,  69],\n",
                            "        [131, 687, 552, ...,  69,  69,  69],\n",
                            "        [201, 810, 622, ...,  69,  69,  69]]),\n",
                            " array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0]))"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "X_proc = np.hstack(X_proc).reshape(-1, MAX_LEN)\n",
                "y = np.array(y)\n",
                "X_proc, y"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, just to prove that we've successfully processed the data, we perform a test train split and feed it into a FNN."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X_proc, y, test_size=0.2, stratify=y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 8)                 4008      \n                                                                 \n dense_1 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 4017 (15.69 KB)\nTrainable params: 4017 (15.69 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/5\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-04-03 21:08:15.924603: I external/local_xla/xla/service/service.cc:168] XLA service 0x151418020150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2025-04-03 21:08:15.924637: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA L4, Compute Capability 8.9\n2025-04-03 21:08:15.932274: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2025-04-03 21:08:15.952077: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1743728896.012992   15316 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "4/4 - 2s - loss: 93.9375 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - 2s/epoch - 401ms/step\nEpoch 2/5\n4/4 - 0s - loss: 77.1722 - accuracy: 0.6250 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - 24ms/epoch - 6ms/step\nEpoch 3/5\n4/4 - 0s - loss: 81.9375 - accuracy: 0.7500 - val_loss: 47.3750 - val_accuracy: 0.5000 - 24ms/epoch - 6ms/step\nEpoch 4/5\n4/4 - 0s - loss: 100.4609 - accuracy: 0.6250 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - 24ms/epoch - 6ms/step\nEpoch 5/5\n4/4 - 0s - loss: 81.0782 - accuracy: 0.6250 - val_loss: 47.9062 - val_accuracy: 0.5000 - 24ms/epoch - 6ms/step\nAccuracy: 50.00%\n"
                }
            ],
            "source": [
                "model = Sequential()\n",
                "\n",
                "model.add(Dense(8, activation='relu',input_dim=MAX_LEN))\n",
                "model.add(Dense(1, activation='sigmoid'))\n",
                "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "print(model.summary())\n",
                "\n",
                "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=2, verbose=2)\n",
                "\n",
                "scores = model.evaluate(X_test, y_test, verbose=0)\n",
                "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It worked! But our subset was very small so we shouldn't get excited about the results above.\u003cbr\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Preprocessed IMDB Data**\n",
                "\n",
                "The IMDB dataset is very popular so `keras` also includes an alternative method for loading the data. This method can save us a lot of time for many reasons:\n",
                "- Cleaned text with less meaningless punctuation\n",
                "- Pre-tokenized and numerically encoded\n",
                "- Allows us to specify maximum vocabulary size"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.datasets import imdb\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small\n",
                "MAX_VOCAB = 10000\n",
                "INDEX_FROM = 3   # word index offset \n",
                "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=MAX_VOCAB,\n",
                "                                                                     index_from=INDEX_FROM)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`get_word_index` will load a json object we can store in a dictionary. This gives us the word-to-integer mapping."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n1641221/1641221 [==============================] - 0s 0us/step\n"
                },
                {
                    "data": {
                        "text/plain": [
                            "{'fawn': 34704,\n",
                            " 'tsukino': 52009,\n",
                            " 'nunnery': 52010,\n",
                            " 'sonja': 16819,\n",
                            " 'vani': 63954,\n",
                            " 'woods': 1411,\n",
                            " 'spiders': 16118,\n",
                            " 'hanging': 2348,\n",
                            " 'woody': 2292,\n",
                            " 'trawling': 52011,\n",
                            " \"hold's\": 52012,\n",
                            " 'comically': 11310,\n",
                            " 'localized': 40833,\n",
                            " 'disobeying': 30571,\n",
                            " \"'royale\": 52013,\n",
                            " \"harpo's\": 40834,\n",
                            " 'canet': 52014,\n",
                            " 'aileen': 19316,\n",
                            " 'acurately': 52015,\n",
                            " \"diplomat's\": 52016,\n",
                            " 'rickman': 25245,\n",
                            " 'arranged': 6749,\n",
                            " 'rumbustious': 52017,\n",
                            " 'familiarness': 52018,\n",
                            " \"spider'\": 52019,\n",
                            " 'hahahah': 68807,\n",
                            " \"wood'\": 52020,\n",
                            " 'transvestism': 40836,\n",
                            " \"hangin'\": 34705,\n",
                            " 'bringing': 2341,\n",
                            " 'seamier': 40837,\n",
                            " 'wooded': 34706,\n",
                            " 'bravora': 52021,\n",
                            " 'grueling': 16820,\n",
                            " 'wooden': 1639,\n",
                            " 'wednesday': 16821,\n",
                            " \"'prix\": 52022,\n",
                            " 'altagracia': 34707,\n",
                            " 'circuitry': 52023,\n",
                            " 'crotch': 11588,\n",
                            " 'busybody': 57769,\n",
                            " \"tart'n'tangy\": 52024,\n",
                            " 'burgade': 14132,\n",
                            " 'thrace': 52026,\n",
                            " \"tom's\": 11041,\n",
                            " 'snuggles': 52028,\n",
                            " 'francesco': 29117,\n",
                            " 'complainers': 52030,\n",
                            " 'templarios': 52128,\n",
                            " '272': 40838,\n",
                            " '273': 52031,\n",
                            " 'zaniacs': 52133,\n",
                            " '275': 34709,\n",
                            " 'consenting': 27634,\n",
                            " 'snuggled': 40839,\n",
                            " 'inanimate': 15495,\n",
                            " 'uality': 52033,\n",
                            " 'bronte': 11929,\n",
                            " 'errors': 4013,\n",
                            " 'dialogs': 3233,\n",
                            " \"yomada's\": 52034,\n",
                            " \"madman's\": 34710,\n",
                            " 'dialoge': 30588,\n",
                            " 'usenet': 52036,\n",
                            " 'videodrome': 40840,\n",
                            " \"kid'\": 26341,\n",
                            " 'pawed': 52037,\n",
                            " \"'girlfriend'\": 30572,\n",
                            " \"'pleasure\": 52038,\n",
                            " \"'reloaded'\": 52039,\n",
                            " \"kazakos'\": 40842,\n",
                            " 'rocque': 52040,\n",
                            " 'mailings': 52041,\n",
                            " 'brainwashed': 11930,\n",
                            " 'mcanally': 16822,\n",
                            " \"tom''\": 52042,\n",
                            " 'kurupt': 25246,\n",
                            " 'affiliated': 21908,\n",
                            " 'babaganoosh': 52043,\n",
                            " \"noe's\": 40843,\n",
                            " 'quart': 40844,\n",
                            " 'kids': 362,\n",
                            " 'uplifting': 5037,\n",
                            " 'controversy': 7096,\n",
                            " 'kida': 21909,\n",
                            " 'kidd': 23382,\n",
                            " \"error'\": 52044,\n",
                            " 'neurologist': 52045,\n",
                            " 'spotty': 18513,\n",
                            " 'cobblers': 30573,\n",
                            " 'projection': 9881,\n",
                            " 'fastforwarding': 40845,\n",
                            " 'sters': 52046,\n",
                            " \"eggar's\": 52047,\n",
                            " 'etherything': 52048,\n",
                            " 'gateshead': 40846,\n",
                            " 'airball': 34711,\n",
                            " 'unsinkable': 25247,\n",
                            " 'stern': 7183,\n",
                            " \"cervi's\": 52049,\n",
                            " 'dnd': 40847,\n",
                            " 'dna': 11589,\n",
                            " 'insecurity': 20601,\n",
                            " \"'reboot'\": 52050,\n",
                            " 'trelkovsky': 11040,\n",
                            " 'jaekel': 52051,\n",
                            " 'sidebars': 52052,\n",
                            " \"sforza's\": 52053,\n",
                            " 'distortions': 17636,\n",
                            " 'mutinies': 52054,\n",
                            " 'sermons': 30605,\n",
                            " '7ft': 40849,\n",
                            " 'boobage': 52055,\n",
                            " \"o'bannon's\": 52056,\n",
                            " 'populations': 23383,\n",
                            " 'chulak': 52057,\n",
                            " 'mesmerize': 27636,\n",
                            " 'quinnell': 52058,\n",
                            " 'yahoo': 10310,\n",
                            " 'meteorologist': 52060,\n",
                            " 'beswick': 42580,\n",
                            " 'boorman': 15496,\n",
                            " 'voicework': 40850,\n",
                            " \"ster'\": 52061,\n",
                            " 'blustering': 22925,\n",
                            " 'hj': 52062,\n",
                            " 'intake': 27637,\n",
                            " 'morally': 5624,\n",
                            " 'jumbling': 40852,\n",
                            " 'bowersock': 52063,\n",
                            " \"'porky's'\": 52064,\n",
                            " 'gershon': 16824,\n",
                            " 'ludicrosity': 40853,\n",
                            " 'coprophilia': 52065,\n",
                            " 'expressively': 40854,\n",
                            " \"india's\": 19503,\n",
                            " \"post's\": 34713,\n",
                            " 'wana': 52066,\n",
                            " 'wang': 5286,\n",
                            " 'wand': 30574,\n",
                            " 'wane': 25248,\n",
                            " 'edgeways': 52324,\n",
                            " 'titanium': 34714,\n",
                            " 'pinta': 40855,\n",
                            " 'want': 181,\n",
                            " 'pinto': 30575,\n",
                            " 'whoopdedoodles': 52068,\n",
                            " 'tchaikovsky': 21911,\n",
                            " 'travel': 2106,\n",
                            " \"'victory'\": 52069,\n",
                            " 'copious': 11931,\n",
                            " 'gouge': 22436,\n",
                            " \"chapters'\": 52070,\n",
                            " 'barbra': 6705,\n",
                            " 'uselessness': 30576,\n",
                            " \"wan'\": 52071,\n",
                            " 'assimilated': 27638,\n",
                            " 'petiot': 16119,\n",
                            " 'most\\x85and': 52072,\n",
                            " 'dinosaurs': 3933,\n",
                            " 'wrong': 355,\n",
                            " 'seda': 52073,\n",
                            " 'stollen': 52074,\n",
                            " 'sentencing': 34715,\n",
                            " 'ouroboros': 40856,\n",
                            " 'assimilates': 40857,\n",
                            " 'colorfully': 40858,\n",
                            " 'glenne': 27639,\n",
                            " 'dongen': 52075,\n",
                            " 'subplots': 4763,\n",
                            " 'kiloton': 52076,\n",
                            " 'chandon': 23384,\n",
                            " \"effect'\": 34716,\n",
                            " 'snugly': 27640,\n",
                            " 'kuei': 40859,\n",
                            " 'welcomed': 9095,\n",
                            " 'dishonor': 30074,\n",
                            " 'concurrence': 52078,\n",
                            " 'stoicism': 23385,\n",
                            " \"guys'\": 14899,\n",
                            " \"beroemd'\": 52080,\n",
                            " 'butcher': 6706,\n",
                            " \"melfi's\": 40860,\n",
                            " 'aargh': 30626,\n",
                            " 'playhouse': 20602,\n",
                            " 'wickedly': 11311,\n",
                            " 'fit': 1183,\n",
                            " 'labratory': 52081,\n",
                            " 'lifeline': 40862,\n",
                            " 'screaming': 1930,\n",
                            " 'fix': 4290,\n",
                            " 'cineliterate': 52082,\n",
                            " 'fic': 52083,\n",
                            " 'fia': 52084,\n",
                            " 'fig': 34717,\n",
                            " 'fmvs': 52085,\n",
                            " 'fie': 52086,\n",
                            " 'reentered': 52087,\n",
                            " 'fin': 30577,\n",
                            " 'doctresses': 52088,\n",
                            " 'fil': 52089,\n",
                            " 'zucker': 12609,\n",
                            " 'ached': 31934,\n",
                            " 'counsil': 52091,\n",
                            " 'paterfamilias': 52092,\n",
                            " 'songwriter': 13888,\n",
                            " 'shivam': 34718,\n",
                            " 'hurting': 9657,\n",
                            " 'effects': 302,\n",
                            " 'slauther': 52093,\n",
                            " \"'flame'\": 52094,\n",
                            " 'sommerset': 52095,\n",
                            " 'interwhined': 52096,\n",
                            " 'whacking': 27641,\n",
                            " 'bartok': 52097,\n",
                            " 'barton': 8778,\n",
                            " 'frewer': 21912,\n",
                            " \"fi'\": 52098,\n",
                            " 'ingrid': 6195,\n",
                            " 'stribor': 30578,\n",
                            " 'approporiately': 52099,\n",
                            " 'wobblyhand': 52100,\n",
                            " 'tantalisingly': 52101,\n",
                            " 'ankylosaurus': 52102,\n",
                            " 'parasites': 17637,\n",
                            " 'childen': 52103,\n",
                            " \"jenkins'\": 52104,\n",
                            " 'metafiction': 52105,\n",
                            " 'golem': 17638,\n",
                            " 'indiscretion': 40863,\n",
                            " \"reeves'\": 23386,\n",
                            " \"inamorata's\": 57784,\n",
                            " 'brittannica': 52107,\n",
                            " 'adapt': 7919,\n",
                            " \"russo's\": 30579,\n",
                            " 'guitarists': 48249,\n",
                            " 'abbott': 10556,\n",
                            " 'abbots': 40864,\n",
                            " 'lanisha': 17652,\n",
                            " 'magickal': 40866,\n",
                            " 'mattter': 52108,\n",
                            " \"'willy\": 52109,\n",
                            " 'pumpkins': 34719,\n",
                            " 'stuntpeople': 52110,\n",
                            " 'estimate': 30580,\n",
                            " 'ugghhh': 40867,\n",
                            " 'gameplay': 11312,\n",
                            " \"wern't\": 52111,\n",
                            " \"n'sync\": 40868,\n",
                            " 'sickeningly': 16120,\n",
                            " 'chiara': 40869,\n",
                            " 'disturbed': 4014,\n",
                            " 'portmanteau': 40870,\n",
                            " 'ineffectively': 52112,\n",
                            " \"duchonvey's\": 82146,\n",
                            " \"nasty'\": 37522,\n",
                            " 'purpose': 1288,\n",
                            " 'lazers': 52115,\n",
                            " 'lightened': 28108,\n",
                            " 'kaliganj': 52116,\n",
                            " 'popularism': 52117,\n",
                            " \"damme's\": 18514,\n",
                            " 'stylistics': 30581,\n",
                            " 'mindgaming': 52118,\n",
                            " 'spoilerish': 46452,\n",
                            " \"'corny'\": 52120,\n",
                            " 'boerner': 34721,\n",
                            " 'olds': 6795,\n",
                            " 'bakelite': 52121,\n",
                            " 'renovated': 27642,\n",
                            " 'forrester': 27643,\n",
                            " \"lumiere's\": 52122,\n",
                            " 'gaskets': 52027,\n",
                            " 'needed': 887,\n",
                            " 'smight': 34722,\n",
                            " 'master': 1300,\n",
                            " \"edie's\": 25908,\n",
                            " 'seeber': 40871,\n",
                            " 'hiya': 52123,\n",
                            " 'fuzziness': 52124,\n",
                            " 'genesis': 14900,\n",
                            " 'rewards': 12610,\n",
                            " 'enthrall': 30582,\n",
                            " \"'about\": 40872,\n",
                            " \"recollection's\": 52125,\n",
                            " 'mutilated': 11042,\n",
                            " 'fatherlands': 52126,\n",
                            " \"fischer's\": 52127,\n",
                            " 'positively': 5402,\n",
                            " '270': 34708,\n",
                            " 'ahmed': 34723,\n",
                            " 'zatoichi': 9839,\n",
                            " 'bannister': 13889,\n",
                            " 'anniversaries': 52130,\n",
                            " \"helm's\": 30583,\n",
                            " \"'work'\": 52131,\n",
                            " 'exclaimed': 34724,\n",
                            " \"'unfunny'\": 52132,\n",
                            " '274': 52032,\n",
                            " 'feeling': 547,\n",
                            " \"wanda's\": 52134,\n",
                            " 'dolan': 33269,\n",
                            " '278': 52136,\n",
                            " 'peacoat': 52137,\n",
                            " 'brawny': 40873,\n",
                            " 'mishra': 40874,\n",
                            " 'worlders': 40875,\n",
                            " 'protags': 52138,\n",
                            " 'skullcap': 52139,\n",
                            " 'dastagir': 57599,\n",
                            " 'affairs': 5625,\n",
                            " 'wholesome': 7802,\n",
                            " 'hymen': 52140,\n",
                            " 'paramedics': 25249,\n",
                            " 'unpersons': 52141,\n",
                            " 'heavyarms': 52142,\n",
                            " 'affaire': 52143,\n",
                            " 'coulisses': 52144,\n",
                            " 'hymer': 40876,\n",
                            " 'kremlin': 52145,\n",
                            " 'shipments': 30584,\n",
                            " 'pixilated': 52146,\n",
                            " \"'00s\": 30585,\n",
                            " 'diminishing': 18515,\n",
                            " 'cinematic': 1360,\n",
                            " 'resonates': 14901,\n",
                            " 'simplify': 40877,\n",
                            " \"nature'\": 40878,\n",
                            " 'temptresses': 40879,\n",
                            " 'reverence': 16825,\n",
                            " 'resonated': 19505,\n",
                            " 'dailey': 34725,\n",
                            " '2\\x85': 52147,\n",
                            " 'treize': 27644,\n",
                            " 'majo': 52148,\n",
                            " 'kiya': 21913,\n",
                            " 'woolnough': 52149,\n",
                            " 'thanatos': 39800,\n",
                            " 'sandoval': 35734,\n",
                            " 'dorama': 40882,\n",
                            " \"o'shaughnessy\": 52150,\n",
                            " 'tech': 4991,\n",
                            " 'fugitives': 32021,\n",
                            " 'teck': 30586,\n",
                            " \"'e'\": 76128,\n",
                            " 'doesn‚Äôt': 40884,\n",
                            " 'purged': 52152,\n",
                            " 'saying': 660,\n",
                            " \"martians'\": 41098,\n",
                            " 'norliss': 23421,\n",
                            " 'dickey': 27645,\n",
                            " 'dicker': 52155,\n",
                            " \"'sependipity\": 52156,\n",
                            " 'padded': 8425,\n",
                            " 'ordell': 57795,\n",
                            " \"sturges'\": 40885,\n",
                            " 'independentcritics': 52157,\n",
                            " 'tempted': 5748,\n",
                            " \"atkinson's\": 34727,\n",
                            " 'hounded': 25250,\n",
                            " 'apace': 52158,\n",
                            " 'clicked': 15497,\n",
                            " \"'humor'\": 30587,\n",
                            " \"martino's\": 17180,\n",
                            " \"'supporting\": 52159,\n",
                            " 'warmongering': 52035,\n",
                            " \"zemeckis's\": 34728,\n",
                            " 'lube': 21914,\n",
                            " 'shocky': 52160,\n",
                            " 'plate': 7479,\n",
                            " 'plata': 40886,\n",
                            " 'sturgess': 40887,\n",
                            " \"nerds'\": 40888,\n",
                            " 'plato': 20603,\n",
                            " 'plath': 34729,\n",
                            " 'platt': 40889,\n",
                            " 'mcnab': 52162,\n",
                            " 'clumsiness': 27646,\n",
                            " 'altogether': 3902,\n",
                            " 'massacring': 42587,\n",
                            " 'bicenntinial': 52163,\n",
                            " 'skaal': 40890,\n",
                            " 'droning': 14363,\n",
                            " 'lds': 8779,\n",
                            " 'jaguar': 21915,\n",
                            " \"cale's\": 34730,\n",
                            " 'nicely': 1780,\n",
                            " 'mummy': 4591,\n",
                            " \"lot's\": 18516,\n",
                            " 'patch': 10089,\n",
                            " 'kerkhof': 50205,\n",
                            " \"leader's\": 52164,\n",
                            " \"'movie\": 27647,\n",
                            " 'uncomfirmed': 52165,\n",
                            " 'heirloom': 40891,\n",
                            " 'wrangle': 47363,\n",
                            " 'emotion\\x85': 52166,\n",
                            " \"'stargate'\": 52167,\n",
                            " 'pinoy': 40892,\n",
                            " 'conchatta': 40893,\n",
                            " 'broeke': 41131,\n",
                            " 'advisedly': 40894,\n",
                            " \"barker's\": 17639,\n",
                            " 'descours': 52169,\n",
                            " 'lots': 775,\n",
                            " 'lotr': 9262,\n",
                            " 'irs': 9882,\n",
                            " 'lott': 52170,\n",
                            " 'xvi': 40895,\n",
                            " 'irk': 34731,\n",
                            " 'irl': 52171,\n",
                            " 'ira': 6890,\n",
                            " 'belzer': 21916,\n",
                            " 'irc': 52172,\n",
                            " 'ire': 27648,\n",
                            " 'requisites': 40896,\n",
                            " 'discipline': 7696,\n",
                            " 'lyoko': 52964,\n",
                            " 'extend': 11313,\n",
                            " 'nature': 876,\n",
                            " \"'dickie'\": 52173,\n",
                            " 'optimist': 40897,\n",
                            " 'lapping': 30589,\n",
                            " 'superficial': 3903,\n",
                            " 'vestment': 52174,\n",
                            " 'extent': 2826,\n",
                            " 'tendons': 52175,\n",
                            " \"heller's\": 52176,\n",
                            " 'quagmires': 52177,\n",
                            " 'miyako': 52178,\n",
                            " 'moocow': 20604,\n",
                            " \"coles'\": 52179,\n",
                            " 'lookit': 40898,\n",
                            " 'ravenously': 52180,\n",
                            " 'levitating': 40899,\n",
                            " 'perfunctorily': 52181,\n",
                            " 'lookin': 30590,\n",
                            " \"lot'\": 40901,\n",
                            " 'lookie': 52182,\n",
                            " 'fearlessly': 34873,\n",
                            " 'libyan': 52184,\n",
                            " 'fondles': 40902,\n",
                            " 'gopher': 35717,\n",
                            " 'wearying': 40904,\n",
                            " \"nz's\": 52185,\n",
                            " 'minuses': 27649,\n",
                            " 'puposelessly': 52186,\n",
                            " 'shandling': 52187,\n",
                            " 'decapitates': 31271,\n",
                            " 'humming': 11932,\n",
                            " \"'nother\": 40905,\n",
                            " 'smackdown': 21917,\n",
                            " 'underdone': 30591,\n",
                            " 'frf': 40906,\n",
                            " 'triviality': 52188,\n",
                            " 'fro': 25251,\n",
                            " 'bothers': 8780,\n",
                            " \"'kensington\": 52189,\n",
                            " 'much': 76,\n",
                            " 'muco': 34733,\n",
                            " 'wiseguy': 22618,\n",
                            " \"richie's\": 27651,\n",
                            " 'tonino': 40907,\n",
                            " 'unleavened': 52190,\n",
                            " 'fry': 11590,\n",
                            " \"'tv'\": 40908,\n",
                            " 'toning': 40909,\n",
                            " 'obese': 14364,\n",
                            " 'sensationalized': 30592,\n",
                            " 'spiv': 40910,\n",
                            " 'spit': 6262,\n",
                            " 'arkin': 7367,\n",
                            " 'charleton': 21918,\n",
                            " 'jeon': 16826,\n",
                            " 'boardroom': 21919,\n",
                            " 'doubts': 4992,\n",
                            " 'spin': 3087,\n",
                            " 'hepo': 53086,\n",
                            " 'wildcat': 27652,\n",
                            " 'venoms': 10587,\n",
                            " 'misconstrues': 52194,\n",
                            " 'mesmerising': 18517,\n",
                            " 'misconstrued': 40911,\n",
                            " 'rescinds': 52195,\n",
                            " 'prostrate': 52196,\n",
                            " 'majid': 40912,\n",
                            " 'climbed': 16482,\n",
                            " 'canoeing': 34734,\n",
                            " 'majin': 52198,\n",
                            " 'animie': 57807,\n",
                            " 'sylke': 40913,\n",
                            " 'conditioned': 14902,\n",
                            " 'waddell': 40914,\n",
                            " '3\\x85': 52199,\n",
                            " 'hyperdrive': 41191,\n",
                            " 'conditioner': 34735,\n",
                            " 'bricklayer': 53156,\n",
                            " 'hong': 2579,\n",
                            " 'memoriam': 52201,\n",
                            " 'inventively': 30595,\n",
                            " \"levant's\": 25252,\n",
                            " 'portobello': 20641,\n",
                            " 'remand': 52203,\n",
                            " 'mummified': 19507,\n",
                            " 'honk': 27653,\n",
                            " 'spews': 19508,\n",
                            " 'visitations': 40915,\n",
                            " 'mummifies': 52204,\n",
                            " 'cavanaugh': 25253,\n",
                            " 'zeon': 23388,\n",
                            " \"jungle's\": 40916,\n",
                            " 'viertel': 34736,\n",
                            " 'frenchmen': 27654,\n",
                            " 'torpedoes': 52205,\n",
                            " 'schlessinger': 52206,\n",
                            " 'torpedoed': 34737,\n",
                            " 'blister': 69879,\n",
                            " 'cinefest': 52207,\n",
                            " 'furlough': 34738,\n",
                            " 'mainsequence': 52208,\n",
                            " 'mentors': 40917,\n",
                            " 'academic': 9097,\n",
                            " 'stillness': 20605,\n",
                            " 'academia': 40918,\n",
                            " 'lonelier': 52209,\n",
                            " 'nibby': 52210,\n",
                            " \"losers'\": 52211,\n",
                            " 'cineastes': 40919,\n",
                            " 'corporate': 4452,\n",
                            " 'massaging': 40920,\n",
                            " 'bellow': 30596,\n",
                            " 'absurdities': 19509,\n",
                            " 'expetations': 53244,\n",
                            " 'nyfiken': 40921,\n",
                            " 'mehras': 75641,\n",
                            " 'lasse': 52212,\n",
                            " 'visability': 52213,\n",
                            " 'militarily': 33949,\n",
                            " \"elder'\": 52214,\n",
                            " 'gainsbourg': 19026,\n",
                            " 'hah': 20606,\n",
                            " 'hai': 13423,\n",
                            " 'haj': 34739,\n",
                            " 'hak': 25254,\n",
                            " 'hal': 4314,\n",
                            " 'ham': 4895,\n",
                            " 'duffer': 53262,\n",
                            " 'haa': 52216,\n",
                            " 'had': 69,\n",
                            " 'advancement': 11933,\n",
                            " 'hag': 16828,\n",
                            " \"hand'\": 25255,\n",
                            " 'hay': 13424,\n",
                            " 'mcnamara': 20607,\n",
                            " \"mozart's\": 52217,\n",
                            " 'duffel': 30734,\n",
                            " 'haq': 30597,\n",
                            " 'har': 13890,\n",
                            " 'has': 47,\n",
                            " 'hat': 2404,\n",
                            " 'hav': 40922,\n",
                            " 'haw': 30598,\n",
                            " 'figtings': 52218,\n",
                            " 'elders': 15498,\n",
                            " 'underpanted': 52219,\n",
                            " 'pninson': 52220,\n",
                            " 'unequivocally': 27655,\n",
                            " \"barbara's\": 23676,\n",
                            " \"bello'\": 52222,\n",
                            " 'indicative': 13000,\n",
                            " 'yawnfest': 40923,\n",
                            " 'hexploitation': 52223,\n",
                            " \"loder's\": 52224,\n",
                            " 'sleuthing': 27656,\n",
                            " \"justin's\": 32625,\n",
                            " \"'ball\": 52225,\n",
                            " \"'summer\": 52226,\n",
                            " \"'demons'\": 34938,\n",
                            " \"mormon's\": 52228,\n",
                            " \"laughton's\": 34740,\n",
                            " 'debell': 52229,\n",
                            " 'shipyard': 39727,\n",
                            " 'unabashedly': 30600,\n",
                            " 'disks': 40404,\n",
                            " 'crowd': 2293,\n",
                            " 'crowe': 10090,\n",
                            " \"vancouver's\": 56437,\n",
                            " 'mosques': 34741,\n",
                            " 'crown': 6630,\n",
                            " 'culpas': 52230,\n",
                            " 'crows': 27657,\n",
                            " 'surrell': 53347,\n",
                            " 'flowless': 52232,\n",
                            " 'sheirk': 52233,\n",
                            " \"'three\": 40926,\n",
                            " \"peterson'\": 52234,\n",
                            " 'ooverall': 52235,\n",
                            " 'perchance': 40927,\n",
                            " 'bottom': 1324,\n",
                            " 'chabert': 53366,\n",
                            " 'sneha': 52236,\n",
                            " 'inhuman': 13891,\n",
                            " 'ichii': 52237,\n",
                            " 'ursla': 52238,\n",
                            " 'completly': 30601,\n",
                            " 'moviedom': 40928,\n",
                            " 'raddick': 52239,\n",
                            " 'brundage': 51998,\n",
                            " 'brigades': 40929,\n",
                            " 'starring': 1184,\n",
                            " \"'goal'\": 52240,\n",
                            " 'caskets': 52241,\n",
                            " 'willcock': 52242,\n",
                            " \"threesome's\": 52243,\n",
                            " \"mosque'\": 52244,\n",
                            " \"cover's\": 52245,\n",
                            " 'spaceships': 17640,\n",
                            " 'anomalous': 40930,\n",
                            " 'ptsd': 27658,\n",
                            " 'shirdan': 52246,\n",
                            " 'obscenity': 21965,\n",
                            " 'lemmings': 30602,\n",
                            " 'duccio': 30603,\n",
                            " \"levene's\": 52247,\n",
                            " \"'gorby'\": 52248,\n",
                            " \"teenager's\": 25258,\n",
                            " 'marshall': 5343,\n",
                            " 'honeymoon': 9098,\n",
                            " 'shoots': 3234,\n",
                            " 'despised': 12261,\n",
                            " 'okabasho': 52249,\n",
                            " 'fabric': 8292,\n",
                            " 'cannavale': 18518,\n",
                            " 'raped': 3540,\n",
                            " \"tutt's\": 52250,\n",
                            " 'grasping': 17641,\n",
                            " 'despises': 18519,\n",
                            " \"thief's\": 40931,\n",
                            " 'rapes': 8929,\n",
                            " 'raper': 52251,\n",
                            " \"eyre'\": 27659,\n",
                            " 'walchek': 52252,\n",
                            " \"elmo's\": 23389,\n",
                            " 'perfumes': 40932,\n",
                            " 'spurting': 21921,\n",
                            " \"exposition'\\x85\": 52253,\n",
                            " 'denoting': 52254,\n",
                            " 'thesaurus': 34743,\n",
                            " \"shoot'\": 40933,\n",
                            " 'bonejack': 49762,\n",
                            " 'simpsonian': 52256,\n",
                            " 'hebetude': 30604,\n",
                            " \"hallow's\": 34744,\n",
                            " 'desperation\\x85': 52257,\n",
                            " 'incinerator': 34745,\n",
                            " 'congratulations': 10311,\n",
                            " 'humbled': 52258,\n",
                            " \"else's\": 5927,\n",
                            " 'trelkovski': 40848,\n",
                            " \"rape'\": 52259,\n",
                            " \"'chapters'\": 59389,\n",
                            " '1600s': 52260,\n",
                            " 'martian': 7256,\n",
                            " 'nicest': 25259,\n",
                            " 'eyred': 52262,\n",
                            " 'passenger': 9460,\n",
                            " 'disgrace': 6044,\n",
                            " 'moderne': 52263,\n",
                            " 'barrymore': 5123,\n",
                            " 'yankovich': 52264,\n",
                            " 'moderns': 40934,\n",
                            " 'studliest': 52265,\n",
                            " 'bedsheet': 52266,\n",
                            " 'decapitation': 14903,\n",
                            " 'slurring': 52267,\n",
                            " \"'nunsploitation'\": 52268,\n",
                            " \"'character'\": 34746,\n",
                            " 'cambodia': 9883,\n",
                            " 'rebelious': 52269,\n",
                            " 'pasadena': 27660,\n",
                            " 'crowne': 40935,\n",
                            " \"'bedchamber\": 52270,\n",
                            " 'conjectural': 52271,\n",
                            " 'appologize': 52272,\n",
                            " 'halfassing': 52273,\n",
                            " 'paycheque': 57819,\n",
                            " 'palms': 20609,\n",
                            " \"'islands\": 52274,\n",
                            " 'hawked': 40936,\n",
                            " 'palme': 21922,\n",
                            " 'conservatively': 40937,\n",
                            " 'larp': 64010,\n",
                            " 'palma': 5561,\n",
                            " 'smelling': 21923,\n",
                            " 'aragorn': 13001,\n",
                            " 'hawker': 52275,\n",
                            " 'hawkes': 52276,\n",
                            " 'explosions': 3978,\n",
                            " 'loren': 8062,\n",
                            " \"pyle's\": 52277,\n",
                            " 'shootout': 6707,\n",
                            " \"mike's\": 18520,\n",
                            " \"driscoll's\": 52278,\n",
                            " 'cogsworth': 40938,\n",
                            " \"britian's\": 52279,\n",
                            " 'childs': 34747,\n",
                            " \"portrait's\": 52280,\n",
                            " 'chain': 3629,\n",
                            " 'whoever': 2500,\n",
                            " 'puttered': 52281,\n",
                            " 'childe': 52282,\n",
                            " 'maywether': 52283,\n",
                            " 'chair': 3039,\n",
                            " \"rance's\": 52284,\n",
                            " 'machu': 34748,\n",
                            " 'ballet': 4520,\n",
                            " 'grapples': 34749,\n",
                            " 'summerize': 76155,\n",
                            " 'freelance': 30606,\n",
                            " \"andrea's\": 52286,\n",
                            " '\\x91very': 52287,\n",
                            " 'coolidge': 45882,\n",
                            " 'mache': 18521,\n",
                            " 'balled': 52288,\n",
                            " 'grappled': 40940,\n",
                            " 'macha': 18522,\n",
                            " 'underlining': 21924,\n",
                            " 'macho': 5626,\n",
                            " 'oversight': 19510,\n",
                            " 'machi': 25260,\n",
                            " 'verbally': 11314,\n",
                            " 'tenacious': 21925,\n",
                            " 'windshields': 40941,\n",
                            " 'paychecks': 18560,\n",
                            " 'jerk': 3399,\n",
                            " \"good'\": 11934,\n",
                            " 'prancer': 34751,\n",
                            " 'prances': 21926,\n",
                            " 'olympus': 52289,\n",
                            " 'lark': 21927,\n",
                            " 'embark': 10788,\n",
                            " 'gloomy': 7368,\n",
                            " 'jehaan': 52290,\n",
                            " 'turaqui': 52291,\n",
                            " \"child'\": 20610,\n",
                            " 'locked': 2897,\n",
                            " 'pranced': 52292,\n",
                            " 'exact': 2591,\n",
                            " 'unattuned': 52293,\n",
                            " 'minute': 786,\n",
                            " 'skewed': 16121,\n",
                            " 'hodgins': 40943,\n",
                            " 'skewer': 34752,\n",
                            " 'think\\x85': 52294,\n",
                            " 'rosenstein': 38768,\n",
                            " 'helmit': 52295,\n",
                            " 'wrestlemanias': 34753,\n",
                            " 'hindered': 16829,\n",
                            " \"martha's\": 30607,\n",
                            " 'cheree': 52296,\n",
                            " \"pluckin'\": 52297,\n",
                            " 'ogles': 40944,\n",
                            " 'heavyweight': 11935,\n",
                            " 'aada': 82193,\n",
                            " 'chopping': 11315,\n",
                            " 'strongboy': 61537,\n",
                            " 'hegemonic': 41345,\n",
                            " 'adorns': 40945,\n",
                            " 'xxth': 41349,\n",
                            " 'nobuhiro': 34754,\n",
                            " 'capit√£es': 52301,\n",
                            " 'kavogianni': 52302,\n",
                            " 'antwerp': 13425,\n",
                            " 'celebrated': 6541,\n",
                            " 'roarke': 52303,\n",
                            " 'baggins': 40946,\n",
                            " 'cheeseburgers': 31273,\n",
                            " 'matras': 52304,\n",
                            " \"nineties'\": 52305,\n",
                            " \"'craig'\": 52306,\n",
                            " 'celebrates': 13002,\n",
                            " 'unintentionally': 3386,\n",
                            " 'drafted': 14365,\n",
                            " 'climby': 52307,\n",
                            " '303': 52308,\n",
                            " 'oldies': 18523,\n",
                            " 'climbs': 9099,\n",
                            " 'honour': 9658,\n",
                            " 'plucking': 34755,\n",
                            " '305': 30077,\n",
                            " 'address': 5517,\n",
                            " 'menjou': 40947,\n",
                            " \"'freak'\": 42595,\n",
                            " 'dwindling': 19511,\n",
                            " 'benson': 9461,\n",
                            " 'white‚Äôs': 52310,\n",
                            " 'shamelessness': 40948,\n",
                            " 'impacted': 21928,\n",
                            " 'upatz': 52311,\n",
                            " 'cusack': 3843,\n",
                            " \"flavia's\": 37570,\n",
                            " 'effette': 52312,\n",
                            " 'influx': 34756,\n",
                            " 'boooooooo': 52313,\n",
                            " 'dimitrova': 52314,\n",
                            " 'houseman': 13426,\n",
                            " 'bigas': 25262,\n",
                            " 'boylen': 52315,\n",
                            " 'phillipenes': 52316,\n",
                            " 'fakery': 40949,\n",
                            " \"grandpa's\": 27661,\n",
                            " 'darnell': 27662,\n",
                            " 'undergone': 19512,\n",
                            " 'handbags': 52318,\n",
                            " 'perished': 21929,\n",
                            " 'pooped': 37781,\n",
                            " 'vigour': 27663,\n",
                            " 'opposed': 3630,\n",
                            " 'etude': 52319,\n",
                            " \"caine's\": 11802,\n",
                            " 'doozers': 52320,\n",
                            " 'photojournals': 34757,\n",
                            " 'perishes': 52321,\n",
                            " 'constrains': 34758,\n",
                            " 'migenes': 40951,\n",
                            " 'consoled': 30608,\n",
                            " 'alastair': 16830,\n",
                            " 'wvs': 52322,\n",
                            " 'ooooooh': 52323,\n",
                            " 'approving': 34759,\n",
                            " 'consoles': 40952,\n",
                            " 'disparagement': 52067,\n",
                            " 'futureistic': 52325,\n",
                            " 'rebounding': 52326,\n",
                            " \"'date\": 52327,\n",
                            " 'gregoire': 52328,\n",
                            " 'rutherford': 21930,\n",
                            " 'americanised': 34760,\n",
                            " 'novikov': 82199,\n",
                            " 'following': 1045,\n",
                            " 'munroe': 34761,\n",
                            " \"morita'\": 52329,\n",
                            " 'christenssen': 52330,\n",
                            " 'oatmeal': 23109,\n",
                            " 'fossey': 25263,\n",
                            " 'livered': 40953,\n",
                            " 'listens': 13003,\n",
                            " \"'marci\": 76167,\n",
                            " \"otis's\": 52333,\n",
                            " 'thanking': 23390,\n",
                            " 'maude': 16022,\n",
                            " 'extensions': 34762,\n",
                            " 'ameteurish': 52335,\n",
                            " \"commender's\": 52336,\n",
                            " 'agricultural': 27664,\n",
                            " 'convincingly': 4521,\n",
                            " 'fueled': 17642,\n",
                            " 'mahattan': 54017,\n",
                            " \"paris's\": 40955,\n",
                            " 'vulkan': 52339,\n",
                            " 'stapes': 52340,\n",
                            " 'odysessy': 52341,\n",
                            " 'harmon': 12262,\n",
                            " 'surfing': 4255,\n",
                            " 'halloran': 23497,\n",
                            " 'unbelieveably': 49583,\n",
                            " \"'offed'\": 52342,\n",
                            " 'quadrant': 30610,\n",
                            " 'inhabiting': 19513,\n",
                            " 'nebbish': 34763,\n",
                            " 'forebears': 40956,\n",
                            " 'skirmish': 34764,\n",
                            " 'ocassionally': 52343,\n",
                            " \"'resist\": 52344,\n",
                            " 'impactful': 21931,\n",
                            " 'spicier': 52345,\n",
                            " 'touristy': 40957,\n",
                            " \"'football'\": 52346,\n",
                            " 'webpage': 40958,\n",
                            " 'exurbia': 52348,\n",
                            " 'jucier': 52349,\n",
                            " 'professors': 14904,\n",
                            " 'structuring': 34765,\n",
                            " 'jig': 30611,\n",
                            " 'overlord': 40959,\n",
                            " 'disconnect': 25264,\n",
                            " 'sniffle': 82204,\n",
                            " 'slimeball': 40960,\n",
                            " 'jia': 40961,\n",
                            " 'milked': 16831,\n",
                            " 'banjoes': 40962,\n",
                            " 'jim': 1240,\n",
                            " 'workforces': 52351,\n",
                            " 'jip': 52352,\n",
                            " 'rotweiller': 52353,\n",
                            " 'mundaneness': 34766,\n",
                            " \"'ninja'\": 52354,\n",
                            " \"dead'\": 11043,\n",
                            " \"cipriani's\": 40963,\n",
                            " 'modestly': 20611,\n",
                            " \"professor'\": 52355,\n",
                            " 'shacked': 40964,\n",
                            " 'bashful': 34767,\n",
                            " 'sorter': 23391,\n",
                            " 'overpowering': 16123,\n",
                            " 'workmanlike': 18524,\n",
                            " 'henpecked': 27665,\n",
                            " 'sorted': 18525,\n",
                            " \"j≈çb's\": 52357,\n",
                            " \"'always\": 52358,\n",
                            " \"'baptists\": 34768,\n",
                            " 'dreamcatchers': 52359,\n",
                            " \"'silence'\": 52360,\n",
                            " 'hickory': 21932,\n",
                            " 'fun\\x97yet': 52361,\n",
                            " 'breakumentary': 52362,\n",
                            " 'didn': 15499,\n",
                            " 'didi': 52363,\n",
                            " 'pealing': 52364,\n",
                            " 'dispite': 40965,\n",
                            " \"italy's\": 25265,\n",
                            " 'instability': 21933,\n",
                            " 'quarter': 6542,\n",
                            " 'quartet': 12611,\n",
                            " 'padm√©': 52365,\n",
                            " \"'bleedmedry\": 52366,\n",
                            " 'pahalniuk': 52367,\n",
                            " 'honduras': 52368,\n",
                            " 'bursting': 10789,\n",
                            " \"pablo's\": 41468,\n",
                            " 'irremediably': 52370,\n",
                            " 'presages': 40966,\n",
                            " 'bowlegged': 57835,\n",
                            " 'dalip': 65186,\n",
                            " 'entering': 6263,\n",
                            " 'newsradio': 76175,\n",
                            " 'presaged': 54153,\n",
                            " \"giallo's\": 27666,\n",
                            " 'bouyant': 40967,\n",
                            " 'amerterish': 52371,\n",
                            " 'rajni': 18526,\n",
                            " 'leeves': 30613,\n",
                            " 'macauley': 34770,\n",
                            " 'seriously': 615,\n",
                            " 'sugercoma': 52372,\n",
                            " 'grimstead': 52373,\n",
                            " \"'fairy'\": 52374,\n",
                            " 'zenda': 30614,\n",
                            " \"'twins'\": 52375,\n",
                            " 'realisation': 17643,\n",
                            " 'highsmith': 27667,\n",
                            " 'raunchy': 7820,\n",
                            " 'incentives': 40968,\n",
                            " 'flatson': 52377,\n",
                            " 'snooker': 35100,\n",
                            " 'crazies': 16832,\n",
                            " 'crazier': 14905,\n",
                            " 'grandma': 7097,\n",
                            " 'napunsaktha': 52378,\n",
                            " 'workmanship': 30615,\n",
                            " 'reisner': 52379,\n",
                            " \"sanford's\": 61309,\n",
                            " '\\x91do√±a': 52380,\n",
                            " 'modest': 6111,\n",
                            " \"everything's\": 19156,\n",
                            " 'hamer': 40969,\n",
                            " \"couldn't'\": 52382,\n",
                            " 'quibble': 13004,\n",
                            " 'socking': 52383,\n",
                            " 'tingler': 21934,\n",
                            " 'gutman': 52384,\n",
                            " 'lachlan': 40970,\n",
                            " 'tableaus': 52385,\n",
                            " 'headbanger': 52386,\n",
                            " 'spoken': 2850,\n",
                            " 'cerebrally': 34771,\n",
                            " \"'road\": 23493,\n",
                            " 'tableaux': 21935,\n",
                            " \"proust's\": 40971,\n",
                            " 'periodical': 40972,\n",
                            " \"shoveller's\": 52388,\n",
                            " 'tamara': 25266,\n",
                            " 'affords': 17644,\n",
                            " 'concert': 3252,\n",
                            " \"yara's\": 87958,\n",
                            " 'someome': 52389,\n",
                            " 'lingering': 8427,\n",
                            " \"abraham's\": 41514,\n",
                            " 'beesley': 34772,\n",
                            " 'cherbourg': 34773,\n",
                            " 'kagan': 28627,\n",
                            " 'snatch': 9100,\n",
                            " \"miyazaki's\": 9263,\n",
                            " 'absorbs': 25267,\n",
                            " \"koltai's\": 40973,\n",
                            " 'tingled': 64030,\n",
                            " 'crossroads': 19514,\n",
                            " 'rehab': 16124,\n",
                            " 'falworth': 52392,\n",
                            " 'sequals': 52393,\n",
                            " ...}"
                        ]
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "word2idx = keras.datasets.imdb.get_word_index(path='imdb_word_index.json')\n",
                "word2idx = {k:(v + INDEX_FROM) for k,v in word2idx.items()}\n",
                "word2idx[\"\u003cPAD\u003e\"] = 0\n",
                "word2idx[\"\u003cSTART\u003e\"] = 1\n",
                "word2idx[\"\u003cUNK\u003e\"] = 2\n",
                "word2idx[\"\u003cUNUSED\u003e\"] = 3\n",
                "word2idx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{34704: 'fawn',\n",
                            " 52009: 'tsukino',\n",
                            " 52010: 'nunnery',\n",
                            " 16819: 'sonja',\n",
                            " 63954: 'vani',\n",
                            " 1411: 'woods',\n",
                            " 16118: 'spiders',\n",
                            " 2348: 'hanging',\n",
                            " 2292: 'woody',\n",
                            " 52011: 'trawling',\n",
                            " 52012: \"hold's\",\n",
                            " 11310: 'comically',\n",
                            " 40833: 'localized',\n",
                            " 30571: 'disobeying',\n",
                            " 52013: \"'royale\",\n",
                            " 40834: \"harpo's\",\n",
                            " 52014: 'canet',\n",
                            " 19316: 'aileen',\n",
                            " 52015: 'acurately',\n",
                            " 52016: \"diplomat's\",\n",
                            " 25245: 'rickman',\n",
                            " 6749: 'arranged',\n",
                            " 52017: 'rumbustious',\n",
                            " 52018: 'familiarness',\n",
                            " 52019: \"spider'\",\n",
                            " 68807: 'hahahah',\n",
                            " 52020: \"wood'\",\n",
                            " 40836: 'transvestism',\n",
                            " 34705: \"hangin'\",\n",
                            " 2341: 'bringing',\n",
                            " 40837: 'seamier',\n",
                            " 34706: 'wooded',\n",
                            " 52021: 'bravora',\n",
                            " 16820: 'grueling',\n",
                            " 1639: 'wooden',\n",
                            " 16821: 'wednesday',\n",
                            " 52022: \"'prix\",\n",
                            " 34707: 'altagracia',\n",
                            " 52023: 'circuitry',\n",
                            " 11588: 'crotch',\n",
                            " 57769: 'busybody',\n",
                            " 52024: \"tart'n'tangy\",\n",
                            " 14132: 'burgade',\n",
                            " 52026: 'thrace',\n",
                            " 11041: \"tom's\",\n",
                            " 52028: 'snuggles',\n",
                            " 29117: 'francesco',\n",
                            " 52030: 'complainers',\n",
                            " 52128: 'templarios',\n",
                            " 40838: '272',\n",
                            " 52031: '273',\n",
                            " 52133: 'zaniacs',\n",
                            " 34709: '275',\n",
                            " 27634: 'consenting',\n",
                            " 40839: 'snuggled',\n",
                            " 15495: 'inanimate',\n",
                            " 52033: 'uality',\n",
                            " 11929: 'bronte',\n",
                            " 4013: 'errors',\n",
                            " 3233: 'dialogs',\n",
                            " 52034: \"yomada's\",\n",
                            " 34710: \"madman's\",\n",
                            " 30588: 'dialoge',\n",
                            " 52036: 'usenet',\n",
                            " 40840: 'videodrome',\n",
                            " 26341: \"kid'\",\n",
                            " 52037: 'pawed',\n",
                            " 30572: \"'girlfriend'\",\n",
                            " 52038: \"'pleasure\",\n",
                            " 52039: \"'reloaded'\",\n",
                            " 40842: \"kazakos'\",\n",
                            " 52040: 'rocque',\n",
                            " 52041: 'mailings',\n",
                            " 11930: 'brainwashed',\n",
                            " 16822: 'mcanally',\n",
                            " 52042: \"tom''\",\n",
                            " 25246: 'kurupt',\n",
                            " 21908: 'affiliated',\n",
                            " 52043: 'babaganoosh',\n",
                            " 40843: \"noe's\",\n",
                            " 40844: 'quart',\n",
                            " 362: 'kids',\n",
                            " 5037: 'uplifting',\n",
                            " 7096: 'controversy',\n",
                            " 21909: 'kida',\n",
                            " 23382: 'kidd',\n",
                            " 52044: \"error'\",\n",
                            " 52045: 'neurologist',\n",
                            " 18513: 'spotty',\n",
                            " 30573: 'cobblers',\n",
                            " 9881: 'projection',\n",
                            " 40845: 'fastforwarding',\n",
                            " 52046: 'sters',\n",
                            " 52047: \"eggar's\",\n",
                            " 52048: 'etherything',\n",
                            " 40846: 'gateshead',\n",
                            " 34711: 'airball',\n",
                            " 25247: 'unsinkable',\n",
                            " 7183: 'stern',\n",
                            " 52049: \"cervi's\",\n",
                            " 40847: 'dnd',\n",
                            " 11589: 'dna',\n",
                            " 20601: 'insecurity',\n",
                            " 52050: \"'reboot'\",\n",
                            " 11040: 'trelkovsky',\n",
                            " 52051: 'jaekel',\n",
                            " 52052: 'sidebars',\n",
                            " 52053: \"sforza's\",\n",
                            " 17636: 'distortions',\n",
                            " 52054: 'mutinies',\n",
                            " 30605: 'sermons',\n",
                            " 40849: '7ft',\n",
                            " 52055: 'boobage',\n",
                            " 52056: \"o'bannon's\",\n",
                            " 23383: 'populations',\n",
                            " 52057: 'chulak',\n",
                            " 27636: 'mesmerize',\n",
                            " 52058: 'quinnell',\n",
                            " 10310: 'yahoo',\n",
                            " 52060: 'meteorologist',\n",
                            " 42580: 'beswick',\n",
                            " 15496: 'boorman',\n",
                            " 40850: 'voicework',\n",
                            " 52061: \"ster'\",\n",
                            " 22925: 'blustering',\n",
                            " 52062: 'hj',\n",
                            " 27637: 'intake',\n",
                            " 5624: 'morally',\n",
                            " 40852: 'jumbling',\n",
                            " 52063: 'bowersock',\n",
                            " 52064: \"'porky's'\",\n",
                            " 16824: 'gershon',\n",
                            " 40853: 'ludicrosity',\n",
                            " 52065: 'coprophilia',\n",
                            " 40854: 'expressively',\n",
                            " 19503: \"india's\",\n",
                            " 34713: \"post's\",\n",
                            " 52066: 'wana',\n",
                            " 5286: 'wang',\n",
                            " 30574: 'wand',\n",
                            " 25248: 'wane',\n",
                            " 52324: 'edgeways',\n",
                            " 34714: 'titanium',\n",
                            " 40855: 'pinta',\n",
                            " 181: 'want',\n",
                            " 30575: 'pinto',\n",
                            " 52068: 'whoopdedoodles',\n",
                            " 21911: 'tchaikovsky',\n",
                            " 2106: 'travel',\n",
                            " 52069: \"'victory'\",\n",
                            " 11931: 'copious',\n",
                            " 22436: 'gouge',\n",
                            " 52070: \"chapters'\",\n",
                            " 6705: 'barbra',\n",
                            " 30576: 'uselessness',\n",
                            " 52071: \"wan'\",\n",
                            " 27638: 'assimilated',\n",
                            " 16119: 'petiot',\n",
                            " 52072: 'most\\x85and',\n",
                            " 3933: 'dinosaurs',\n",
                            " 355: 'wrong',\n",
                            " 52073: 'seda',\n",
                            " 52074: 'stollen',\n",
                            " 34715: 'sentencing',\n",
                            " 40856: 'ouroboros',\n",
                            " 40857: 'assimilates',\n",
                            " 40858: 'colorfully',\n",
                            " 27639: 'glenne',\n",
                            " 52075: 'dongen',\n",
                            " 4763: 'subplots',\n",
                            " 52076: 'kiloton',\n",
                            " 23384: 'chandon',\n",
                            " 34716: \"effect'\",\n",
                            " 27640: 'snugly',\n",
                            " 40859: 'kuei',\n",
                            " 9095: 'welcomed',\n",
                            " 30074: 'dishonor',\n",
                            " 52078: 'concurrence',\n",
                            " 23385: 'stoicism',\n",
                            " 14899: \"guys'\",\n",
                            " 52080: \"beroemd'\",\n",
                            " 6706: 'butcher',\n",
                            " 40860: \"melfi's\",\n",
                            " 30626: 'aargh',\n",
                            " 20602: 'playhouse',\n",
                            " 11311: 'wickedly',\n",
                            " 1183: 'fit',\n",
                            " 52081: 'labratory',\n",
                            " 40862: 'lifeline',\n",
                            " 1930: 'screaming',\n",
                            " 4290: 'fix',\n",
                            " 52082: 'cineliterate',\n",
                            " 52083: 'fic',\n",
                            " 52084: 'fia',\n",
                            " 34717: 'fig',\n",
                            " 52085: 'fmvs',\n",
                            " 52086: 'fie',\n",
                            " 52087: 'reentered',\n",
                            " 30577: 'fin',\n",
                            " 52088: 'doctresses',\n",
                            " 52089: 'fil',\n",
                            " 12609: 'zucker',\n",
                            " 31934: 'ached',\n",
                            " 52091: 'counsil',\n",
                            " 52092: 'paterfamilias',\n",
                            " 13888: 'songwriter',\n",
                            " 34718: 'shivam',\n",
                            " 9657: 'hurting',\n",
                            " 302: 'effects',\n",
                            " 52093: 'slauther',\n",
                            " 52094: \"'flame'\",\n",
                            " 52095: 'sommerset',\n",
                            " 52096: 'interwhined',\n",
                            " 27641: 'whacking',\n",
                            " 52097: 'bartok',\n",
                            " 8778: 'barton',\n",
                            " 21912: 'frewer',\n",
                            " 52098: \"fi'\",\n",
                            " 6195: 'ingrid',\n",
                            " 30578: 'stribor',\n",
                            " 52099: 'approporiately',\n",
                            " 52100: 'wobblyhand',\n",
                            " 52101: 'tantalisingly',\n",
                            " 52102: 'ankylosaurus',\n",
                            " 17637: 'parasites',\n",
                            " 52103: 'childen',\n",
                            " 52104: \"jenkins'\",\n",
                            " 52105: 'metafiction',\n",
                            " 17638: 'golem',\n",
                            " 40863: 'indiscretion',\n",
                            " 23386: \"reeves'\",\n",
                            " 57784: \"inamorata's\",\n",
                            " 52107: 'brittannica',\n",
                            " 7919: 'adapt',\n",
                            " 30579: \"russo's\",\n",
                            " 48249: 'guitarists',\n",
                            " 10556: 'abbott',\n",
                            " 40864: 'abbots',\n",
                            " 17652: 'lanisha',\n",
                            " 40866: 'magickal',\n",
                            " 52108: 'mattter',\n",
                            " 52109: \"'willy\",\n",
                            " 34719: 'pumpkins',\n",
                            " 52110: 'stuntpeople',\n",
                            " 30580: 'estimate',\n",
                            " 40867: 'ugghhh',\n",
                            " 11312: 'gameplay',\n",
                            " 52111: \"wern't\",\n",
                            " 40868: \"n'sync\",\n",
                            " 16120: 'sickeningly',\n",
                            " 40869: 'chiara',\n",
                            " 4014: 'disturbed',\n",
                            " 40870: 'portmanteau',\n",
                            " 52112: 'ineffectively',\n",
                            " 82146: \"duchonvey's\",\n",
                            " 37522: \"nasty'\",\n",
                            " 1288: 'purpose',\n",
                            " 52115: 'lazers',\n",
                            " 28108: 'lightened',\n",
                            " 52116: 'kaliganj',\n",
                            " 52117: 'popularism',\n",
                            " 18514: \"damme's\",\n",
                            " 30581: 'stylistics',\n",
                            " 52118: 'mindgaming',\n",
                            " 46452: 'spoilerish',\n",
                            " 52120: \"'corny'\",\n",
                            " 34721: 'boerner',\n",
                            " 6795: 'olds',\n",
                            " 52121: 'bakelite',\n",
                            " 27642: 'renovated',\n",
                            " 27643: 'forrester',\n",
                            " 52122: \"lumiere's\",\n",
                            " 52027: 'gaskets',\n",
                            " 887: 'needed',\n",
                            " 34722: 'smight',\n",
                            " 1300: 'master',\n",
                            " 25908: \"edie's\",\n",
                            " 40871: 'seeber',\n",
                            " 52123: 'hiya',\n",
                            " 52124: 'fuzziness',\n",
                            " 14900: 'genesis',\n",
                            " 12610: 'rewards',\n",
                            " 30582: 'enthrall',\n",
                            " 40872: \"'about\",\n",
                            " 52125: \"recollection's\",\n",
                            " 11042: 'mutilated',\n",
                            " 52126: 'fatherlands',\n",
                            " 52127: \"fischer's\",\n",
                            " 5402: 'positively',\n",
                            " 34708: '270',\n",
                            " 34723: 'ahmed',\n",
                            " 9839: 'zatoichi',\n",
                            " 13889: 'bannister',\n",
                            " 52130: 'anniversaries',\n",
                            " 30583: \"helm's\",\n",
                            " 52131: \"'work'\",\n",
                            " 34724: 'exclaimed',\n",
                            " 52132: \"'unfunny'\",\n",
                            " 52032: '274',\n",
                            " 547: 'feeling',\n",
                            " 52134: \"wanda's\",\n",
                            " 33269: 'dolan',\n",
                            " 52136: '278',\n",
                            " 52137: 'peacoat',\n",
                            " 40873: 'brawny',\n",
                            " 40874: 'mishra',\n",
                            " 40875: 'worlders',\n",
                            " 52138: 'protags',\n",
                            " 52139: 'skullcap',\n",
                            " 57599: 'dastagir',\n",
                            " 5625: 'affairs',\n",
                            " 7802: 'wholesome',\n",
                            " 52140: 'hymen',\n",
                            " 25249: 'paramedics',\n",
                            " 52141: 'unpersons',\n",
                            " 52142: 'heavyarms',\n",
                            " 52143: 'affaire',\n",
                            " 52144: 'coulisses',\n",
                            " 40876: 'hymer',\n",
                            " 52145: 'kremlin',\n",
                            " 30584: 'shipments',\n",
                            " 52146: 'pixilated',\n",
                            " 30585: \"'00s\",\n",
                            " 18515: 'diminishing',\n",
                            " 1360: 'cinematic',\n",
                            " 14901: 'resonates',\n",
                            " 40877: 'simplify',\n",
                            " 40878: \"nature'\",\n",
                            " 40879: 'temptresses',\n",
                            " 16825: 'reverence',\n",
                            " 19505: 'resonated',\n",
                            " 34725: 'dailey',\n",
                            " 52147: '2\\x85',\n",
                            " 27644: 'treize',\n",
                            " 52148: 'majo',\n",
                            " 21913: 'kiya',\n",
                            " 52149: 'woolnough',\n",
                            " 39800: 'thanatos',\n",
                            " 35734: 'sandoval',\n",
                            " 40882: 'dorama',\n",
                            " 52150: \"o'shaughnessy\",\n",
                            " 4991: 'tech',\n",
                            " 32021: 'fugitives',\n",
                            " 30586: 'teck',\n",
                            " 76128: \"'e'\",\n",
                            " 40884: 'doesn‚Äôt',\n",
                            " 52152: 'purged',\n",
                            " 660: 'saying',\n",
                            " 41098: \"martians'\",\n",
                            " 23421: 'norliss',\n",
                            " 27645: 'dickey',\n",
                            " 52155: 'dicker',\n",
                            " 52156: \"'sependipity\",\n",
                            " 8425: 'padded',\n",
                            " 57795: 'ordell',\n",
                            " 40885: \"sturges'\",\n",
                            " 52157: 'independentcritics',\n",
                            " 5748: 'tempted',\n",
                            " 34727: \"atkinson's\",\n",
                            " 25250: 'hounded',\n",
                            " 52158: 'apace',\n",
                            " 15497: 'clicked',\n",
                            " 30587: \"'humor'\",\n",
                            " 17180: \"martino's\",\n",
                            " 52159: \"'supporting\",\n",
                            " 52035: 'warmongering',\n",
                            " 34728: \"zemeckis's\",\n",
                            " 21914: 'lube',\n",
                            " 52160: 'shocky',\n",
                            " 7479: 'plate',\n",
                            " 40886: 'plata',\n",
                            " 40887: 'sturgess',\n",
                            " 40888: \"nerds'\",\n",
                            " 20603: 'plato',\n",
                            " 34729: 'plath',\n",
                            " 40889: 'platt',\n",
                            " 52162: 'mcnab',\n",
                            " 27646: 'clumsiness',\n",
                            " 3902: 'altogether',\n",
                            " 42587: 'massacring',\n",
                            " 52163: 'bicenntinial',\n",
                            " 40890: 'skaal',\n",
                            " 14363: 'droning',\n",
                            " 8779: 'lds',\n",
                            " 21915: 'jaguar',\n",
                            " 34730: \"cale's\",\n",
                            " 1780: 'nicely',\n",
                            " 4591: 'mummy',\n",
                            " 18516: \"lot's\",\n",
                            " 10089: 'patch',\n",
                            " 50205: 'kerkhof',\n",
                            " 52164: \"leader's\",\n",
                            " 27647: \"'movie\",\n",
                            " 52165: 'uncomfirmed',\n",
                            " 40891: 'heirloom',\n",
                            " 47363: 'wrangle',\n",
                            " 52166: 'emotion\\x85',\n",
                            " 52167: \"'stargate'\",\n",
                            " 40892: 'pinoy',\n",
                            " 40893: 'conchatta',\n",
                            " 41131: 'broeke',\n",
                            " 40894: 'advisedly',\n",
                            " 17639: \"barker's\",\n",
                            " 52169: 'descours',\n",
                            " 775: 'lots',\n",
                            " 9262: 'lotr',\n",
                            " 9882: 'irs',\n",
                            " 52170: 'lott',\n",
                            " 40895: 'xvi',\n",
                            " 34731: 'irk',\n",
                            " 52171: 'irl',\n",
                            " 6890: 'ira',\n",
                            " 21916: 'belzer',\n",
                            " 52172: 'irc',\n",
                            " 27648: 'ire',\n",
                            " 40896: 'requisites',\n",
                            " 7696: 'discipline',\n",
                            " 52964: 'lyoko',\n",
                            " 11313: 'extend',\n",
                            " 876: 'nature',\n",
                            " 52173: \"'dickie'\",\n",
                            " 40897: 'optimist',\n",
                            " 30589: 'lapping',\n",
                            " 3903: 'superficial',\n",
                            " 52174: 'vestment',\n",
                            " 2826: 'extent',\n",
                            " 52175: 'tendons',\n",
                            " 52176: \"heller's\",\n",
                            " 52177: 'quagmires',\n",
                            " 52178: 'miyako',\n",
                            " 20604: 'moocow',\n",
                            " 52179: \"coles'\",\n",
                            " 40898: 'lookit',\n",
                            " 52180: 'ravenously',\n",
                            " 40899: 'levitating',\n",
                            " 52181: 'perfunctorily',\n",
                            " 30590: 'lookin',\n",
                            " 40901: \"lot'\",\n",
                            " 52182: 'lookie',\n",
                            " 34873: 'fearlessly',\n",
                            " 52184: 'libyan',\n",
                            " 40902: 'fondles',\n",
                            " 35717: 'gopher',\n",
                            " 40904: 'wearying',\n",
                            " 52185: \"nz's\",\n",
                            " 27649: 'minuses',\n",
                            " 52186: 'puposelessly',\n",
                            " 52187: 'shandling',\n",
                            " 31271: 'decapitates',\n",
                            " 11932: 'humming',\n",
                            " 40905: \"'nother\",\n",
                            " 21917: 'smackdown',\n",
                            " 30591: 'underdone',\n",
                            " 40906: 'frf',\n",
                            " 52188: 'triviality',\n",
                            " 25251: 'fro',\n",
                            " 8780: 'bothers',\n",
                            " 52189: \"'kensington\",\n",
                            " 76: 'much',\n",
                            " 34733: 'muco',\n",
                            " 22618: 'wiseguy',\n",
                            " 27651: \"richie's\",\n",
                            " 40907: 'tonino',\n",
                            " 52190: 'unleavened',\n",
                            " 11590: 'fry',\n",
                            " 40908: \"'tv'\",\n",
                            " 40909: 'toning',\n",
                            " 14364: 'obese',\n",
                            " 30592: 'sensationalized',\n",
                            " 40910: 'spiv',\n",
                            " 6262: 'spit',\n",
                            " 7367: 'arkin',\n",
                            " 21918: 'charleton',\n",
                            " 16826: 'jeon',\n",
                            " 21919: 'boardroom',\n",
                            " 4992: 'doubts',\n",
                            " 3087: 'spin',\n",
                            " 53086: 'hepo',\n",
                            " 27652: 'wildcat',\n",
                            " 10587: 'venoms',\n",
                            " 52194: 'misconstrues',\n",
                            " 18517: 'mesmerising',\n",
                            " 40911: 'misconstrued',\n",
                            " 52195: 'rescinds',\n",
                            " 52196: 'prostrate',\n",
                            " 40912: 'majid',\n",
                            " 16482: 'climbed',\n",
                            " 34734: 'canoeing',\n",
                            " 52198: 'majin',\n",
                            " 57807: 'animie',\n",
                            " 40913: 'sylke',\n",
                            " 14902: 'conditioned',\n",
                            " 40914: 'waddell',\n",
                            " 52199: '3\\x85',\n",
                            " 41191: 'hyperdrive',\n",
                            " 34735: 'conditioner',\n",
                            " 53156: 'bricklayer',\n",
                            " 2579: 'hong',\n",
                            " 52201: 'memoriam',\n",
                            " 30595: 'inventively',\n",
                            " 25252: \"levant's\",\n",
                            " 20641: 'portobello',\n",
                            " 52203: 'remand',\n",
                            " 19507: 'mummified',\n",
                            " 27653: 'honk',\n",
                            " 19508: 'spews',\n",
                            " 40915: 'visitations',\n",
                            " 52204: 'mummifies',\n",
                            " 25253: 'cavanaugh',\n",
                            " 23388: 'zeon',\n",
                            " 40916: \"jungle's\",\n",
                            " 34736: 'viertel',\n",
                            " 27654: 'frenchmen',\n",
                            " 52205: 'torpedoes',\n",
                            " 52206: 'schlessinger',\n",
                            " 34737: 'torpedoed',\n",
                            " 69879: 'blister',\n",
                            " 52207: 'cinefest',\n",
                            " 34738: 'furlough',\n",
                            " 52208: 'mainsequence',\n",
                            " 40917: 'mentors',\n",
                            " 9097: 'academic',\n",
                            " 20605: 'stillness',\n",
                            " 40918: 'academia',\n",
                            " 52209: 'lonelier',\n",
                            " 52210: 'nibby',\n",
                            " 52211: \"losers'\",\n",
                            " 40919: 'cineastes',\n",
                            " 4452: 'corporate',\n",
                            " 40920: 'massaging',\n",
                            " 30596: 'bellow',\n",
                            " 19509: 'absurdities',\n",
                            " 53244: 'expetations',\n",
                            " 40921: 'nyfiken',\n",
                            " 75641: 'mehras',\n",
                            " 52212: 'lasse',\n",
                            " 52213: 'visability',\n",
                            " 33949: 'militarily',\n",
                            " 52214: \"elder'\",\n",
                            " 19026: 'gainsbourg',\n",
                            " 20606: 'hah',\n",
                            " 13423: 'hai',\n",
                            " 34739: 'haj',\n",
                            " 25254: 'hak',\n",
                            " 4314: 'hal',\n",
                            " 4895: 'ham',\n",
                            " 53262: 'duffer',\n",
                            " 52216: 'haa',\n",
                            " 69: 'had',\n",
                            " 11933: 'advancement',\n",
                            " 16828: 'hag',\n",
                            " 25255: \"hand'\",\n",
                            " 13424: 'hay',\n",
                            " 20607: 'mcnamara',\n",
                            " 52217: \"mozart's\",\n",
                            " 30734: 'duffel',\n",
                            " 30597: 'haq',\n",
                            " 13890: 'har',\n",
                            " 47: 'has',\n",
                            " 2404: 'hat',\n",
                            " 40922: 'hav',\n",
                            " 30598: 'haw',\n",
                            " 52218: 'figtings',\n",
                            " 15498: 'elders',\n",
                            " 52219: 'underpanted',\n",
                            " 52220: 'pninson',\n",
                            " 27655: 'unequivocally',\n",
                            " 23676: \"barbara's\",\n",
                            " 52222: \"bello'\",\n",
                            " 13000: 'indicative',\n",
                            " 40923: 'yawnfest',\n",
                            " 52223: 'hexploitation',\n",
                            " 52224: \"loder's\",\n",
                            " 27656: 'sleuthing',\n",
                            " 32625: \"justin's\",\n",
                            " 52225: \"'ball\",\n",
                            " 52226: \"'summer\",\n",
                            " 34938: \"'demons'\",\n",
                            " 52228: \"mormon's\",\n",
                            " 34740: \"laughton's\",\n",
                            " 52229: 'debell',\n",
                            " 39727: 'shipyard',\n",
                            " 30600: 'unabashedly',\n",
                            " 40404: 'disks',\n",
                            " 2293: 'crowd',\n",
                            " 10090: 'crowe',\n",
                            " 56437: \"vancouver's\",\n",
                            " 34741: 'mosques',\n",
                            " 6630: 'crown',\n",
                            " 52230: 'culpas',\n",
                            " 27657: 'crows',\n",
                            " 53347: 'surrell',\n",
                            " 52232: 'flowless',\n",
                            " 52233: 'sheirk',\n",
                            " 40926: \"'three\",\n",
                            " 52234: \"peterson'\",\n",
                            " 52235: 'ooverall',\n",
                            " 40927: 'perchance',\n",
                            " 1324: 'bottom',\n",
                            " 53366: 'chabert',\n",
                            " 52236: 'sneha',\n",
                            " 13891: 'inhuman',\n",
                            " 52237: 'ichii',\n",
                            " 52238: 'ursla',\n",
                            " 30601: 'completly',\n",
                            " 40928: 'moviedom',\n",
                            " 52239: 'raddick',\n",
                            " 51998: 'brundage',\n",
                            " 40929: 'brigades',\n",
                            " 1184: 'starring',\n",
                            " 52240: \"'goal'\",\n",
                            " 52241: 'caskets',\n",
                            " 52242: 'willcock',\n",
                            " 52243: \"threesome's\",\n",
                            " 52244: \"mosque'\",\n",
                            " 52245: \"cover's\",\n",
                            " 17640: 'spaceships',\n",
                            " 40930: 'anomalous',\n",
                            " 27658: 'ptsd',\n",
                            " 52246: 'shirdan',\n",
                            " 21965: 'obscenity',\n",
                            " 30602: 'lemmings',\n",
                            " 30603: 'duccio',\n",
                            " 52247: \"levene's\",\n",
                            " 52248: \"'gorby'\",\n",
                            " 25258: \"teenager's\",\n",
                            " 5343: 'marshall',\n",
                            " 9098: 'honeymoon',\n",
                            " 3234: 'shoots',\n",
                            " 12261: 'despised',\n",
                            " 52249: 'okabasho',\n",
                            " 8292: 'fabric',\n",
                            " 18518: 'cannavale',\n",
                            " 3540: 'raped',\n",
                            " 52250: \"tutt's\",\n",
                            " 17641: 'grasping',\n",
                            " 18519: 'despises',\n",
                            " 40931: \"thief's\",\n",
                            " 8929: 'rapes',\n",
                            " 52251: 'raper',\n",
                            " 27659: \"eyre'\",\n",
                            " 52252: 'walchek',\n",
                            " 23389: \"elmo's\",\n",
                            " 40932: 'perfumes',\n",
                            " 21921: 'spurting',\n",
                            " 52253: \"exposition'\\x85\",\n",
                            " 52254: 'denoting',\n",
                            " 34743: 'thesaurus',\n",
                            " 40933: \"shoot'\",\n",
                            " 49762: 'bonejack',\n",
                            " 52256: 'simpsonian',\n",
                            " 30604: 'hebetude',\n",
                            " 34744: \"hallow's\",\n",
                            " 52257: 'desperation\\x85',\n",
                            " 34745: 'incinerator',\n",
                            " 10311: 'congratulations',\n",
                            " 52258: 'humbled',\n",
                            " 5927: \"else's\",\n",
                            " 40848: 'trelkovski',\n",
                            " 52259: \"rape'\",\n",
                            " 59389: \"'chapters'\",\n",
                            " 52260: '1600s',\n",
                            " 7256: 'martian',\n",
                            " 25259: 'nicest',\n",
                            " 52262: 'eyred',\n",
                            " 9460: 'passenger',\n",
                            " 6044: 'disgrace',\n",
                            " 52263: 'moderne',\n",
                            " 5123: 'barrymore',\n",
                            " 52264: 'yankovich',\n",
                            " 40934: 'moderns',\n",
                            " 52265: 'studliest',\n",
                            " 52266: 'bedsheet',\n",
                            " 14903: 'decapitation',\n",
                            " 52267: 'slurring',\n",
                            " 52268: \"'nunsploitation'\",\n",
                            " 34746: \"'character'\",\n",
                            " 9883: 'cambodia',\n",
                            " 52269: 'rebelious',\n",
                            " 27660: 'pasadena',\n",
                            " 40935: 'crowne',\n",
                            " 52270: \"'bedchamber\",\n",
                            " 52271: 'conjectural',\n",
                            " 52272: 'appologize',\n",
                            " 52273: 'halfassing',\n",
                            " 57819: 'paycheque',\n",
                            " 20609: 'palms',\n",
                            " 52274: \"'islands\",\n",
                            " 40936: 'hawked',\n",
                            " 21922: 'palme',\n",
                            " 40937: 'conservatively',\n",
                            " 64010: 'larp',\n",
                            " 5561: 'palma',\n",
                            " 21923: 'smelling',\n",
                            " 13001: 'aragorn',\n",
                            " 52275: 'hawker',\n",
                            " 52276: 'hawkes',\n",
                            " 3978: 'explosions',\n",
                            " 8062: 'loren',\n",
                            " 52277: \"pyle's\",\n",
                            " 6707: 'shootout',\n",
                            " 18520: \"mike's\",\n",
                            " 52278: \"driscoll's\",\n",
                            " 40938: 'cogsworth',\n",
                            " 52279: \"britian's\",\n",
                            " 34747: 'childs',\n",
                            " 52280: \"portrait's\",\n",
                            " 3629: 'chain',\n",
                            " 2500: 'whoever',\n",
                            " 52281: 'puttered',\n",
                            " 52282: 'childe',\n",
                            " 52283: 'maywether',\n",
                            " 3039: 'chair',\n",
                            " 52284: \"rance's\",\n",
                            " 34748: 'machu',\n",
                            " 4520: 'ballet',\n",
                            " 34749: 'grapples',\n",
                            " 76155: 'summerize',\n",
                            " 30606: 'freelance',\n",
                            " 52286: \"andrea's\",\n",
                            " 52287: '\\x91very',\n",
                            " 45882: 'coolidge',\n",
                            " 18521: 'mache',\n",
                            " 52288: 'balled',\n",
                            " 40940: 'grappled',\n",
                            " 18522: 'macha',\n",
                            " 21924: 'underlining',\n",
                            " 5626: 'macho',\n",
                            " 19510: 'oversight',\n",
                            " 25260: 'machi',\n",
                            " 11314: 'verbally',\n",
                            " 21925: 'tenacious',\n",
                            " 40941: 'windshields',\n",
                            " 18560: 'paychecks',\n",
                            " 3399: 'jerk',\n",
                            " 11934: \"good'\",\n",
                            " 34751: 'prancer',\n",
                            " 21926: 'prances',\n",
                            " 52289: 'olympus',\n",
                            " 21927: 'lark',\n",
                            " 10788: 'embark',\n",
                            " 7368: 'gloomy',\n",
                            " 52290: 'jehaan',\n",
                            " 52291: 'turaqui',\n",
                            " 20610: \"child'\",\n",
                            " 2897: 'locked',\n",
                            " 52292: 'pranced',\n",
                            " 2591: 'exact',\n",
                            " 52293: 'unattuned',\n",
                            " 786: 'minute',\n",
                            " 16121: 'skewed',\n",
                            " 40943: 'hodgins',\n",
                            " 34752: 'skewer',\n",
                            " 52294: 'think\\x85',\n",
                            " 38768: 'rosenstein',\n",
                            " 52295: 'helmit',\n",
                            " 34753: 'wrestlemanias',\n",
                            " 16829: 'hindered',\n",
                            " 30607: \"martha's\",\n",
                            " 52296: 'cheree',\n",
                            " 52297: \"pluckin'\",\n",
                            " 40944: 'ogles',\n",
                            " 11935: 'heavyweight',\n",
                            " 82193: 'aada',\n",
                            " 11315: 'chopping',\n",
                            " 61537: 'strongboy',\n",
                            " 41345: 'hegemonic',\n",
                            " 40945: 'adorns',\n",
                            " 41349: 'xxth',\n",
                            " 34754: 'nobuhiro',\n",
                            " 52301: 'capit√£es',\n",
                            " 52302: 'kavogianni',\n",
                            " 13425: 'antwerp',\n",
                            " 6541: 'celebrated',\n",
                            " 52303: 'roarke',\n",
                            " 40946: 'baggins',\n",
                            " 31273: 'cheeseburgers',\n",
                            " 52304: 'matras',\n",
                            " 52305: \"nineties'\",\n",
                            " 52306: \"'craig'\",\n",
                            " 13002: 'celebrates',\n",
                            " 3386: 'unintentionally',\n",
                            " 14365: 'drafted',\n",
                            " 52307: 'climby',\n",
                            " 52308: '303',\n",
                            " 18523: 'oldies',\n",
                            " 9099: 'climbs',\n",
                            " 9658: 'honour',\n",
                            " 34755: 'plucking',\n",
                            " 30077: '305',\n",
                            " 5517: 'address',\n",
                            " 40947: 'menjou',\n",
                            " 42595: \"'freak'\",\n",
                            " 19511: 'dwindling',\n",
                            " 9461: 'benson',\n",
                            " 52310: 'white‚Äôs',\n",
                            " 40948: 'shamelessness',\n",
                            " 21928: 'impacted',\n",
                            " 52311: 'upatz',\n",
                            " 3843: 'cusack',\n",
                            " 37570: \"flavia's\",\n",
                            " 52312: 'effette',\n",
                            " 34756: 'influx',\n",
                            " 52313: 'boooooooo',\n",
                            " 52314: 'dimitrova',\n",
                            " 13426: 'houseman',\n",
                            " 25262: 'bigas',\n",
                            " 52315: 'boylen',\n",
                            " 52316: 'phillipenes',\n",
                            " 40949: 'fakery',\n",
                            " 27661: \"grandpa's\",\n",
                            " 27662: 'darnell',\n",
                            " 19512: 'undergone',\n",
                            " 52318: 'handbags',\n",
                            " 21929: 'perished',\n",
                            " 37781: 'pooped',\n",
                            " 27663: 'vigour',\n",
                            " 3630: 'opposed',\n",
                            " 52319: 'etude',\n",
                            " 11802: \"caine's\",\n",
                            " 52320: 'doozers',\n",
                            " 34757: 'photojournals',\n",
                            " 52321: 'perishes',\n",
                            " 34758: 'constrains',\n",
                            " 40951: 'migenes',\n",
                            " 30608: 'consoled',\n",
                            " 16830: 'alastair',\n",
                            " 52322: 'wvs',\n",
                            " 52323: 'ooooooh',\n",
                            " 34759: 'approving',\n",
                            " 40952: 'consoles',\n",
                            " 52067: 'disparagement',\n",
                            " 52325: 'futureistic',\n",
                            " 52326: 'rebounding',\n",
                            " 52327: \"'date\",\n",
                            " 52328: 'gregoire',\n",
                            " 21930: 'rutherford',\n",
                            " 34760: 'americanised',\n",
                            " 82199: 'novikov',\n",
                            " 1045: 'following',\n",
                            " 34761: 'munroe',\n",
                            " 52329: \"morita'\",\n",
                            " 52330: 'christenssen',\n",
                            " 23109: 'oatmeal',\n",
                            " 25263: 'fossey',\n",
                            " 40953: 'livered',\n",
                            " 13003: 'listens',\n",
                            " 76167: \"'marci\",\n",
                            " 52333: \"otis's\",\n",
                            " 23390: 'thanking',\n",
                            " 16022: 'maude',\n",
                            " 34762: 'extensions',\n",
                            " 52335: 'ameteurish',\n",
                            " 52336: \"commender's\",\n",
                            " 27664: 'agricultural',\n",
                            " 4521: 'convincingly',\n",
                            " 17642: 'fueled',\n",
                            " 54017: 'mahattan',\n",
                            " 40955: \"paris's\",\n",
                            " 52339: 'vulkan',\n",
                            " 52340: 'stapes',\n",
                            " 52341: 'odysessy',\n",
                            " 12262: 'harmon',\n",
                            " 4255: 'surfing',\n",
                            " 23497: 'halloran',\n",
                            " 49583: 'unbelieveably',\n",
                            " 52342: \"'offed'\",\n",
                            " 30610: 'quadrant',\n",
                            " 19513: 'inhabiting',\n",
                            " 34763: 'nebbish',\n",
                            " 40956: 'forebears',\n",
                            " 34764: 'skirmish',\n",
                            " 52343: 'ocassionally',\n",
                            " 52344: \"'resist\",\n",
                            " 21931: 'impactful',\n",
                            " 52345: 'spicier',\n",
                            " 40957: 'touristy',\n",
                            " 52346: \"'football'\",\n",
                            " 40958: 'webpage',\n",
                            " 52348: 'exurbia',\n",
                            " 52349: 'jucier',\n",
                            " 14904: 'professors',\n",
                            " 34765: 'structuring',\n",
                            " 30611: 'jig',\n",
                            " 40959: 'overlord',\n",
                            " 25264: 'disconnect',\n",
                            " 82204: 'sniffle',\n",
                            " 40960: 'slimeball',\n",
                            " 40961: 'jia',\n",
                            " 16831: 'milked',\n",
                            " 40962: 'banjoes',\n",
                            " 1240: 'jim',\n",
                            " 52351: 'workforces',\n",
                            " 52352: 'jip',\n",
                            " 52353: 'rotweiller',\n",
                            " 34766: 'mundaneness',\n",
                            " 52354: \"'ninja'\",\n",
                            " 11043: \"dead'\",\n",
                            " 40963: \"cipriani's\",\n",
                            " 20611: 'modestly',\n",
                            " 52355: \"professor'\",\n",
                            " 40964: 'shacked',\n",
                            " 34767: 'bashful',\n",
                            " 23391: 'sorter',\n",
                            " 16123: 'overpowering',\n",
                            " 18524: 'workmanlike',\n",
                            " 27665: 'henpecked',\n",
                            " 18525: 'sorted',\n",
                            " 52357: \"j≈çb's\",\n",
                            " 52358: \"'always\",\n",
                            " 34768: \"'baptists\",\n",
                            " 52359: 'dreamcatchers',\n",
                            " 52360: \"'silence'\",\n",
                            " 21932: 'hickory',\n",
                            " 52361: 'fun\\x97yet',\n",
                            " 52362: 'breakumentary',\n",
                            " 15499: 'didn',\n",
                            " 52363: 'didi',\n",
                            " 52364: 'pealing',\n",
                            " 40965: 'dispite',\n",
                            " 25265: \"italy's\",\n",
                            " 21933: 'instability',\n",
                            " 6542: 'quarter',\n",
                            " 12611: 'quartet',\n",
                            " 52365: 'padm√©',\n",
                            " 52366: \"'bleedmedry\",\n",
                            " 52367: 'pahalniuk',\n",
                            " 52368: 'honduras',\n",
                            " 10789: 'bursting',\n",
                            " 41468: \"pablo's\",\n",
                            " 52370: 'irremediably',\n",
                            " 40966: 'presages',\n",
                            " 57835: 'bowlegged',\n",
                            " 65186: 'dalip',\n",
                            " 6263: 'entering',\n",
                            " 76175: 'newsradio',\n",
                            " 54153: 'presaged',\n",
                            " 27666: \"giallo's\",\n",
                            " 40967: 'bouyant',\n",
                            " 52371: 'amerterish',\n",
                            " 18526: 'rajni',\n",
                            " 30613: 'leeves',\n",
                            " 34770: 'macauley',\n",
                            " 615: 'seriously',\n",
                            " 52372: 'sugercoma',\n",
                            " 52373: 'grimstead',\n",
                            " 52374: \"'fairy'\",\n",
                            " 30614: 'zenda',\n",
                            " 52375: \"'twins'\",\n",
                            " 17643: 'realisation',\n",
                            " 27667: 'highsmith',\n",
                            " 7820: 'raunchy',\n",
                            " 40968: 'incentives',\n",
                            " 52377: 'flatson',\n",
                            " 35100: 'snooker',\n",
                            " 16832: 'crazies',\n",
                            " 14905: 'crazier',\n",
                            " 7097: 'grandma',\n",
                            " 52378: 'napunsaktha',\n",
                            " 30615: 'workmanship',\n",
                            " 52379: 'reisner',\n",
                            " 61309: \"sanford's\",\n",
                            " 52380: '\\x91do√±a',\n",
                            " 6111: 'modest',\n",
                            " 19156: \"everything's\",\n",
                            " 40969: 'hamer',\n",
                            " 52382: \"couldn't'\",\n",
                            " 13004: 'quibble',\n",
                            " 52383: 'socking',\n",
                            " 21934: 'tingler',\n",
                            " 52384: 'gutman',\n",
                            " 40970: 'lachlan',\n",
                            " 52385: 'tableaus',\n",
                            " 52386: 'headbanger',\n",
                            " 2850: 'spoken',\n",
                            " 34771: 'cerebrally',\n",
                            " 23493: \"'road\",\n",
                            " 21935: 'tableaux',\n",
                            " 40971: \"proust's\",\n",
                            " 40972: 'periodical',\n",
                            " 52388: \"shoveller's\",\n",
                            " 25266: 'tamara',\n",
                            " 17644: 'affords',\n",
                            " 3252: 'concert',\n",
                            " 87958: \"yara's\",\n",
                            " 52389: 'someome',\n",
                            " 8427: 'lingering',\n",
                            " 41514: \"abraham's\",\n",
                            " 34772: 'beesley',\n",
                            " 34773: 'cherbourg',\n",
                            " 28627: 'kagan',\n",
                            " 9100: 'snatch',\n",
                            " 9263: \"miyazaki's\",\n",
                            " 25267: 'absorbs',\n",
                            " 40973: \"koltai's\",\n",
                            " 64030: 'tingled',\n",
                            " 19514: 'crossroads',\n",
                            " 16124: 'rehab',\n",
                            " 52392: 'falworth',\n",
                            " 52393: 'sequals',\n",
                            " ...}"
                        ]
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "idx2word = {v: k for k,v in word2idx.items()}\n",
                "idx2word"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We actually don't even have 10,000 unique tokens in the original dataset so we should update this variable. Otherwise we will have wasted space in embedding and output layers (this will make sense when you see the model architectures).\n",
                "\n",
                "**Important:** This count includes the special tokens like `0` which represent padding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "# update vocab length\n",
                "MAX_VOCAB = len(word2idx)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can see that the text data is already preprocessed for us."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Number of reviews 25000\nLength of first and fifth review before padding 218 147\nFirst review [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\nFirst label 1\n"
                }
            ],
            "source": [
                "print('Number of reviews', len(X_train))\n",
                "print('Length of first and fifth review before padding', len(X_train[0]) ,len(X_train[4]))\n",
                "print('First review', X_train[0])\n",
                "print('First label', y_train[0])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Review Decoding**\n",
                "\n",
                "Here's an example review using the index-to-word mapping we created from the loaded JSON file to view the a review in its original form."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u003cSTART\u003e this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert \u003cUNK\u003e is an amazing actor and now the same being director \u003cUNK\u003e father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for \u003cUNK\u003e and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also \u003cUNK\u003e to the two little boy's that played the \u003cUNK\u003e of norman and paul they were just brilliant children are often left out of the \u003cUNK\u003e list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
                }
            ],
            "source": [
                "def show_review(x):\n",
                "    review = ' '.join([idx2word[idx] for idx in x])\n",
                "    print(review)\n",
                "\n",
                "show_review(X_train[0])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Padding**\n",
                "\n",
                "The only thing what isn't done for us is the padding. Looking at the distribution of lengths will help us determine what a reasonable length to pad to will be."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "\u003cFigure size 640x480 with 1 Axes\u003e"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Distribution of review lengths\n",
                "plt.hist([len(x) for x in X_train])\n",
                "plt.title('review lengths');"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We saw one way of doing this earlier, but Keras actually has a built in `pad_sequences` helper function. This handles both padding and truncating. By default padding is added to the *beginning* of a sequence."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"exercise\"  style=\"background-color:#b3e6ff\"\u003e\n",
                "\u003cb\u003eQ\u003c/b\u003e: Why might we want to truncate? Why might we want to pad from the beginning?\n",
                "\u003c/div\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing.sequence import pad_sequences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Length of first and fifth review after padding 500 500\n"
                }
            ],
            "source": [
                "MAX_LEN = 500\n",
                "X_train = pad_sequences(X_train, maxlen=MAX_LEN)\n",
                "X_test = pad_sequences(X_test, maxlen=MAX_LEN)\n",
                "print('Length of first and fifth review after padding', len(X_train[0]) ,len(X_train[4]))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='FFNN'\u003e\u003cb\u003eModel 1: Naive Feed-Forward Network\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let us build a single-layer feed-forward net with a hidden layer of 250 nodes. Each input would be a 500-dim vector of tokens since we padded all our sequences to size 500.\n",
                "\n",
                "\u003cbr\u003e\n",
                "\u003cdiv class=\"exercise\"  style=\"background-color:#b3e6ff\"\u003e\n",
                "\u003cb\u003eQ\u003c/b\u003e: How would you calculate the number of parameters in this network?\n",
                "\u003c/div\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"Naive_FFNN\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 250)               125250    \n                                                                 \n dense_3 (Dense)             (None, 1)                 251       \n                                                                 \n=================================================================\nTotal params: 125501 (490.24 KB)\nTrainable params: 125501 (490.24 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n"
                }
            ],
            "source": [
                "model = Sequential(name='Naive_FFNN')\n",
                "model.add(Dense(250, activation='relu',input_dim=MAX_LEN))\n",
                "model.add(Dense(1, activation='sigmoid'))\n",
                "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/10\n196/196 - 1s - loss: 191.8202 - accuracy: 0.5025 - val_loss: 90.5763 - val_accuracy: 0.5043 - 1s/epoch - 7ms/step\nEpoch 2/10\n196/196 - 1s - loss: 46.0562 - accuracy: 0.5830 - val_loss: 44.2539 - val_accuracy: 0.4996 - 546ms/epoch - 3ms/step\nEpoch 3/10\n196/196 - 1s - loss: 15.2216 - accuracy: 0.6588 - val_loss: 26.4614 - val_accuracy: 0.4964 - 541ms/epoch - 3ms/step\nEpoch 4/10\n196/196 - 1s - loss: 6.2277 - accuracy: 0.7146 - val_loss: 15.7935 - val_accuracy: 0.5026 - 540ms/epoch - 3ms/step\nEpoch 5/10\n196/196 - 1s - loss: 3.0387 - accuracy: 0.7570 - val_loss: 11.7361 - val_accuracy: 0.5029 - 539ms/epoch - 3ms/step\nEpoch 6/10\n196/196 - 1s - loss: 1.9168 - accuracy: 0.7740 - val_loss: 8.7116 - val_accuracy: 0.4974 - 539ms/epoch - 3ms/step\nEpoch 7/10\n196/196 - 1s - loss: 1.2677 - accuracy: 0.7971 - val_loss: 7.0700 - val_accuracy: 0.5000 - 539ms/epoch - 3ms/step\nEpoch 8/10\n196/196 - 1s - loss: 1.0722 - accuracy: 0.8030 - val_loss: 6.5899 - val_accuracy: 0.5011 - 540ms/epoch - 3ms/step\nEpoch 9/10\n196/196 - 1s - loss: 0.8616 - accuracy: 0.8222 - val_loss: 6.4307 - val_accuracy: 0.5031 - 541ms/epoch - 3ms/step\nEpoch 10/10\n196/196 - 1s - loss: 0.7819 - accuracy: 0.8322 - val_loss: 6.4552 - val_accuracy: 0.4984 - 540ms/epoch - 3ms/step\nAccuracy: 49.84%\n"
                }
            ],
            "source": [
                "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128, verbose=2)\n",
                "\n",
                "scores = model.evaluate(X_test, y_test, verbose=0)\n",
                "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"exercise\"  style=\"background-color:#b3e6ff\"\u003e\n",
                "\u003cb\u003eQ\u003c/b\u003e: Why was the performance so poor? How could we improve our tokenization?\n",
                "\u003c/div\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='FFNN_emb'\u003e\u003cb\u003eModel 2: Feed-Forward Network /w Embeddings\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\u003cimg src='fig/wordembedding2.png' width=450px\u003e\n",
                "\n",
                "One can view the embedding process as a linear projection from one vector space to another. For NLP, we usually use embeddings to project the sparse one-hot encodings of words on to a lower-dimensional continuous space so that the input surface is 'dense' and possibly smooth. Thus, one can view this embedding layer process as just a transformation from $\\mathbb{R}^{inp}$ to $\\mathbb{R}^{emb}$\n",
                "\n",
                "This not only reduces dimensionality but also allows semantic similarities between tokens to be captured by 'similiarities' between the embedding vectors. This is not possible with one-hot encoding as all vectors there were orthogonal to one another. \n",
                "\n",
                "\u003cimg src='fig/wordembedding.png' width=450px\u003e\n",
                "\n",
                "It is also possible to load pretrained embeddings that were learned from giant corpora. This would be an instance of transfer learning.\n",
                "\n",
                "If you are interested in learning more, start with the astromonically impactful papers of [word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) and [GloVe](https://www.aclweb.org/anthology/D14-1162.pdf).\n",
                "\n",
                "In Keras we use the [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer:\n",
                "```\n",
                "tf.keras.layers.Embedding(\n",
                "    input_dim, output_dim, embeddings_initializer='uniform',\n",
                "    embeddings_regularizer=None, activity_regularizer=None,\n",
                "    embeddings_constraint=None, mask_zero=True, input_length=None, **kwargs\n",
                ")\n",
                "```\n",
                "We'll need to specify the `input_dim` and `output_dim`. If working with sequences, as we are, you'll also need to set the `input_length`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"FFNN_EMBED\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 500, 100)          8858800   \n                                                                 \n flatten (Flatten)           (None, 50000)             0         \n                                                                 \n dense_4 (Dense)             (None, 250)               12500250  \n                                                                 \n dense_5 (Dense)             (None, 1)                 251       \n                                                                 \n=================================================================\nTotal params: 21359301 (81.48 MB)\nTrainable params: 21359301 (81.48 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n"
                }
            ],
            "source": [
                "EMBED_DIM = 100\n",
                "\n",
                "model = Sequential(name='FFNN_EMBED')\n",
                "model.add(Embedding(MAX_VOCAB, EMBED_DIM, input_length=MAX_LEN, mask_zero=True))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(250, activation='relu'))\n",
                "model.add(Dense(1, activation='sigmoid'))\n",
                "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/2\n196/196 - 18s - loss: 0.5100 - accuracy: 0.7264 - val_loss: 0.2934 - val_accuracy: 0.8767 - 18s/epoch - 90ms/step\nEpoch 2/2\n196/196 - 12s - loss: 0.1471 - accuracy: 0.9444 - val_loss: 0.3100 - val_accuracy: 0.8730 - 12s/epoch - 63ms/step\n782/782 - 1s - loss: 0.3100 - accuracy: 0.8730 - 960ms/epoch - 1ms/step\nAccuracy: 87.30%\n"
                }
            ],
            "source": [
                "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
                "\n",
                "scores = model.evaluate(X_test, y_test, verbose=2)\n",
                "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='1D_CNN'\u003e\u003cb\u003eModel 3: 1-Dimensional Convolutional Network\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\u003cimg src='fig/1D-CNN.png'\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Text can be thought of as 1-dimensional sequence (a single, long vector) and we can apply 1D Convolutions over a set of word embeddings.\u003cbr\u003e\n",
                "\n",
                "More information on convolutions on text data can be found on [this blog](http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/11/27/Understanding-Convolutions-In-Text/). If you want to learn more, read this [published and well-cited paper](https://www.aclweb.org/anthology/I17-1026.pdf)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"exercise\"  style=\"background-color:#b3e6ff\"\u003e\n",
                "\u003cb\u003eQ\u003c/b\u003e: Why do we use Conv1D if our input, a sequence of word embeddings, is 2D?\n",
                "\u003c/div\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"1D_CNN\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 500, 100)          8858800   \n                                                                 \n conv1d (Conv1D)             (None, 500, 200)          60200     \n                                                                 \n max_pooling1d (MaxPooling1  (None, 250, 200)          0         \n D)                                                              \n                                                                 \n flatten_1 (Flatten)         (None, 50000)             0         \n                                                                 \n dense_6 (Dense)             (None, 250)               12500250  \n                                                                 \n dense_7 (Dense)             (None, 1)                 251       \n                                                                 \n=================================================================\nTotal params: 21419501 (81.71 MB)\nTrainable params: 21419501 (81.71 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/2\n196/196 [==============================] - 16s 72ms/step - loss: 0.5395 - accuracy: 0.6908\nEpoch 2/2\n196/196 [==============================] - 12s 60ms/step - loss: 0.2253 - accuracy: 0.9134\nAccuracy: 88.70%\n"
                }
            ],
            "source": [
                "model = Sequential(name='1D_CNN')\n",
                "model.add(Embedding(MAX_VOCAB, EMBED_DIM, input_length=MAX_LEN, mask_zero=True))\n",
                "model.add(Conv1D(filters=200, kernel_size=3, padding='same', activation='relu'))\n",
                "model.add(MaxPool1D(pool_size=2))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(250, activation='relu'))\n",
                "model.add(Dense(1, activation='sigmoid'))\n",
                "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "print(model.summary())\n",
                "\n",
                "model.fit(X_train, y_train, epochs=2, batch_size=128)\n",
                "\n",
                "scores = model.evaluate(X_test, y_test, verbose=0)\n",
                "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='rnn_model'\u003e\u003cb\u003eModel 4: Simple RNN\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\u003cimg src='fig/simplernn.png' width=300px\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "At a high-level, an RNN is similar to a feed-forward neural network (FFNN) in that there is an input layer, a hidden layer, and an output layer. The input layer is fully connected to the hidden layer, and the hidden layer is fully connected to the output layer. However, the crux of what makes it a **recurrent** neural network is that the hidden layer for a given time _t_ is not only based on the input layer at time _t_ but also the hidden layer from time _t-1_.\n",
                "\n",
                "Here's a popular blog post on [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
                "\n",
                "In Keras, the vanilla RNN unit is implemented the`SimpleRNN` layer:\n",
                "```\n",
                "tf.keras.layers.SimpleRNN(\n",
                "    units, activation='tanh', use_bias=True,\n",
                "    kernel_initializer='glorot_uniform',\n",
                "    recurrent_initializer='orthogonal',\n",
                "    bias_initializer='zeros', kernel_regularizer=None,\n",
                "    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
                "    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
                "    dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False,\n",
                "    go_backwards=False, stateful=False, unroll=False, **kwargs\n",
                ")\n",
                "```\n",
                "As you can see, recurrent layers in Keras take many arguments. We only need to be concerned with `units`, which specifies the size of the hidden state, and `return_sequences`, which will be discussed shortly. For the moment is it fine to leave this set to the default of `False`.\n",
                "\n",
                "Due to the limitations of the vanilla RNN unit (more on that next) it tends not to be used much in practice. For this reason it seems that the Keras developers neglected to implement GPU acceleration for this layer! Notice how much slower the trainig is even for a network with far fewer parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"SimpleRNN\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_2 (Embedding)     (None, 500, 100)          8858800   \n                                                                 \n simple_rnn (SimpleRNN)      (None, 100)               20100     \n                                                                 \n dense_8 (Dense)             (None, 1)                 101       \n                                                                 \n=================================================================\nTotal params: 8879001 (33.87 MB)\nTrainable params: 8879001 (33.87 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/3\n196/196 [==============================] - 88s 438ms/step - loss: 0.6248 - accuracy: 0.6333\nEpoch 2/3\n196/196 [==============================] - 83s 423ms/step - loss: 0.3866 - accuracy: 0.8280\nEpoch 3/3\n196/196 [==============================] - 80s 410ms/step - loss: 0.2304 - accuracy: 0.9070\nAccuracy: 84.00%\n"
                }
            ],
            "source": [
                "model = Sequential(name='SimpleRNN')\n",
                "model.add(Embedding(MAX_VOCAB, EMBED_DIM, input_length=MAX_LEN, mask_zero=True))\n",
                "model.add(SimpleRNN(100))\n",
                "model.add(Dense(1, activation='sigmoid'))\n",
                "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "print(model.summary())\n",
                "\n",
                "model.fit(X_train, y_train, epochs=3, batch_size=128)\n",
                "\n",
                "scores = model.evaluate(X_test, y_test, verbose=0)\n",
                "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='rnn_model'\u003e\u003cb\u003eVanishing/Exploding Gradients\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cimg src = \"fig/backprop.png\" width=500px\u003e\n",
                "\u003cbr\u003e\n",
                "\n",
                "We need to backpropogate through every time step to calculate the gradients used for our weight updates.\n",
                "\n",
                "This requires the use of the chain rule which amounts to repeated multiplications.\n",
                "\n",
                "This can cause two types of problems. First, this product can quickly 'explode,' becoming large, causing destructive updates to the model and numerical overflow. One hack to solve this problem is to **clip** the gradient at some threshold.\n",
                "\n",
                "Alternatively, the gradient can 'vanish,' getting smaller and smaller as the gradient moves backwards in time. Gradient clipping will not help us here. If we can't propogate gradients suffuciently far back in time then our network will be unable to learn long temporal dependencies. This problem motivates the architecture of the GRU and LSTM units as substitutes for the 'vanilla' RNN."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='rnn_model'\u003e\u003cb\u003eModel 5: GRU\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\u003cimg src='fig/gru.png' width=800px\u003e\n",
                "\n",
                "$X_{t}$: input\u003cbr\u003e\n",
                "$U$, $V$, and $\\beta$: parameter matrices and vector\u003cbr\u003e\n",
                "$\\tilde{h_t}$: candidate activation vector\u003cbr\u003e\n",
                "$h_{t}$: output vector\u003cbr\u003e\n",
                "$R_t$: reset gate\u003cbr\u003e\n",
                "$Z_t$: update gate\u003cbr\u003e\n",
                "\n",
                "The gates of the GRU allow for the gradients to flow more freely to previous time steps, helping to mitigate the vanishing gradient problem.\n",
                "\n",
                "In Keras, the [`GRU`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) layer is used in exactly the same way as the `SimpleRNN` layer. \n",
                "```\n",
                "tf.keras.layers.GRU(\n",
                "    units, activation='tanh', recurrent_activation='sigmoid',\n",
                "    use_bias=True, kernel_initializer='glorot_uniform',\n",
                "    recurrent_initializer='orthogonal',\n",
                "    bias_initializer='zeros', kernel_regularizer=None,\n",
                "    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
                "    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
                "    dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False,\n",
                "    go_backwards=False, stateful=False, unroll=False, time_major=False,\n",
                "    reset_after=True, **kwargs\n",
                ")\n",
                "```\n",
                "\n",
                "Here we just swap it in to the previous architecture. Note how much faster it trains with GPU excelleration than the simple RNN!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"GRU\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_3 (Embedding)     (None, 500, 100)          8858800   \n                                                                 \n gru (GRU)                   (None, 100)               60600     \n                                                                 \n dense_9 (Dense)             (None, 1)                 101       \n                                                                 \n=================================================================\nTotal params: 8919501 (34.03 MB)\nTrainable params: 8919501 (34.03 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/3\n391/391 [==============================] - 235s 593ms/step - loss: 0.4262 - accuracy: 0.7975\nEpoch 2/3\n391/391 [==============================] - 217s 555ms/step - loss: 0.2403 - accuracy: 0.9079\nEpoch 3/3\n391/391 [==============================] - 211s 541ms/step - loss: 0.1682 - accuracy: 0.9383\nAccuracy: 87.82%\n"
                }
            ],
            "source": [
                "model = Sequential(name='GRU')\n",
                "model.add(Embedding(MAX_VOCAB, EMBED_DIM, input_length=MAX_LEN, mask_zero=True))\n",
                "model.add(GRU(100))\n",
                "model.add(Dense(1, activation='sigmoid'))\n",
                "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "print(model.summary())\n",
                "\n",
                "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
                "\n",
                "scores = model.evaluate(X_test, y_test, verbose=0)\n",
                "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='rnn_model'\u003e\u003cb\u003eModel 6: LSTM\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\u003cimg src='fig/lstm.png' width=600px\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The LSTM lacks the GRU's 'short cut' connection (see GRU's $h_t$ above).\n",
                "\n",
                "The LSTM also has a distinct 'cell state' in addition to the hidden state. \n",
                "\n",
                "Futher reading: \n",
                "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
                "- [LSTM: A Search Space Odyssey](https://arxiv.org/abs/1503.04069)\n",
                "- [An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf)\n",
                "\n",
                "Again, Kera's [`LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) works like all the other recurrent layers.\n",
                "```\n",
                "tf.keras.layers.LSTM(\n",
                "    units, activation='tanh', recurrent_activation='sigmoid',\n",
                "    use_bias=True, kernel_initializer='glorot_uniform',\n",
                "    recurrent_initializer='orthogonal',\n",
                "    bias_initializer='zeros', unit_forget_bias=True,\n",
                "    kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None,\n",
                "    activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None,\n",
                "    bias_constraint=None, dropout=0.0, recurrent_dropout=0.0,\n",
                "    return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n",
                "    time_major=False, unroll=False, **kwargs\n",
                ")\n",
                "```\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"LSTM\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_4 (Embedding)     (None, 500, 100)          8858800   \n                                                                 \n lstm (LSTM)                 (None, 100)               80400     \n                                                                 \n dense_10 (Dense)            (None, 1)                 101       \n                                                                 \n=================================================================\nTotal params: 8939301 (34.10 MB)\nTrainable params: 8939301 (34.10 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/3\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "2025-04-03 21:26:05.350569: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\ntype_id: TFT_OPTIONAL\nargs {\n  type_id: TFT_PRODUCT\n  args {\n    type_id: TFT_TENSOR\n    args {\n      type_id: TFT_INT32\n    }\n  }\n}\n is neither a subtype nor a supertype of the combined inputs preceding it:\ntype_id: TFT_OPTIONAL\nargs {\n  type_id: TFT_PRODUCT\n  args {\n    type_id: TFT_TENSOR\n    args {\n      type_id: TFT_HALF\n    }\n  }\n}\n\n\tfor Tuple type infernce function 0\n\twhile inferring type of node 'cond_36/output/_23'\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "391/391 [==============================] - 225s 568ms/step - loss: 0.4347 - accuracy: 0.8086\nEpoch 2/3\n391/391 [==============================] - 214s 548ms/step - loss: 0.3080 - accuracy: 0.8796\nEpoch 3/3\n391/391 [==============================] - 209s 535ms/step - loss: 0.2493 - accuracy: 0.9065\nAccuracy: 86.16%\n"
                }
            ],
            "source": [
                "model = Sequential(name='LSTM')\n",
                "model.add(Embedding(MAX_VOCAB, EMBED_DIM, input_length=MAX_LEN, mask_zero=True))\n",
                "model.add(LSTM(100))\n",
                "model.add(Dense(1, activation='sigmoid'))\n",
                "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "print(model.summary())\n",
                "\n",
                "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
                "\n",
                "scores = model.evaluate(X_test, y_test, verbose=0)\n",
                "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='bidir'\u003e\u003cb\u003eBiDirectional Layer\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\u003cimg src='fig/bi-directional-RNN.png' width=600px\u003e\n",
                "\n",
                "We may want our model to learn dependencies in either direction. A **BiDirectional RNN** consists of two separate recurrent units. One processing the sequence from left to right, the other processes that same sequence but in reverse, from right to left. The output of the two units are then merged together (typically concatenated) and feed to the next layer of the network.\u003cbr\u003e\n",
                "\n",
                "\n",
                "\n",
                "Creating a Bidirection RNN in Keras is quite simple. We just 'wrap' a recurrent layer in the [`Bidirectional`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) layer. The default behavior is to concatenate the output from each direction.\n",
                "\n",
                "```\n",
                "tf.keras.layers.Bidirectional(\n",
                "    layer, merge_mode='concat', weights=None, backward_layer=None,\n",
                "    **kwargs\n",
                ")\n",
                "```\n",
                "\n",
                "Example:\n",
                "```\n",
                "model = Sequential()\n",
                "...\n",
                "model.add(Bidirectional(SimpleRNN(n_nodes))\n",
                "...\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='deep'\u003e\u003cb\u003eDeep RNNs\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\u003cimg src='fig/deeprnn.png' width=600px\u003e\n",
                "\n",
                "We may want to stack RNN layers one after another. But there is a problem. A recurrent layer expects to be given a sequence as input, and yet we can see that the recurrent layer in each of our models above outputs a single vector. This is because the default behavior of Keras's recurrent layers is to suppress the output until the final time step. If we want to have two recurrent units in a row then the first will have to given an output after each time step, thus providing a sequence to the 2nd recurrent layer.\n",
                "\n",
                "We can have our recurrent layers output at each time step setting `return_sequences=True`.\u003cbr\u003e\n",
                "Example:\n",
                "```\n",
                "model = Sequential()\n",
                "...\n",
                "model.add(LSTM(100, return_sequences=True))\n",
                "model.add(LSTM(100)\n",
                "...\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='timedist'\u003e\u003cb\u003eTimeDistributed Layer\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[`TimeDistributed`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed) is a 'wrapper' that applies a layer to all time steps of an input sequence.\n",
                "```\n",
                "tf.keras.layers.TimeDistributed(\n",
                "    layer, **kwargs\n",
                ")\n",
                "```\n",
                "Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.\n",
                "\n",
                "Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with channels_last data format, across 10 timesteps. The batch input shape is (32, 10, 128, 128, 3).\n",
                "\n",
                "You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "TensorShape([None, 10, 126, 126, 64])"
                        ]
                    },
                    "execution_count": 45,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from tensorflow import TensorShape\n",
                "\n",
                "inputs = tf.keras.Input(shape=(10, 128, 128, 3))\n",
                "conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))\n",
                "outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)\n",
                "outputs.shape\n",
                "TensorShape([None, 10, 126, 126, 64])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='repeatvec'\u003e\u003cb\u003eRepeatVector Layer\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The [`RepeatVector`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed) repeats the output of a layer a specified number of times. Dimension changes from \u003cbr\u003e\n",
                "(batch_size, number_of_elements)\u003cbr\u003e\n",
                "to\u003cbr\u003e\n",
                "(batch_size, number_of_repetitions, number_of_elements)\n",
                "\n",
                "This effectively generates a sequence from a single input."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_11 (Dense)            (None, 2)                 4         \n                                                                 \n repeat_vector (RepeatVecto  (None, 3, 2)              0         \n r)                                                              \n                                                                 \n=================================================================\nTotal params: 4 (16.00 Byte)\nTrainable params: 4 (16.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n"
                }
            ],
            "source": [
                "model = Sequential()\n",
                "model.add(Dense(2, input_dim=1))\n",
                "model.add(RepeatVector(3))\n",
                "model.summary()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"exercise\"  style=\"background-color:#b3e6ff\"\u003e\n",
                "\u003cb\u003eQ\u003c/b\u003e: Can you think of some tasks that would require the use of `RepeatVector`?\n",
                "\u003c/div\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise' id='cnn_rnn'\u003e\u003cb\u003eModel 7: CNN + RNN\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "CNNs are good at learning spatial features, and sentences can be thought of as 1-D spatial vectors (dimensionality is determined by the number of words in the sentence). We can then take the features learned by the CNN (after a maxpooling layer) and feed them into an RNN! We expect the CNN to be able to pick out invariant features across the 1-D spatial structure (i.e., sentence) that characterize good and bad sentiment. This learned spatial features may then be learned as sequences by a reccurent layer. The classification step is then performed by a final dense layer."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"exercise\"  style=\"background-color:#F5E4C3\"\u003e\n",
                "    \u003cb\u003eExercise:\u003c/b\u003e Build a CNN + Deep, BiDirectional GRU Model\n",
                "\u003c/div\u003e\n",
                "\n",
                "Let's put together everything we've learned so far.\u003cbr\u003e\n",
                "Create a network with:\n",
                "- word embeddings in a 100-dimensional space\n",
                "- conv layer with 32 filters, kernels of width 3, 'same' padding, and ReLU activate\n",
                "- max pooling of size 2\n",
                "- 2 bidirectional GRU layers, each with 50 units *per direction*\n",
                "- dense output layer for binary classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"CNN_GRU\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_5 (Embedding)     (None, 500, 100)          8858800   \n                                                                 \n conv1d_1 (Conv1D)           (None, 500, 32)           9632      \n                                                                 \n max_pooling1d_1 (MaxPoolin  (None, 250, 32)           0         \n g1D)                                                            \n                                                                 \n bidirectional (Bidirection  (None, 250, 100)          25200     \n al)                                                             \n                                                                 \n bidirectional_1 (Bidirecti  (None, 100)               45600     \n onal)                                                           \n                                                                 \n dense_12 (Dense)            (None, 1)                 101       \n                                                                 \n=================================================================\nTotal params: 8939333 (34.10 MB)\nTrainable params: 8939333 (34.10 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n"
                }
            ],
            "source": [
                "model = Sequential(name='CNN_GRU')\n",
                "# your code here\n",
                "model.add(Embedding(MAX_VOCAB, 100, input_length=MAX_LEN, mask_zero=True))\n",
                "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
                "model.add(MaxPool1D(pool_size=2))\n",
                "model.add(Bidirectional(GRU(50, return_sequences=True)))\n",
                "model.add(Bidirectional(GRU(50)))\n",
                "model.add(Dense(1, activation='sigmoid'))\n",
                "\n",
                "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/3\n391/391 [==============================] - 32s 70ms/step - loss: 0.3858 - accuracy: 0.8123\nEpoch 2/3\n391/391 [==============================] - 19s 48ms/step - loss: 0.1885 - accuracy: 0.9296\nEpoch 3/3\n391/391 [==============================] - 15s 37ms/step - loss: 0.1076 - accuracy: 0.9623\nAccuracy: 88.26%\n"
                }
            ],
            "source": [
                "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
                "\n",
                "scores = model.evaluate(X_test, y_test, verbose=0)\n",
                "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Using the `show_review` function we defined earlier..."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What is the *worst* movie review in the test set according to your model? üçÖ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "782/782 [==============================] - 8s 9ms/step\nnot renowned for his talents as a writer or a director but he has made some undeniably important films kids bully and to a lesser extent ken park all achieve their intended purpose shock \u003cUNK\u003e and even disgust these films are \u003cUNK\u003e in their content and use their controversial nature to expose very serious problems in modern youth kids exposed us to the \u003cUNK\u003e of a i d s and sexual \u003cUNK\u003e among the young bully touched upon similar issues ken park dealt somewhat ham \u003cUNK\u003e with sexual abuse and suburban \u003cUNK\u003e \u003cUNK\u003e all of these films exposed something horrifying and left a bad taste in your mouth br br \u003cUNK\u003e \u003cUNK\u003e is about a group of poor hispanic from south central las angeles who go to \u003cUNK\u003e go to beverly hills to \u003cUNK\u003e that's it br br \u003cUNK\u003e \u003cUNK\u003e is nothing br br it has no substance it has an essentially nonexistent narrative and like kids it features a cast of first time actors who were drawn out of the films setting however unlike kids none of them have any semblance of talent there is better acting in porn this film features without a doubt the most terrible performances i've ever seen in a feature film one can respect larry clark to exposing these young men to the film making process but these kids are absolutely cringe worthy folks might i add that apparently these \u003cUNK\u003e also produced the soundtrack which features some of the most \u003cUNK\u003e inept garage punk you'll ever hear my advice is to pop a couple of \u003cUNK\u003e \u003cUNK\u003e before you enter the theater or you'll regret it afterward br br but then again it's not like they had much of a script to work with every line that is \u003cUNK\u003e is a contrived \u003cUNK\u003e delivered and irritating beyond all measure the story itself is ludicrous it starts out reasonably enough but soon slips quite unexpectedly into sheer absurdity this begins of course with a \u003cUNK\u003e with a pair of rich white girls followed by a series of clich√©d national encounters characters being killed off for no reason and finally resulting in a ridiculous anti climax shots go on much longer than they need to be prepared to watch people fall of \u003cUNK\u003e for about fifteen minutes straight overlong lingering shots of characters doing nothing or \u003cUNK\u003e down streets but then again with the script at a \u003cUNK\u003e \u003cUNK\u003e pages they need as much useless filler as possible perhaps \u003cUNK\u003e \u003cUNK\u003e would have worked better as a short film br br anyways i could go on like this this is the worst film larry clark has made yet for those of you who are interested in seeing a clark movie if only for his shocking \u003cUNK\u003e antics look elsewhere this is by far the \u003cUNK\u003e film he's made yet and it's also the worst it's flat out horrible like uwe boll horrible definitely the worst one i saw at the festival br br 1 10\n"
                }
            ],
            "source": [
                "preds = model.predict(X_test)\n",
                "worst_review = X_test[preds.argmin()]\n",
                "show_review(worst_review)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What is the *best* movie review in the test set according to your model? üèÜ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cPAD\u003e \u003cSTART\u003e in \u003cUNK\u003e a young man alone in a single engine \u003cUNK\u003e flew non stop from \u003cUNK\u003e field in new york across the entire north atlantic ocean to le field in paris a distance of three thousand six hundred and ten miles in this triumph of mind body and spirit charles a \u003cUNK\u003e influenced the lives of everyone on earth for in the 33 hours and thirty minutes of his flight the air age became a reality this is the story of that flight br br billy wilder \u003cUNK\u003e from charles a \u003cUNK\u003e prize winning novel of the same name in what is re creation of historical \u003cUNK\u003e solo flight \u003cUNK\u003e by a considerably strong lead performance from james stewart himself a pilot as \u003cUNK\u003e and containing an intelligent screenplay from wilder and \u003cUNK\u003e spirit of st louis is a sincerely well told story br br in what at times threatens to become a monotonous film wilder keeps it \u003cUNK\u003e over by using flashbacks to life after the nicely told build up to the event such as the peril being realised as and go missing never to be found whilst attempting the same trip in reverse we learn stuff like how he come to buy his first plane and his work with the flying circus this is all relative to understanding the man and his obvious passion for flying this also helps to give us a complete picture of \u003cUNK\u003e thus putting us with him in his isolated \u003cUNK\u003e as he \u003cUNK\u003e this dangerous journey battling isolation his only company is a fly and \u003cUNK\u003e \u003cUNK\u003e it's here where stewart perfectly portrays devotion to the task aided by a terrific score from \u003cUNK\u003e \u003cUNK\u003e and academy award nominated effects by louis \u003cUNK\u003e movie turns out to be an engaging human interest story that got a thoroughly professional production 7 10\n"
                }
            ],
            "source": [
                "best_review = X_test[preds.argmax()]\n",
                "show_review(best_review)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"exercise\"  style=\"background-color:#F5E4C3\"\u003e\n",
                "    \u003cb\u003eEnd of Exercise\u003c/b\u003e\n",
                "\u003c/div\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "üåà **The End**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
